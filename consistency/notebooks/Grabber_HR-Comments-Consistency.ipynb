{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import openai\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('/home/pervinco/LLM-tutorials/keys.env')\n",
    "openai_api_key = os.getenv('GRAVY_LAB_OPENAI')\n",
    "\n",
    "from src.data_processor import translate_and_convert_to_string, process_vision_result, extract_workstyle_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_path = \"./data/amy_gt.json\"\n",
    "data_path = \"./data/amy_culture_fit.json\"\n",
    "\n",
    "n_iter = 100\n",
    "temperature = 0.1\n",
    "prompt_version = 1\n",
    "embed_model = \"text-embedding-3-large\"\n",
    "\n",
    "output_dir = \"./result\"\n",
    "csv_file_name = f\"Non-CoT-N_{n_iter}.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if prompt_version == 1:\n",
    "    from src.prompt_processor import vision_prompt, workstyle_prompt, summary_prompt\n",
    "else:\n",
    "    from src.cot_prompt_processor import vision_prompt, workstyle_prompt, summary_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI(api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path, 'r', encoding='utf-8') as file:\n",
    "    hr_data_dict = json.load(file)\n",
    "\n",
    "with open(data_path, 'r', encoding=\"utf-8\") as file:\n",
    "    gt_data_dict = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_summary = translate_and_convert_to_string(hr_data_dict['summaryResult'])\n",
    "vision_data = process_vision_result(hr_data_dict['visionResult'], hr_data_dict['summaryResult'])\n",
    "workstyle_data = extract_workstyle_info(hr_data_dict['workstyleResult'], hr_data_dict['summaryResult'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "채용 권장 수준, 입사 후 적응 기간, 조기 퇴사 가능성 : \n",
      "조기 퇴사 가능성: 낮음\n",
      "입사 후 적응 기간: 보통\n",
      "채용 권장 수준: 보통\n",
      "\n",
      "검사 항목별 결과 : \n",
      "5) 사고방식이 기업 비전,가치관에 부합하는가?: 매우 그렇다\n",
      "2) 타 팀, 타 구성원과의 원만한 협업을 기대할 수 있는가?: 그렇다\n",
      "3) 경영진, 상급자와의 원활한 소통을 기대할 수 있는가?: 그렇다\n",
      "4) 기업이 추구하는 일하는 방식과 부합하는가?: 매우 그렇다\n",
      "1) 구성원들과 원활한 소통이 가능한가?: 그렇다\n",
      "\n",
      "이직 스트레스 요인 : \n",
      "공정인사\n",
      "\n",
      "위험 성향 : 오만형\n"
     ]
    }
   ],
   "source": [
    "print(processed_data_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'company_top_keywords': '전문성:4.6, 성과:4.2, 사회공헌:4.2', 'company_remaining_keywords': '상생:4.0, 최고지향:3.6, 고객:3.0, 성장:3.0', 'compute_top_keywords': '창조:5.0, 열정:5.0, 혁신:4.58', 'compute_remaining_keywords': '신속성:4.23, 사회공헌:4.17, 고객:4.0, 성장:3.85, 소통:3.75, 성과:3.33, 도전:3.21, 상생:3.13, 최고지향:2.78, 문제해결:2.5, 인재:2.27, 즐거움:1.5, 전문성:0.38', 'compute_vision_total_evalation': '보통'}\n"
     ]
    }
   ],
   "source": [
    "print(vision_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'company_keywords': '스피드형:3.2, 책임형:2.7, 목표지향형:4.0, 끈기형:3.0, 긍정형:3.5, 유니크형:2.9, 혁신형:3.4, 도전형:3.0, 스마트형:3.5, 윤리형:2.2, 성취형:2.2, 열린사고형:3.3, 솔선수범형:3.5, 몰입형:4.3, 신뢰형:1.8, 자기확신형:2.7', 'compute_keywords': '스피드형:4.33, 책임형:0.63, 목표지향형:3.33, 끈기형:0.67, 긍정형:2.5, 유니크형:5.0, 혁신형:5.0, 도전형:3.5, 스마트형:2.75, 윤리형:1.25, 성취형:3.44, 열린사고형:3.75, 솔선수범형:1.88, 몰입형:3.44, 신뢰형:0.65, 자기확신형:3.0', 'workstyle_match_percentage': 18.75, 'workstyle_company_total_score': 49.2, 'workstyle_compute_total_score': 45.12, 'comparison_ratio': 91.70731707317073, 'compute_workstyle_total_evalation': '우수'}\n",
      "company_keywords 스피드형:3.2, 책임형:2.7, 목표지향형:4.0, 끈기형:3.0, 긍정형:3.5, 유니크형:2.9, 혁신형:3.4, 도전형:3.0, 스마트형:3.5, 윤리형:2.2, 성취형:2.2, 열린사고형:3.3, 솔선수범형:3.5, 몰입형:4.3, 신뢰형:1.8, 자기확신형:2.7\n",
      "compute_keywords 스피드형:4.33, 책임형:0.63, 목표지향형:3.33, 끈기형:0.67, 긍정형:2.5, 유니크형:5.0, 혁신형:5.0, 도전형:3.5, 스마트형:2.75, 윤리형:1.25, 성취형:3.44, 열린사고형:3.75, 솔선수범형:1.88, 몰입형:3.44, 신뢰형:0.65, 자기확신형:3.0\n",
      "workstyle_match_percentage 18.75\n",
      "workstyle_company_total_score 49.2\n",
      "workstyle_compute_total_score 45.12\n",
      "comparison_ratio 91.70731707317073\n",
      "compute_workstyle_total_evalation 우수\n"
     ]
    }
   ],
   "source": [
    "print(workstyle_data)\n",
    "\n",
    "for k, v in workstyle_data.items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_input = f\"\"\"\n",
    "기업의 비전 데이터: {hr_data_dict[\"visionResult\"]['company']}\n",
    "피검사자의 비전 데이터: {hr_data_dict[\"visionResult\"]['compute']}\n",
    "\"\"\"\n",
    "\n",
    "workstyle_input = f\"\"\"\n",
    "company_keywords : {workstyle_data['company_keywords']}\n",
    "compute_keywords : {workstyle_data['compute_keywords']}\n",
    "workstyle_company_total_score : {workstyle_data['workstyle_company_total_score']}\n",
    "workstyle_compute_total_score : {workstyle_data['workstyle_compute_total_score']}\n",
    "workstyle_compute_total_score : {workstyle_data['workstyle_match_percentage']}\n",
    "comparison_ratio: {workstyle_data['comparison_ratio']}\n",
    "compute_workstyle_total_evaluation : {workstyle_data['compute_workstyle_total_evalation']}\n",
    "\"\"\"\n",
    "\n",
    "summary_input = f\"\"\"\n",
    "additionalInformation : {hr_data_dict['summaryResult']['additionalInformation']}\n",
    "recruitentQuestions : {hr_data_dict['summaryResult']['recruitentQuestions']}\n",
    "turnOVerFactors : {hr_data_dict['summaryResult']['turnOverFactors']}\n",
    "fued :{hr_data_dict['summaryResult']['fued']}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_openai_api(n_iter, prompt, input, temperature):\n",
    "    results = []\n",
    "    for i in range(n_iter):\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": prompt},\n",
    "                {\"role\": \"user\", \"content\": input}\n",
    "            ],\n",
    "            temperature=temperature\n",
    "        )\n",
    "        response_content = completion.choices[0].message.content\n",
    "        results.append({\n",
    "            \"iteration\": i + 1,\n",
    "            \"response\": response_content\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "company top3 : 전문성:4.6, 성과:4.2, 사회공헌:4.2\n",
      "\n",
      "compute top3 : 창조:5.0, 열정:5.0, 혁신:4.58\n",
      "\n",
      "compnay remain : 상생:4.0, 최고지향:3.6, 고객:3.0, 성장:3.0\n",
      "\n",
      "compute remain : 신속성:4.23, 사회공헌:4.17, 고객:4.0, 성장:3.85, 소통:3.75, 성과:3.33, 도전:3.21, 상생:3.13, 최고지향:2.78, 문제해결:2.5, 인재:2.27, 즐거움:1.5, 전문성:0.38\n",
      "\n",
      "fianl eval : 보통\n"
     ]
    }
   ],
   "source": [
    "print(f\"company top3 : {vision_data['company_top_keywords']}\\n\")\n",
    "print(f\"compute top3 : {vision_data['compute_top_keywords']}\\n\")\n",
    "\n",
    "print(f\"compnay remain : {vision_data['company_remaining_keywords']}\\n\")\n",
    "print(f\"compute remain : {vision_data['compute_remaining_keywords']}\\n\")\n",
    "\n",
    "print(f\"fianl eval : {vision_data['compute_vision_total_evalation']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_results = run_openai_api(n_iter, vision_prompt, vision_input, temperature)\n",
    "workstyle_results = run_openai_api(n_iter, workstyle_prompt, workstyle_input, temperature)\n",
    "summary_results = run_openai_api(n_iter, summary_prompt, summary_input, temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_results_dataframe(vision_results, workstyle_results):\n",
    "    \"\"\"Create separate dataframes for vision and workstyle results\"\"\"\n",
    "    vision_df = pd.DataFrame(vision_results)\n",
    "    vision_df['type'] = 'vision'\n",
    "\n",
    "    workstyle_df = pd.DataFrame(workstyle_results)\n",
    "    workstyle_df['type'] = 'workstyle'\n",
    "\n",
    "    summary_df = pd.DataFrame(summary_results)\n",
    "    summary_df['type'] = 'summary'\n",
    "    \n",
    "    # Combine the dataframes\n",
    "    combined_df = pd.concat([vision_df, workstyle_df, summary_df], ignore_index=True)\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to ./result/CoT_N-1.csv\n"
     ]
    }
   ],
   "source": [
    "df = create_results_dataframe(vision_results, workstyle_results)\n",
    "df.to_csv(os.path.join(output_dir, csv_file_name), index=False, encoding='utf-8-sig')\n",
    "print(f\"Results saved to {os.path.join(output_dir, csv_file_name)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"{output_dir}/{csv_file_name}\")\n",
    "responses = df[\"response\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_embedding_similarity_and_embeddings(responses, client):\n",
    "    embeddings = []\n",
    "    for response in responses:\n",
    "        embedding_response = client.embeddings.create(\n",
    "            input=response,\n",
    "            model=embed_model\n",
    "        )\n",
    "        embeddings.append(embedding_response.data[0].embedding)\n",
    "\n",
    "    embeddings = np.array(embeddings)\n",
    "    similarity_matrix = cosine_similarity(embeddings)\n",
    "    mean_similarity = similarity_matrix.mean()\n",
    "    \n",
    "    return mean_similarity, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_lexical_similarity(responses):\n",
    "    \"\"\"Calculate lexical overlap between responses\"\"\"\n",
    "    def lexical_overlap(response1, response2):\n",
    "        words1 = set(response1.split())\n",
    "        words2 = set(response2.split())\n",
    "        return len(words1 & words2) / len(words1 | words2)\n",
    "    \n",
    "    lexical_similarities = [\n",
    "        lexical_overlap(responses[i], responses[j])\n",
    "        for i in range(len(responses)) for j in range(i + 1, len(responses))\n",
    "    ]\n",
    "    mean_lexical_similarity = sum(lexical_similarities) / len(lexical_similarities)\n",
    "    return mean_lexical_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_responses(df, client):\n",
    "    \"\"\"Analyze responses for each type (vision and workstyle)\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for response_type in ['vision', 'workstyle', 'summary']:\n",
    "        type_responses = df[df['type'] == response_type]['response'].tolist()\n",
    "        \n",
    "        # Calculate semantic similarity\n",
    "        mean_similarity, embeddings = calculate_embedding_similarity_and_embeddings(type_responses, client)\n",
    "        \n",
    "        # Calculate lexical similarity\n",
    "        mean_lexical_similarity = calculate_lexical_similarity(type_responses)\n",
    "        \n",
    "        results[response_type] = {\n",
    "            'semantic_similarity': mean_similarity,\n",
    "            'lexical_similarity': mean_lexical_similarity,\n",
    "            'embeddings': embeddings\n",
    "        }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n",
      "\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Analyze responses\u001b[39;00m\n",
      "\u001b[0;32m----> 2\u001b[0m analysis_results \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_responses\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Print results for both types\u001b[39;00m\n",
      "\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m response_type, metrics \u001b[38;5;129;01min\u001b[39;00m analysis_results\u001b[38;5;241m.\u001b[39mitems():\n",
      "\n",
      "Cell \u001b[0;32mIn[19], line 12\u001b[0m, in \u001b[0;36manalyze_responses\u001b[0;34m(df, client)\u001b[0m\n",
      "\u001b[1;32m      9\u001b[0m     mean_similarity, embeddings \u001b[38;5;241m=\u001b[39m calculate_embedding_similarity_and_embeddings(type_responses, client)\n",
      "\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Calculate lexical similarity\u001b[39;00m\n",
      "\u001b[0;32m---> 12\u001b[0m     mean_lexical_similarity \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_lexical_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtype_responses\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     14\u001b[0m     results[response_type] \u001b[38;5;241m=\u001b[39m {\n",
      "\u001b[1;32m     15\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msemantic_similarity\u001b[39m\u001b[38;5;124m'\u001b[39m: mean_similarity,\n",
      "\u001b[1;32m     16\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlexical_similarity\u001b[39m\u001b[38;5;124m'\u001b[39m: mean_lexical_similarity,\n",
      "\u001b[1;32m     17\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m'\u001b[39m: embeddings\n",
      "\u001b[1;32m     18\u001b[0m     }\n",
      "\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "\n",
      "Cell \u001b[0;32mIn[18], line 12\u001b[0m, in \u001b[0;36mcalculate_lexical_similarity\u001b[0;34m(responses)\u001b[0m\n",
      "\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(words1 \u001b[38;5;241m&\u001b[39m words2) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(words1 \u001b[38;5;241m|\u001b[39m words2)\n",
      "\u001b[1;32m      8\u001b[0m lexical_similarities \u001b[38;5;241m=\u001b[39m [\n",
      "\u001b[1;32m      9\u001b[0m     lexical_overlap(responses[i], responses[j])\n",
      "\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(responses)) \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(responses))\n",
      "\u001b[1;32m     11\u001b[0m ]\n",
      "\u001b[0;32m---> 12\u001b[0m mean_lexical_similarity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlexical_similarities\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlexical_similarities\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mean_lexical_similarity\n",
      "\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "# Analyze responses\n",
    "analysis_results = analyze_responses(df, client)\n",
    "\n",
    "# Print results for both types\n",
    "for response_type, metrics in analysis_results.items():\n",
    "    print(f\"\\nResults for {response_type.upper()}:\")\n",
    "    print(f\"Semantic Similarity (mean): {metrics['semantic_similarity']:.2f}\")\n",
    "    print(f\"Lexical Overlap (mean): {metrics['lexical_similarity']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
