{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q -U bitsandbytes\n",
    "# %pip install datasets -U\n",
    "# %pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "# %pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "# %pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "# %pip install pandas\n",
    "# %pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "\n",
    "from peft import (\n",
    "    PeftModel,\n",
    "    PeftConfig,\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "\n",
    "from huggingface_hub import login\n",
    "from datasets import load_dataset\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../keys.env\")\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "login(hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_id = \"Bingsu/ko_alpaca_data\" ## \"nlpai-lab/kullm-v2\"\n",
    "model_id = \"beomi/Llama-3-Open-Ko-8B\" ## \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "val_size = 0.005\n",
    "max_seq_len = 256 ## 4098\n",
    "train_on_inputs = True\n",
    "add_eos_token = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(data_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.all_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bos = tokenizer.bos_token_id\n",
    "eos = tokenizer.eos_token_id\n",
    "\n",
    "tokenizer.pad_token_id = eos\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dataset['train'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "rand_idx = randint(0, df.shape[0])\n",
    "sample_row = df.iloc[rand_idx]\n",
    "\n",
    "print(rand_idx)\n",
    "print(f\"input : \\n{sample_row['input']}\\n\\n\")\n",
    "print(f\"instruction : \\n{sample_row['instruction']}\\n\\n\")\n",
    "print(f\"output : \\n{sample_row['output']}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "template = {\n",
    "    \"prompt_input\": \"아래는 문제를 설명하는 지시사항과 구체적인 답변 방식을 요구하는 입력이 함께 있는 문장입니다. 이 요청에 대해 적절하게 답변해주세요.\\n###입력:{input}\\n###지시사항:{instruction}\\n###답변:\",\n",
    "    \"prompt_no_input\": \"아래는 문제를 설명하는 지시사항입니다. 이 요청에 대해 적절하게 답변해주세요.\\n###지시사항:{instruction}\\n###답변:\"\n",
    "}\n",
    "\n",
    "def generate_prompt(instruction : str, input : Union[None, str]=None, label : Union[None, str]=None, verbose : bool = False):\n",
    "    if input:\n",
    "        result = template[\"prompt_input\"].format(instruction=instruction, input=input)\n",
    "    else:\n",
    "        result = template[\"prompt_no_input\"].format(instruction=instruction)\n",
    "\n",
    "    if label:\n",
    "        result = f\"{result}{label}\"\n",
    "    \n",
    "    if verbose:\n",
    "        print(result)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(prompt, max_seq_len, add_eos_token=True):\n",
    "    result = tokenizer(prompt, \n",
    "                       truncation=True, ## 최대 길이를 초과하는 경우 자르기 수행\n",
    "                       max_length=max_seq_len, ## 최대 시퀀스 길이 설정\n",
    "                       padding=False, ## 패딩 비활성화. True로 설정하면 배치 내 모든 시퀀스가 max_length에 맞춰짐\n",
    "                       return_tensors=None)\n",
    "    \n",
    "    ## 토큰화된 시퀀스 데이터의 마지막이 eos 토큰이 아니며, 시퀀스 길이가 max_seq_len보다 작고, add_eos_token이 True일 경우\n",
    "    ## eos_token을 시퀀스 마지막에 추가하며 attention_mask의 마지막에 1을 추가한다.\n",
    "    if(result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "       and len(result[\"input_ids\"]) < max_seq_len\n",
    "       and add_eos_token):\n",
    "        result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "        result[\"attention_mask\"].append(1)\n",
    "\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_tokenize_prompt(data_point, train_on_inputs, max_seq_len, add_eos_token):\n",
    "    ## 원천 데이터를 알파카 프롬프트로 변환.\n",
    "    full_prompt = generate_prompt(data_point[\"instruction\"], data_point[\"input\"], data_point[\"output\"])\n",
    "\n",
    "    ## 토크나이징.\n",
    "    tokenized_full_prompt = tokenize(full_prompt, max_seq_len, add_eos_token)\n",
    "    \n",
    "    ## train_on_inputs == False인 경우 input에서는 손실이 계산되지 않게 함.\n",
    "    ## train_on_inputs == True인 경우 input에서도 손실이 계산(학습)이 되는데 연산량은 증가하지만 성능이 더 좋다는 연구가 있음.\n",
    "    if not train_on_inputs:\n",
    "        user_prompt = generate_prompt(data_point[\"instruction\"], data_point[\"input\"])\n",
    "        \n",
    "        ## eos 토큰을 추가하지 않게 함.(토큰 하나를 줄이기 위함)\n",
    "        tokenized_user_prompt = tokenize(user_prompt, add_eos_token=add_eos_token)\n",
    "\n",
    "        user_prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n",
    "\n",
    "        if add_eos_token:\n",
    "            user_prompt_len -= 1\n",
    "\n",
    "        ## 질문에 해당하는 부분에는 -100을 곱하게 되어 학습을 하지 않게 하고, 정답 부분에서만 손실이 발생되도록 만든다.\n",
    "        tokenized_full_prompt[\"labels\"] = [-100] * user_prompt_len + tokenized_full_prompt[\"labels\"][user_prompt_len:]\n",
    "\n",
    "    return tokenized_full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if val_size > 0:\n",
    "    train_val = dataset[\"train\"].train_test_split(\n",
    "        test_size=val_size, \n",
    "        shuffle=True, \n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    train_data = (\n",
    "        train_val[\"train\"]\n",
    "        .shuffle()\n",
    "        .map(lambda x: generate_and_tokenize_prompt(\n",
    "            x, \n",
    "            train_on_inputs=train_on_inputs, \n",
    "            max_seq_len=max_seq_len,\n",
    "            add_eos_token=add_eos_token\n",
    "        ))\n",
    "    )\n",
    "    \n",
    "    val_data = (\n",
    "        train_val[\"test\"]\n",
    "        .shuffle()\n",
    "        .map(lambda x: generate_and_tokenize_prompt(\n",
    "            x, \n",
    "            train_on_inputs=train_on_inputs, \n",
    "            max_seq_len=max_seq_len,\n",
    "            add_eos_token=add_eos_token\n",
    "        ))\n",
    "    )\n",
    "else:\n",
    "    train_data = (\n",
    "        dataset[\"train\"]\n",
    "        .shuffle()\n",
    "        .map(lambda x: generate_and_tokenize_prompt(\n",
    "            x, \n",
    "            train_on_inputs=train_on_inputs, \n",
    "            max_seq_len=max_seq_len,\n",
    "            add_eos_token=add_eos_token\n",
    "        ))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sequence_lengths(dataset, tokenizer, template):\n",
    "    lengths = []\n",
    "    \n",
    "    for item in dataset['train']:\n",
    "        # Generate full prompt\n",
    "        if item.get('input'):\n",
    "            prompt = template[\"prompt_input\"].format(\n",
    "                instruction=item['instruction'], \n",
    "                input=item['input']\n",
    "            ) + item['output']\n",
    "        else:\n",
    "            prompt = template[\"prompt_no_input\"].format(\n",
    "                instruction=item['instruction']\n",
    "            ) + item['output']\n",
    "            \n",
    "        # Tokenize and get length\n",
    "        tokens = tokenizer(prompt, truncation=False)\n",
    "        lengths.append(len(tokens['input_ids']))\n",
    "    \n",
    "    import numpy as np\n",
    "    \n",
    "    # Calculate statistics\n",
    "    stats = {\n",
    "        'mean': np.mean(lengths),\n",
    "        'median': np.median(lengths),\n",
    "        'p95': np.percentile(lengths, 95),\n",
    "        'p99': np.percentile(lengths, 99),\n",
    "        'max': np.max(lengths),\n",
    "        'min': np.min(lengths)\n",
    "    }\n",
    "    \n",
    "    return stats, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분석 실행\n",
    "stats, lengths = analyze_sequence_lengths(dataset, tokenizer, template)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"Sequence Length Statistics:\")\n",
    "print(f\"Mean: {stats['mean']:.2f}\")\n",
    "print(f\"Median: {stats['median']:.2f}\")\n",
    "print(f\"95th percentile: {stats['p95']:.2f}\")\n",
    "print(f\"99th percentile: {stats['p99']:.2f}\")\n",
    "print(f\"Max: {stats['max']}\")\n",
    "print(f\"Min: {stats['min']}\")\n",
    "\n",
    "# 히스토그램 시각화\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(lengths, bins=50)\n",
    "plt.title('Distribution of Sequence Lengths')\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axvline(stats['p95'], color='r', linestyle='--', label='95th percentile')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, ## 모델 파라미터를 NF4로 양자화.\n",
    "    bnb_4bit_use_double_quant=True, ## 양자화 단계에서 발생하는 상수(scaling factor)도 양자화.\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, ## 연산 단계에서는 bf16으로 dequantize.\n",
    "    bnb_4bit_quant_storage=torch.bfloat16, ## 양자화된 가중치를 저장할 때 사용할 데이터 타입.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config = quantization_config,\n",
    "    torch_dtype = torch.bfloat16,\n",
    "    device_map = {\"\" : 0}\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model) ## 모델 양자화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## adapter를 추가할 선형 모듈들이 무엇이 있는지 탐색.\n",
    "def find_all_linear_names(model):\n",
    "  cls = bnb.nn.Linear4bit\n",
    "  lora_module_names = set()\n",
    "  for name, module in model.named_modules():\n",
    "    if isinstance(module, cls):\n",
    "      names = name.split('.')\n",
    "      lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "  return list(lora_module_names)\n",
    "\n",
    "print('Trainable targer module:',find_all_linear_names(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper parameter setting\n",
    "output_dir=f\"{model_id.replace('/', '_')}-{data_id.replace('/', '_')}-QLoRA\"\n",
    "\n",
    "num_epochs = 1\n",
    "train_batch_size = 4\n",
    "valid_batch_size = 1\n",
    "gradient_accumulation_steps = 16\n",
    "warmup_steps = 10\n",
    "learning_rate = 5e-7\n",
    "group_by_length = False\n",
    "optimizer = 'paged_adamw_8bit'\n",
    "\n",
    "# adam 활용시\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "\n",
    "lr_scheduler = 'cosine'\n",
    "logging_steps = 1\n",
    "\n",
    "use_wandb = True\n",
    "wandb_run_name = 'Single_GPU_Optim'\n",
    "\n",
    "use_fp16 = False\n",
    "use_bf_16 = True\n",
    "evaluation_strategy = 'steps'\n",
    "eval_steps = 50\n",
    "save_steps = 50\n",
    "save_strategy = 'steps'\n",
    "\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    args=TrainingArguments(per_device_train_batch_size=train_batch_size,\n",
    "                           per_device_eval_batch_size=valid_batch_size,\n",
    "                           gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "                           warmup_steps=warmup_steps,\n",
    "                           num_train_epochs=num_epochs,\n",
    "                           learning_rate=learning_rate,\n",
    "                           adam_beta1=beta1,\n",
    "                           adam_beta2=beta2,\n",
    "                           fp16=use_fp16,\n",
    "                           bf16=use_bf_16,\n",
    "                           logging_steps=logging_steps,\n",
    "                           optim=optimizer,\n",
    "                           evaluation_strategy=evaluation_strategy if val_size > 0 else \"no\",\n",
    "                           save_strategy=\"steps\",  #스텝기준으로 save\n",
    "                           eval_steps=eval_steps if val_size > 0 else None,\n",
    "                           save_steps=save_steps,\n",
    "                           lr_scheduler_type=lr_scheduler,\n",
    "                           output_dir=output_dir,\n",
    "                           #save_total_limit = 4,\n",
    "                           load_best_model_at_end=True if val_size > 0 else False ,\n",
    "                           group_by_length=group_by_length,\n",
    "                           report_to=\"wandb\" if use_wandb else None,\n",
    "                           run_name=wandb_run_name if use_wandb else None),\n",
    "\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer, \n",
    "                                         pad_to_multiple_of=8, ## 패딩할 길이를 8의 배수로 맞춤(GPU 연산 최적화를 위해 시퀀스 길이를 8의 배수로 맞춤)\n",
    "                                         return_tensors=\"pt\", \n",
    "                                         padding=True), ## 배치 내 시퀀스들을 동일한 길이로 패딩\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = False\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer가 저장하는 것은 전체 모델 + 어뎁터가 아니라 어뎁터만 저장하는 것이기 때문에 별도로 base_model과 학습된 어뎁터를 병합하는 과정이 필요하다.\n",
    "\n",
    "from peft import PeftModel\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_id, token=hf_token)\n",
    "\n",
    "merged_model = PeftModel.from_pretrained(base_model, output_dir)\n",
    "merged_model = merged_model.merge_and_unload()\n",
    "\n",
    "merged_model.push_to_hub(f\"Pervinco/{output_dir}\")\n",
    "tokenizer.push_to_hub(f\"Pervinco/{output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path):\n",
    "    \"\"\"\n",
    "    학습된 모델과 토크나이저를 로드하는 함수\n",
    "    \"\"\"\n",
    "    # 토크나이저 로드\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    \n",
    "    # 모델 설정 로드\n",
    "    config = PeftConfig.from_pretrained(model_path)\n",
    "    \n",
    "    # 기본 모델 로드\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        config.base_model_name_or_path,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    # LoRA 모델 로드\n",
    "    model = PeftModel.from_pretrained(base_model, model_path)\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def generate_response(instruction, input_text=None, model=None, tokenizer=None, max_length=256):\n",
    "    \"\"\"\n",
    "    주어진 instruction과 input에 대한 응답을 생성하는 함수\n",
    "    \"\"\"\n",
    "    # 프롬프트 템플릿\n",
    "    if input_text:\n",
    "        prompt = f\"아래는 문제를 설명하는 지시사항과 구체적인 답변 방식을 요구하는 입력이 함께 있는 문장입니다. 이 요청에 대해 적절하게 답변해주세요.\\n###입력:{input_text}\\n###지시사항:{instruction}\\n###답변:\"\n",
    "    else:\n",
    "        prompt = f\"아래는 문제를 설명하는 지시사항입니다. 이 요청에 대해 적절하게 답변해주세요.\\n###지시사항:{instruction}\\n###답변:\"\n",
    "    \n",
    "    # 입력 인코딩\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    # 생성 파라미터 설정\n",
    "    generation_config = {\n",
    "        \"max_new_tokens\": max_length,\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.9,\n",
    "        \"repetition_penalty\": 1.1,\n",
    "        \"num_return_sequences\": 1,\n",
    "        \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    }\n",
    "    \n",
    "    # 텍스트 생성\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, **generation_config)\n",
    "    \n",
    "    # 디코딩 및 응답 추출\n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response = full_response.split(\"###답변:\")[-1].strip()\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 경로 설정\n",
    "model_path = \"Pervinco/beomi_Llama-3-Open-Ko-8B-Bingsu_ko_alpaca_data-QLoRA\"\n",
    "\n",
    "# 모델 및 토크나이저 로드\n",
    "print(\"모델을 로딩중입니다...\")\n",
    "model, tokenizer = load_model(model_path)\n",
    "print(\"모델 로딩이 완료되었습니다.\")\n",
    "\n",
    "# 대화형 인터페이스\n",
    "print(\"\\n종료하려면 'quit' 또는 'exit'를 입력하세요.\")\n",
    "while True:\n",
    "    # 사용자 입력 받기\n",
    "    instruction = input(\"\\n지시사항을 입력하세요: \")\n",
    "    if instruction.lower() in ['quit', 'exit']:\n",
    "        break\n",
    "        \n",
    "    input_text = input(\"입력(선택사항, 없으면 엔터): \")\n",
    "    if not input_text:\n",
    "        input_text = None\n",
    "        \n",
    "    # 응답 생성\n",
    "    print(\"\\n응답을 생성중입니다...\")\n",
    "    response = generate_response(instruction, input_text, model, tokenizer)\n",
    "    print(f\"\\n응답: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
