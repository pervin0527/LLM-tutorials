{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers==4.40.1 datasets==2.19.0 huggingface_hub==0.23.0 -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert encoded input : {'input_ids': tensor([[  101,  2054,  2003, 17662, 12172, 19081,  1029,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "bert output : BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.3009,  0.0158,  0.0698,  ..., -0.3406,  0.5976,  0.5820],\n",
      "         [-0.1109,  0.0754, -0.1906,  ...,  0.2970,  0.4278, -0.0391],\n",
      "         [-0.5813, -0.0042,  0.4034,  ..., -0.2549,  0.2216,  0.8121],\n",
      "         ...,\n",
      "         [ 0.9971,  0.3301, -0.0688,  ..., -0.4873,  0.0168, -0.0345],\n",
      "         [-0.2394, -0.0573, -0.5885,  ..., -0.0415,  0.3123, -0.0288],\n",
      "         [ 0.7884,  0.4039,  0.0217,  ...,  0.3869, -0.4785, -0.4116]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-8.7247e-01, -3.1464e-01, -5.2733e-01,  7.3685e-01,  1.8460e-01,\n",
      "         -1.8295e-01,  8.9985e-01,  2.9762e-01, -3.9314e-01, -9.9994e-01,\n",
      "          7.1272e-02,  7.2618e-01,  9.7232e-01,  3.2968e-01,  9.1369e-01,\n",
      "         -7.5043e-01, -3.5754e-01, -5.6530e-01,  2.8968e-01, -6.4040e-01,\n",
      "          6.0199e-01,  9.9856e-01,  3.8675e-01,  2.9078e-01,  4.0431e-01,\n",
      "          7.7010e-01, -7.6515e-01,  9.1830e-01,  9.4834e-01,  7.2557e-01,\n",
      "         -7.3735e-01,  1.9867e-01, -9.8172e-01, -1.8334e-01, -4.9430e-01,\n",
      "         -9.8059e-01,  2.6051e-01, -7.4618e-01,  4.8118e-02,  5.9711e-02,\n",
      "         -8.1904e-01,  2.1572e-01,  9.9974e-01, -6.2062e-01,  1.3180e-02,\n",
      "         -3.7181e-01, -1.0000e+00,  2.5721e-01, -8.5921e-01,  3.5142e-01,\n",
      "          4.2221e-01,  2.7743e-02,  1.5075e-01,  3.6331e-01,  4.4037e-01,\n",
      "          2.4008e-02, -1.7561e-01,  6.5655e-02, -2.2944e-01, -5.1775e-01,\n",
      "         -6.1054e-01,  4.0539e-01, -4.9568e-01, -8.7416e-01,  5.6503e-01,\n",
      "          5.4685e-01, -9.5151e-02, -2.8588e-01, -6.3585e-02, -5.9148e-02,\n",
      "          8.8904e-01,  1.0488e-01,  2.8848e-01, -8.3510e-01,  1.3450e-01,\n",
      "          2.2096e-01, -6.5445e-01,  1.0000e+00, -5.5778e-01, -9.5749e-01,\n",
      "          4.4432e-01,  5.2114e-01,  5.8760e-01,  2.3122e-02,  3.6001e-01,\n",
      "         -1.0000e+00,  2.6260e-01, -6.2052e-02, -9.8500e-01,  2.5122e-01,\n",
      "          4.3635e-01, -1.6588e-01,  3.9133e-01,  5.3988e-01, -5.7578e-01,\n",
      "         -1.7328e-01, -8.3042e-02, -3.6534e-01, -1.3268e-01, -1.2891e-01,\n",
      "          2.1034e-02, -2.0756e-01, -2.4547e-01, -3.3774e-01,  2.4423e-01,\n",
      "         -4.7804e-01, -4.5389e-01,  3.9510e-01,  7.4779e-02,  6.1011e-01,\n",
      "          5.0728e-01, -3.0130e-01,  3.7723e-01, -9.3758e-01,  4.9906e-01,\n",
      "         -3.2958e-01, -9.8185e-01, -6.1150e-01, -9.7898e-01,  6.7381e-01,\n",
      "         -6.0384e-02, -2.6210e-01,  9.3328e-01,  1.9747e-01,  3.5069e-01,\n",
      "         -7.0665e-02, -3.4704e-01, -1.0000e+00, -2.7822e-01, -3.7445e-01,\n",
      "         -1.7645e-01, -2.1203e-01, -9.6432e-01, -9.4622e-01,  5.7054e-01,\n",
      "          9.4001e-01,  1.8941e-01,  9.9923e-01, -1.5256e-01,  9.1510e-01,\n",
      "          7.8500e-02, -5.6503e-01, -7.2292e-02, -4.5555e-01,  5.5808e-01,\n",
      "          1.8401e-01, -6.0199e-01,  2.3481e-01, -3.3688e-02, -2.7570e-01,\n",
      "         -5.3239e-01, -1.7641e-01, -4.0169e-01, -9.2079e-01, -3.1998e-01,\n",
      "          9.3809e-01, -7.1152e-02, -6.9783e-01,  6.7254e-02, -2.0274e-01,\n",
      "         -3.7808e-01,  8.4576e-01,  5.0007e-01,  3.6423e-01, -4.1740e-01,\n",
      "          3.6109e-01,  1.4876e-01,  3.5860e-01, -8.4838e-01,  2.6076e-01,\n",
      "          4.1682e-01, -3.1267e-01, -5.2371e-01, -9.6102e-01, -2.9391e-01,\n",
      "          5.4447e-01,  9.8303e-01,  7.4894e-01,  2.8395e-01,  1.4951e-01,\n",
      "         -2.6745e-01,  1.8008e-01, -9.3940e-01,  9.6656e-01, -1.0958e-01,\n",
      "          1.8235e-01,  1.6497e-01,  2.4865e-01, -8.2815e-01,  7.0423e-04,\n",
      "          8.1064e-01, -1.2264e-02, -8.1630e-01, -4.3830e-02, -4.7633e-01,\n",
      "         -3.8495e-01, -3.8118e-01,  4.5080e-01, -1.9782e-01, -3.4960e-01,\n",
      "          1.1252e-01,  8.9026e-01,  9.7578e-01,  6.9775e-01, -1.2651e-01,\n",
      "          5.7326e-01, -8.3355e-01, -4.0410e-01,  5.5684e-02,  1.7855e-01,\n",
      "          8.9321e-02,  9.8696e-01, -3.9308e-01, -7.7631e-02, -9.1122e-01,\n",
      "         -9.7244e-01,  5.5039e-03, -8.4574e-01, -2.4717e-02, -6.3462e-01,\n",
      "          4.9246e-01,  5.5000e-01,  1.1419e-01,  3.0862e-01, -9.6484e-01,\n",
      "         -7.7138e-01,  3.5477e-01, -2.1973e-01,  4.2417e-01, -2.4453e-01,\n",
      "          6.6965e-01,  6.5434e-01, -6.3527e-01,  8.0241e-01,  9.1417e-01,\n",
      "         -4.4787e-01, -6.7925e-01,  8.3256e-01, -2.6945e-01,  8.7207e-01,\n",
      "         -5.2281e-01,  9.8420e-01,  4.2696e-01,  4.8134e-01, -8.7702e-01,\n",
      "         -5.7352e-01, -8.7672e-01, -3.6707e-01,  7.0803e-02, -1.4201e-01,\n",
      "          6.3290e-01,  6.3346e-01,  2.9797e-01,  4.1055e-01, -6.0411e-01,\n",
      "          9.9386e-01, -7.6779e-01, -9.4760e-01,  2.8060e-01,  1.7030e-02,\n",
      "         -9.8417e-01,  3.4940e-01,  2.5224e-01, -4.0932e-01, -3.8472e-01,\n",
      "         -6.1061e-01, -9.4425e-01,  8.4996e-01,  1.8030e-01,  9.8473e-01,\n",
      "         -8.3625e-02, -8.9589e-01, -4.1394e-01, -8.9448e-01, -2.8790e-01,\n",
      "         -1.7622e-01,  1.2934e-01, -1.4377e-01, -9.4701e-01,  4.8915e-01,\n",
      "          5.5083e-01,  3.5019e-01, -5.7310e-01,  9.9580e-01,  9.9998e-01,\n",
      "          9.5247e-01,  8.9147e-01,  8.6635e-01, -9.9394e-01, -3.3778e-01,\n",
      "          9.9998e-01, -9.1078e-01, -1.0000e+00, -9.3294e-01, -5.2658e-01,\n",
      "          3.7828e-01, -1.0000e+00, -3.1776e-01,  7.3381e-02, -8.6886e-01,\n",
      "          3.6298e-01,  9.6532e-01,  9.8247e-01, -1.0000e+00,  8.0954e-01,\n",
      "          9.0122e-01, -6.6414e-01,  6.7350e-01, -3.1281e-01,  9.6308e-01,\n",
      "          5.0662e-01,  4.2693e-01, -2.2366e-01,  3.7596e-01, -8.1644e-01,\n",
      "         -8.3308e-01, -1.3696e-01, -4.6714e-01,  9.4581e-01,  1.8325e-01,\n",
      "         -7.8852e-01, -8.7354e-01, -5.8153e-02, -6.9013e-02, -3.4173e-01,\n",
      "         -9.4760e-01, -1.7820e-01, -1.3929e-01,  6.8355e-01,  4.5169e-02,\n",
      "          2.6814e-01, -6.8768e-01,  2.1210e-01, -2.3396e-01,  3.4735e-01,\n",
      "          6.8611e-01, -9.2963e-01, -5.2597e-01,  3.0077e-01, -3.7891e-01,\n",
      "         -4.7398e-01, -9.5464e-01,  9.4878e-01, -3.2371e-01,  3.0548e-01,\n",
      "          1.0000e+00, -1.6540e-01, -7.9525e-01,  5.4072e-01,  1.6751e-01,\n",
      "         -1.2475e-01,  1.0000e+00,  6.4986e-01, -9.5677e-01, -6.0413e-01,\n",
      "          3.7020e-01, -4.8581e-01, -4.7777e-01,  9.9866e-01, -2.5729e-01,\n",
      "         -2.4091e-01,  1.1121e-01,  9.6287e-01, -9.8189e-01,  9.3767e-01,\n",
      "         -8.7587e-01, -9.4305e-01,  9.3710e-01,  9.0091e-01, -3.8187e-01,\n",
      "         -6.1976e-01,  1.4977e-01, -5.8692e-01,  2.6118e-01, -9.5406e-01,\n",
      "          6.2279e-01,  4.8689e-01, -8.8539e-02,  8.5752e-01, -8.3492e-01,\n",
      "         -5.3720e-01,  2.5680e-01, -4.1266e-01,  2.2846e-01,  5.8122e-01,\n",
      "          4.3429e-01, -2.4578e-01,  1.2280e-01, -2.9919e-01, -3.1362e-01,\n",
      "         -9.6913e-01,  6.2339e-02,  1.0000e+00,  1.3628e-02, -1.2817e-01,\n",
      "         -3.8745e-01,  1.8794e-02, -1.8447e-01,  4.2568e-01,  5.1443e-01,\n",
      "         -2.2532e-01, -8.0441e-01,  1.4079e-01, -9.4743e-01, -9.8253e-01,\n",
      "          7.6432e-01,  1.4451e-01, -3.2291e-01,  9.9988e-01,  3.1823e-01,\n",
      "          1.5380e-01,  1.3672e-01,  7.6152e-01,  1.0233e-01,  5.8834e-01,\n",
      "          5.5959e-01,  9.6941e-01, -2.7468e-01,  6.1814e-01,  8.2141e-01,\n",
      "         -5.6554e-01, -2.7680e-01, -6.2172e-01, -8.9432e-02, -9.0074e-01,\n",
      "          1.2009e-01, -9.2751e-01,  9.4329e-01,  4.9252e-01,  2.7484e-01,\n",
      "          2.1843e-01,  6.6065e-02,  1.0000e+00, -2.2593e-01,  6.4518e-01,\n",
      "         -5.1134e-01,  8.2547e-01, -9.8957e-01, -7.8994e-01, -3.4983e-01,\n",
      "         -8.3618e-02, -3.6136e-01, -2.2800e-01,  2.0661e-01, -9.5846e-01,\n",
      "          3.5442e-01,  9.7109e-02, -9.7417e-01, -9.8264e-01,  2.9154e-01,\n",
      "          8.3737e-01,  6.6634e-02, -8.0501e-01, -6.1920e-01, -5.9091e-01,\n",
      "          1.7083e-01, -1.7838e-01, -9.0619e-01,  2.4375e-01, -1.7610e-01,\n",
      "          4.1765e-01, -2.2214e-01,  5.1794e-01,  5.8822e-01,  6.4995e-01,\n",
      "          1.3124e-01, -9.7189e-02, -8.3228e-02, -8.4713e-01,  8.4501e-01,\n",
      "         -7.9905e-01, -4.0446e-01, -2.0988e-01,  1.0000e+00, -5.1438e-01,\n",
      "          6.3233e-01,  7.9540e-01,  6.0073e-01, -1.3826e-01,  2.1753e-01,\n",
      "          6.5091e-01,  1.4618e-01, -6.2026e-01, -4.0101e-01, -7.0176e-01,\n",
      "         -3.3691e-01,  6.5601e-01, -8.1212e-02,  4.6828e-01,  6.5125e-01,\n",
      "          4.2083e-01,  1.4356e-01, -3.9187e-02, -4.4730e-02,  9.9797e-01,\n",
      "         -1.2616e-01,  5.6322e-02, -5.0923e-01,  7.9204e-03, -3.3530e-01,\n",
      "         -4.9021e-01,  1.0000e+00,  2.7262e-01, -1.3420e-01, -9.8275e-01,\n",
      "         -3.7159e-01, -8.9745e-01,  9.9994e-01,  8.0115e-01, -7.2103e-01,\n",
      "          6.0988e-01,  2.2475e-01, -1.7136e-01,  7.3667e-01, -1.3005e-01,\n",
      "         -2.5390e-01,  2.7347e-01,  9.6592e-02,  9.2964e-01, -4.7674e-01,\n",
      "         -9.5305e-01, -6.8485e-01,  3.5561e-01, -9.3918e-01,  9.9616e-01,\n",
      "         -4.4200e-01, -1.8582e-01, -4.1399e-01,  4.5030e-01,  6.3904e-01,\n",
      "         -1.7178e-01, -9.7449e-01, -5.1296e-02,  1.6738e-01,  9.3006e-01,\n",
      "          1.3585e-01, -5.9081e-01, -9.1515e-01,  1.1425e-01,  4.2950e-01,\n",
      "         -5.2346e-01, -8.7475e-01,  9.4331e-01, -9.7478e-01,  3.9603e-01,\n",
      "          1.0000e+00,  3.9404e-01, -6.0146e-01,  2.9373e-02, -5.1601e-01,\n",
      "          2.4861e-01,  2.4923e-01,  6.7270e-01, -9.2070e-01, -2.8287e-01,\n",
      "         -7.6749e-02,  1.7067e-01, -8.0915e-02,  1.2053e-01,  5.9641e-01,\n",
      "          1.7944e-01, -5.4069e-01, -5.6923e-01, -2.5089e-02,  4.6722e-01,\n",
      "          8.3220e-01, -3.5286e-01, -1.6219e-01,  9.4050e-02, -9.4970e-02,\n",
      "         -8.7677e-01, -2.7396e-01, -2.1794e-01, -9.9938e-01,  7.5556e-01,\n",
      "         -1.0000e+00, -1.7111e-01, -1.8512e-01, -2.3656e-01,  7.9198e-01,\n",
      "          2.8102e-01,  2.7825e-01, -6.7673e-01, -5.1072e-01,  6.6396e-01,\n",
      "          6.7959e-01, -1.5748e-01,  1.9009e-01, -6.7916e-01,  1.1148e-01,\n",
      "         -4.1107e-03, -7.4857e-03, -1.5137e-01,  8.2224e-01, -1.6503e-01,\n",
      "          1.0000e+00,  1.4966e-01, -6.3775e-01, -9.6420e-01,  2.2951e-01,\n",
      "         -2.5219e-01,  1.0000e+00, -9.1262e-01, -9.3021e-01,  2.5156e-01,\n",
      "         -6.5498e-01, -8.3972e-01,  2.0794e-01,  1.4597e-01, -5.7221e-01,\n",
      "         -5.5899e-01,  9.3297e-01,  9.1276e-01, -6.3061e-01,  3.3351e-01,\n",
      "         -3.3155e-01, -3.4310e-01,  5.5119e-02,  1.7480e-01,  9.7497e-01,\n",
      "          1.8068e-01,  8.6350e-01,  4.1295e-01,  6.8414e-02,  9.4591e-01,\n",
      "          2.3672e-01,  4.7458e-01,  5.9380e-02,  1.0000e+00,  2.6528e-01,\n",
      "         -8.8648e-01,  3.4260e-01, -9.7573e-01, -1.5098e-01, -9.4319e-01,\n",
      "          2.2633e-01,  2.1388e-01,  8.7123e-01, -9.6188e-02,  9.3925e-01,\n",
      "          2.4525e-02, -9.8376e-02, -1.0091e-01,  1.4681e-01,  3.8430e-01,\n",
      "         -8.7014e-01, -9.7812e-01, -9.7528e-01,  3.6960e-01, -3.8164e-01,\n",
      "         -4.7706e-02,  1.4460e-01,  1.0588e-01,  3.0843e-01,  4.1187e-01,\n",
      "         -1.0000e+00,  9.1188e-01,  3.6028e-01,  7.0124e-01,  9.3118e-01,\n",
      "          5.1771e-01,  2.7587e-01,  2.8873e-01, -9.7848e-01, -9.4348e-01,\n",
      "         -3.1992e-01, -2.0700e-01,  6.7060e-01,  6.4204e-01,  8.7050e-01,\n",
      "          3.0572e-01, -4.9177e-01, -2.6046e-01,  1.0768e-01, -5.5766e-01,\n",
      "         -9.8769e-01,  3.4500e-01, -1.2493e-01, -9.4611e-01,  9.4435e-01,\n",
      "         -2.2333e-01, -1.4135e-01,  1.9776e-01, -3.9046e-01,  9.3880e-01,\n",
      "          6.8928e-01,  3.8767e-01,  1.1025e-01,  5.1922e-01,  8.3273e-01,\n",
      "          9.4730e-01,  9.7341e-01, -5.7427e-01,  7.8560e-01, -6.8621e-02,\n",
      "          4.1192e-01,  6.9466e-01, -9.3774e-01,  2.3971e-01,  1.3474e-01,\n",
      "         -4.2771e-01,  1.6046e-01, -2.4939e-01, -9.7068e-01,  5.0688e-01,\n",
      "         -2.9860e-01,  5.1771e-01, -3.1591e-01,  1.0868e-01, -3.8842e-01,\n",
      "         -1.5632e-01, -6.4045e-01, -5.4585e-01,  6.6964e-01,  3.3962e-01,\n",
      "          8.5658e-01,  5.4788e-01, -1.0526e-01, -5.1888e-01, -1.6210e-01,\n",
      "         -5.4360e-01, -8.8313e-01,  8.7812e-01,  4.3411e-02, -2.1279e-01,\n",
      "          3.2289e-01, -1.8518e-01,  6.5461e-01, -6.4463e-02, -3.7191e-01,\n",
      "         -3.7232e-01, -7.5502e-01,  8.0540e-01, -1.7699e-01, -5.2501e-01,\n",
      "         -5.6918e-01,  3.7711e-01,  3.8391e-01,  9.9864e-01, -5.3484e-01,\n",
      "         -5.4315e-01, -7.2384e-02, -2.4831e-01,  4.2074e-01, -2.2461e-01,\n",
      "         -1.0000e+00,  2.4747e-01,  7.5572e-02,  3.7574e-01, -7.1563e-02,\n",
      "          3.3389e-01, -1.6045e-01, -9.6561e-01, -1.6314e-01,  7.3813e-02,\n",
      "          3.5186e-01, -4.4838e-01, -2.2300e-01,  5.7876e-01,  6.1218e-01,\n",
      "          7.0052e-01,  8.2005e-01,  1.1350e-01,  2.0501e-01,  6.4786e-01,\n",
      "         -2.2238e-01, -6.3388e-01,  8.9791e-01]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pervinco/miniconda3/envs/nlp-project/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "text = \"What is Huggingface Transformers?\"\n",
    "\n",
    "bert_model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "encoded_input = bert_tokenizer(text, return_tensors=\"pt\")\n",
    "print(f\"bert encoded input : {encoded_input}\")\n",
    "\n",
    "bert_output = bert_model(**encoded_input)\n",
    "print(f\"bert output : {bert_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2 encoded input : {'input_ids': tensor([[ 2061,   318, 12905,  2667,  2550, 39185,    30]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n",
      "gpt2 output : BaseModelOutputWithPastAndCrossAttentions(last_hidden_state=tensor([[[-0.1643,  0.0957, -0.2844,  ..., -0.1632, -0.0774, -0.2154],\n",
      "         [-0.1234,  0.0442,  0.0240,  ..., -0.2752,  0.1524,  0.3333],\n",
      "         [ 0.2047, -0.3868, -0.1290,  ..., -0.6443,  0.0370,  0.0601],\n",
      "         ...,\n",
      "         [-0.8368,  0.3412, -0.6933,  ..., -0.2577,  0.3119, -0.1379],\n",
      "         [-0.6274, -0.0040, -0.4727,  ..., -0.4236,  0.3078, -0.0780],\n",
      "         [-0.3692, -0.4871, -0.3275,  ..., -0.2425, -0.3942, -0.0420]]],\n",
      "       grad_fn=<ViewBackward0>), past_key_values=((tensor([[[[-1.3190e+00,  1.8644e+00,  8.9757e-01,  ..., -1.4033e+00,\n",
      "           -2.3651e-01,  1.2896e+00],\n",
      "          [-1.8348e+00,  2.4955e+00,  1.7497e+00,  ..., -1.5397e+00,\n",
      "           -2.3685e+00,  2.4482e+00],\n",
      "          [-1.5084e+00,  2.5453e+00,  1.3205e+00,  ..., -2.3020e+00,\n",
      "           -1.0723e+00,  1.5726e+00],\n",
      "          ...,\n",
      "          [-2.3563e+00,  1.6290e+00,  2.0251e+00,  ..., -1.6290e+00,\n",
      "           -5.7846e-01,  2.3659e+00],\n",
      "          [-1.5984e-01,  1.5552e+00,  4.4775e-01,  ..., -9.6080e-02,\n",
      "           -1.6245e+00,  6.5023e-01],\n",
      "          [-2.5907e+00,  2.8342e+00,  1.9796e+00,  ..., -1.0663e+00,\n",
      "           -2.3898e+00,  1.9323e+00]],\n",
      "\n",
      "         [[-3.7719e-01,  4.4023e-01, -6.4755e-01,  ..., -3.9102e-01,\n",
      "            2.5417e+00,  1.0485e+00],\n",
      "          [ 6.7388e-01, -1.3429e+00, -1.0824e-01,  ..., -3.4649e+00,\n",
      "            3.4113e+00,  9.9918e-01],\n",
      "          [-2.3680e+00, -1.5786e+00, -1.8804e+00,  ...,  2.0171e+00,\n",
      "            4.0580e+00,  3.4980e+00],\n",
      "          ...,\n",
      "          [ 8.2783e-01, -9.9290e-01, -6.6732e-02,  ...,  2.3606e-01,\n",
      "            2.1066e+00,  1.6495e+00],\n",
      "          [-8.2169e-01,  1.8334e-02,  4.0507e-01,  ...,  3.0979e-01,\n",
      "            4.4831e+00, -1.2077e-01],\n",
      "          [-2.8257e-01, -1.0342e+00, -6.8794e-02,  ..., -1.8987e+00,\n",
      "            2.5742e+00,  9.3441e-02]],\n",
      "\n",
      "         [[ 2.2048e-02, -7.5673e-02,  8.3024e-01,  ..., -1.4517e+00,\n",
      "           -1.6848e+00,  8.1629e-01],\n",
      "          [ 4.3450e-01,  2.0733e-01,  3.2424e-01,  ..., -2.4565e+00,\n",
      "            1.1946e-01,  1.9386e+00],\n",
      "          [ 1.0572e+00, -2.0914e+00, -1.7248e-02,  ..., -2.9289e+00,\n",
      "           -1.7070e+00,  1.4309e+00],\n",
      "          ...,\n",
      "          [ 3.4590e-01, -8.5831e-01,  1.7898e-01,  ..., -3.1062e+00,\n",
      "           -6.5091e-01,  1.3285e+00],\n",
      "          [ 3.3449e+00,  2.1424e-01, -1.0417e+00,  ..., -1.8506e+00,\n",
      "           -2.0041e+00,  3.1076e-01],\n",
      "          [ 6.2279e-01,  7.0403e-01,  1.1326e+00,  ..., -2.0606e+00,\n",
      "            1.5596e+00,  1.8599e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.3110e-01, -4.6927e-02, -1.3321e-01,  ...,  3.7510e-01,\n",
      "            6.8562e-01,  5.7607e-01],\n",
      "          [ 2.5121e-01,  1.1030e-01, -2.8209e-02,  ...,  8.2454e-01,\n",
      "            3.1058e-01,  5.9335e-01],\n",
      "          [ 9.5926e-01,  8.9731e-01, -1.4581e+00,  ...,  7.9989e-01,\n",
      "            4.7008e-01,  2.4543e-01],\n",
      "          ...,\n",
      "          [-1.0234e+00,  6.2903e-01,  1.8656e-01,  ...,  1.6453e+00,\n",
      "            7.9949e-01,  1.2545e+00],\n",
      "          [-1.4651e+00, -4.4232e-01,  8.6083e-01,  ..., -9.6581e-02,\n",
      "            2.5770e-01,  1.6385e+00],\n",
      "          [ 2.9037e-02, -1.3610e-02,  2.2313e-01,  ...,  1.0997e+00,\n",
      "            4.2422e-01,  4.5301e-01]],\n",
      "\n",
      "         [[ 1.4246e+00,  1.3487e+00, -2.4757e-01,  ..., -2.8181e-01,\n",
      "            9.5264e-01, -1.1325e+00],\n",
      "          [ 8.5059e-01,  6.2667e-01, -7.5365e-01,  ..., -9.0627e-01,\n",
      "            8.3284e-01, -5.7886e-01],\n",
      "          [ 3.6234e-01,  1.0985e-01, -2.6956e-01,  ..., -1.1915e+00,\n",
      "            2.1922e+00, -1.1695e-01],\n",
      "          ...,\n",
      "          [ 4.1117e-01, -7.7677e-01, -3.7013e-01,  ..., -4.6709e-01,\n",
      "            9.2535e-01, -1.2913e-01],\n",
      "          [ 5.0988e-01, -1.5749e-01, -2.6997e-01,  ..., -5.7358e-01,\n",
      "            5.6265e-01, -7.2300e-01],\n",
      "          [ 8.7646e-01,  7.2043e-02, -4.5450e-01,  ..., -6.8621e-01,\n",
      "            7.1214e-01,  4.3558e-01]],\n",
      "\n",
      "         [[ 4.9501e-01,  2.0561e-01, -7.4766e-02,  ..., -3.4694e-01,\n",
      "            1.5860e-01,  1.7998e+00],\n",
      "          [ 1.2434e+00,  6.7664e-02,  6.1694e-02,  ...,  7.0984e-01,\n",
      "            5.2591e-01,  1.7975e+00],\n",
      "          [ 2.2218e-01, -4.0843e-01, -8.9828e-01,  ..., -1.8797e-03,\n",
      "            4.7362e-01,  1.2197e+00],\n",
      "          ...,\n",
      "          [ 6.1422e-01,  1.3368e-01,  4.6723e-02,  ...,  1.9303e-01,\n",
      "            3.2994e-01,  1.7550e+00],\n",
      "          [-2.6335e-01, -2.9988e-01, -5.5705e-01,  ..., -4.7819e-01,\n",
      "            7.8153e-01,  1.5978e+00],\n",
      "          [-1.2731e-01,  1.1539e+00,  1.5019e+00,  ..., -8.0560e-01,\n",
      "            1.2781e-01,  9.0954e-01]]]], grad_fn=<PermuteBackward0>), tensor([[[[ 2.3089e-02,  8.2066e-02,  2.7945e-02,  ...,  2.6097e-02,\n",
      "           -2.3956e-02,  9.6702e-02],\n",
      "          [ 2.3196e-01, -2.3861e-01,  2.9457e-01,  ..., -5.8200e-02,\n",
      "            7.4808e-02, -8.7832e-02],\n",
      "          [ 1.7246e-01, -2.9933e-01,  2.6434e-02,  ..., -1.4364e-01,\n",
      "            8.6979e-02, -2.5634e-01],\n",
      "          ...,\n",
      "          [-1.6325e-01, -6.3143e-01, -8.7025e-02,  ...,  1.5842e-01,\n",
      "            1.6753e-01, -9.8523e-02],\n",
      "          [ 3.8947e-01,  3.3975e-01, -5.1550e-02,  ..., -1.7153e-01,\n",
      "            5.4001e-02, -1.8370e-01],\n",
      "          [-4.9209e-02,  5.5642e-02, -2.2348e-01,  ...,  1.5487e-01,\n",
      "           -2.8182e-01, -2.5348e-02]],\n",
      "\n",
      "         [[ 4.7284e-01,  1.1574e-01, -2.9770e-01,  ..., -6.3272e-01,\n",
      "           -2.1164e-01,  1.9056e-01],\n",
      "          [ 5.5125e-01,  7.0201e-02,  1.0743e-01,  ...,  1.1353e-02,\n",
      "            2.2987e-01, -7.8098e-02],\n",
      "          [ 4.3007e-01,  2.1730e-01, -4.5603e-01,  ...,  5.5595e-02,\n",
      "           -4.0139e-01,  6.2129e-02],\n",
      "          ...,\n",
      "          [ 2.1241e-01,  2.1841e-01, -1.9118e-01,  ...,  2.3870e-01,\n",
      "           -1.4345e-01,  3.6011e-02],\n",
      "          [ 4.7959e-02, -6.1850e-02,  1.0105e-01,  ...,  2.6298e-01,\n",
      "            1.1248e-01,  3.4883e-02],\n",
      "          [ 3.9379e-01,  3.1259e-02, -4.5614e-02,  ..., -1.8704e-01,\n",
      "            5.5764e-01, -2.4426e-01]],\n",
      "\n",
      "         [[ 7.3907e-02, -1.1804e-01,  8.6928e-02,  ..., -7.0653e-03,\n",
      "           -1.7059e-04, -6.2186e-02],\n",
      "          [ 9.5054e-02,  9.0989e-02,  5.3470e-02,  ..., -2.7077e-02,\n",
      "            9.1225e-02,  1.0601e-01],\n",
      "          [-8.5024e-02, -4.1767e-02,  4.1371e-01,  ..., -5.0289e-03,\n",
      "           -1.7775e-01, -1.4934e-01],\n",
      "          ...,\n",
      "          [ 3.9767e-03, -2.3647e-01, -1.5363e-02,  ...,  4.0441e-02,\n",
      "           -9.0315e-02,  2.1643e-01],\n",
      "          [ 2.6616e-01,  9.7383e-04,  2.9607e-01,  ...,  4.6613e-02,\n",
      "            1.1808e-01, -2.1727e-01],\n",
      "          [-2.3431e-01,  2.0921e-01, -7.1643e-02,  ..., -2.0041e-03,\n",
      "           -7.0139e-02,  2.0161e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-6.8233e-02,  7.8248e-02,  8.7423e-02,  ..., -1.5830e-01,\n",
      "            8.4335e-02, -6.9996e-02],\n",
      "          [-4.7730e-01,  5.3618e-01,  1.1470e-01,  ...,  2.1382e-01,\n",
      "           -4.0779e-01,  1.0846e-01],\n",
      "          [-2.2120e-01, -3.9845e-01, -7.9989e-02,  ...,  1.9387e-01,\n",
      "            3.3910e-01,  6.0269e-02],\n",
      "          ...,\n",
      "          [ 3.0942e-01, -1.5688e-01,  3.6059e-01,  ...,  1.4230e-01,\n",
      "            1.8511e-01,  2.6393e-01],\n",
      "          [-6.8671e-01,  2.2616e-01, -4.3543e-01,  ..., -2.6103e-01,\n",
      "            4.4166e-02, -1.9647e-01],\n",
      "          [ 3.0716e-02,  3.2629e-01,  8.4872e-02,  ..., -5.5610e-02,\n",
      "            1.9845e-02,  1.8047e-01]],\n",
      "\n",
      "         [[ 1.7370e-02, -8.3901e-02, -2.1068e-01,  ...,  1.3147e-01,\n",
      "            2.3579e-01, -4.4249e-02],\n",
      "          [-3.5043e-01,  7.0391e-02,  2.1141e-01,  ..., -6.4529e-01,\n",
      "           -1.4333e-01,  8.3923e-02],\n",
      "          [ 1.5684e-01, -2.5725e-01,  4.1375e-01,  ..., -9.6985e-02,\n",
      "            7.1628e-02,  6.8391e-02],\n",
      "          ...,\n",
      "          [-4.4232e-01, -1.7930e-01,  2.2012e-01,  ..., -8.9179e-03,\n",
      "            5.6077e-02,  2.4827e-01],\n",
      "          [-1.1280e-01,  1.0156e-01,  5.6393e-02,  ...,  3.5989e-01,\n",
      "            1.4149e-02, -7.6634e-01],\n",
      "          [-6.2178e-02,  1.3429e-01, -1.7119e-02,  ..., -3.3925e-02,\n",
      "            6.2164e-02, -9.5420e-02]],\n",
      "\n",
      "         [[ 7.5052e-02, -4.4469e-01,  1.6571e-01,  ...,  7.0882e-03,\n",
      "           -2.7171e-01, -8.7654e-02],\n",
      "          [-5.8485e-02, -1.2240e-02, -3.8542e-02,  ...,  1.7494e-01,\n",
      "            3.7288e-01,  1.2658e-01],\n",
      "          [-1.9885e-01,  3.4041e-01, -1.6553e-01,  ...,  1.1601e-01,\n",
      "            1.0526e-02,  1.9891e-01],\n",
      "          ...,\n",
      "          [-8.6604e-02, -1.5700e-02, -3.0828e-02,  ..., -1.2051e-01,\n",
      "            3.7557e-01,  2.5741e-01],\n",
      "          [-8.4689e-02,  2.3201e-01, -1.1719e-02,  ...,  4.8740e-01,\n",
      "           -2.1733e-01, -4.8569e-02],\n",
      "          [ 2.7541e-01, -3.0071e-02,  1.5139e-03,  ...,  3.5108e-02,\n",
      "            2.6774e-01, -2.6701e-01]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[-1.5472e-01,  1.6268e+00, -1.8836e+00,  ...,  1.4842e+00,\n",
      "           -1.3666e+00,  2.1387e-01],\n",
      "          [ 9.7150e-01,  2.6372e+00, -1.7462e+00,  ..., -6.0442e-01,\n",
      "           -2.1933e+00,  4.0203e-01],\n",
      "          [ 8.0066e-02,  1.9074e+00, -7.2478e-01,  ...,  9.6469e-02,\n",
      "           -1.9864e+00, -7.3168e-01],\n",
      "          ...,\n",
      "          [ 2.2190e-01,  9.0340e-01, -9.0920e-01,  ..., -5.3192e-02,\n",
      "           -2.4278e+00,  2.7949e-01],\n",
      "          [-2.1125e-01,  1.8963e+00, -1.1786e+00,  ...,  4.3981e-02,\n",
      "           -1.7371e+00, -2.6591e-01],\n",
      "          [ 1.9873e-01,  8.1230e-01, -8.0648e-01,  ...,  1.8519e-01,\n",
      "           -7.7891e-01, -3.0349e-03]],\n",
      "\n",
      "         [[-9.3697e-01, -2.7260e-01, -5.7192e-01,  ..., -9.0300e-02,\n",
      "            5.2307e-01, -8.0643e-01],\n",
      "          [-5.9683e-01,  3.3612e-01, -1.4138e+00,  ..., -7.0285e-01,\n",
      "            5.1227e-01, -3.1268e-01],\n",
      "          [-6.3326e-01,  4.6026e-01, -8.4958e-01,  ...,  5.9056e-01,\n",
      "            2.8729e-01, -3.3945e-02],\n",
      "          ...,\n",
      "          [-4.9147e-02, -3.0440e-02, -2.1093e+00,  ..., -6.3451e-01,\n",
      "           -2.0808e-01,  3.7564e-03],\n",
      "          [-2.7341e-01,  2.4920e-01, -9.5016e-01,  ..., -7.3025e-02,\n",
      "           -6.1457e-01, -7.5780e-01],\n",
      "          [-5.2475e-01,  1.3551e+00, -1.5672e+00,  ..., -5.3176e-01,\n",
      "           -4.9572e-01, -1.0241e+00]],\n",
      "\n",
      "         [[ 5.1715e-01,  1.6527e-02, -6.9063e-02,  ..., -1.3421e+00,\n",
      "            9.8668e-02, -3.5192e-01],\n",
      "          [-1.0992e-01,  1.3660e-01, -1.2379e-01,  ..., -1.0395e+00,\n",
      "           -1.6754e-01,  3.4076e-01],\n",
      "          [-6.1468e-02,  2.9959e-01, -2.1115e-01,  ..., -9.2325e-01,\n",
      "           -2.6646e-01,  4.3665e-01],\n",
      "          ...,\n",
      "          [ 4.9668e-02,  4.4576e-01, -3.3587e-01,  ..., -1.0292e+00,\n",
      "            3.0664e-01,  4.9703e-02],\n",
      "          [-3.7346e-01, -1.2622e-01, -1.6924e-01,  ..., -1.0190e+00,\n",
      "            6.5223e-02, -1.8968e-01],\n",
      "          [-2.6516e-01, -6.8261e-02, -3.5912e-01,  ..., -1.1834e+00,\n",
      "            2.4500e-01,  2.5712e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.2233e-02, -8.5425e-01, -7.2925e-01,  ..., -1.0152e+00,\n",
      "            7.2173e-01, -7.8648e-01],\n",
      "          [-1.8866e-01,  2.2690e+00,  1.4309e+00,  ..., -8.9004e-02,\n",
      "           -2.2363e-01, -5.9859e-03],\n",
      "          [-3.0819e+00,  2.4044e-01,  2.7401e+00,  ...,  1.5239e-01,\n",
      "           -1.1621e+00,  1.9427e+00],\n",
      "          ...,\n",
      "          [-1.0082e+00,  1.6114e+00,  2.1285e+00,  ..., -3.2688e-01,\n",
      "           -1.0154e+00, -6.2318e-01],\n",
      "          [-4.7441e-01,  8.9028e-02,  2.0685e+00,  ...,  1.6194e+00,\n",
      "           -7.2151e-01, -6.9248e-02],\n",
      "          [ 5.0293e-02,  1.7367e+00,  1.8350e+00,  ...,  1.4062e+00,\n",
      "           -1.5780e+00,  1.0748e+00]],\n",
      "\n",
      "         [[-1.1444e+00, -2.9872e+00,  1.7379e-01,  ...,  1.7589e+00,\n",
      "            1.6085e+00, -1.3629e+00],\n",
      "          [ 1.7109e-01,  9.8623e-01, -4.7831e-01,  ..., -9.1817e-01,\n",
      "            5.0882e-01, -4.7400e-01],\n",
      "          [ 1.4765e-01,  4.5590e-01, -4.8718e-01,  ..., -8.4021e-01,\n",
      "            6.6935e-01,  1.9944e-01],\n",
      "          ...,\n",
      "          [-2.2444e-01,  6.9714e-01, -6.8851e-01,  ..., -5.7624e-01,\n",
      "            7.1912e-01, -1.0739e-01],\n",
      "          [ 2.4894e-01,  4.7323e-01, -6.4496e-01,  ..., -7.3011e-01,\n",
      "            6.4431e-01,  2.4437e-01],\n",
      "          [-2.0237e-01,  5.9126e-01, -6.9035e-01,  ..., -1.9685e-02,\n",
      "            7.0362e-01, -1.2924e-01]],\n",
      "\n",
      "         [[ 9.3066e-01,  2.5125e+00,  5.3014e-01,  ..., -6.1372e-01,\n",
      "           -5.9689e-01,  8.7409e-01],\n",
      "          [ 1.1523e+00,  1.9405e+00, -3.5638e-01,  ...,  1.1550e+00,\n",
      "            1.2631e-01, -1.1208e+00],\n",
      "          [ 6.9608e-01,  2.0884e+00,  1.6744e+00,  ...,  2.9676e+00,\n",
      "           -2.3023e+00,  1.5366e-01],\n",
      "          ...,\n",
      "          [ 3.3885e-01,  1.2914e+00,  1.6714e+00,  ...,  1.8157e+00,\n",
      "            3.0345e-01, -5.4106e-01],\n",
      "          [-7.1358e-01,  2.1283e+00,  4.8684e-01,  ...,  1.1484e+00,\n",
      "           -7.8615e-01,  1.1355e-02],\n",
      "          [ 2.3335e-01,  2.3985e+00,  4.1963e-01,  ..., -4.5991e-01,\n",
      "           -1.5214e-01,  8.4545e-01]]]], grad_fn=<PermuteBackward0>), tensor([[[[ 0.6888, -0.1220,  0.0583,  ..., -0.1614, -0.0519, -0.0682],\n",
      "          [ 0.1137,  0.1232, -0.1274,  ..., -0.2536, -0.3814, -0.0300],\n",
      "          [ 0.3740, -0.0441,  0.2936,  ..., -0.2050,  0.3886,  0.0207],\n",
      "          ...,\n",
      "          [-0.0767, -0.7338,  0.0270,  ...,  0.1318,  0.5827,  0.4566],\n",
      "          [ 0.5318, -0.1308, -0.0984,  ...,  0.1536,  0.1628,  0.3375],\n",
      "          [ 0.6872,  0.0842,  0.0291,  ..., -0.3644, -0.0815, -0.0041]],\n",
      "\n",
      "         [[ 0.2817, -0.1418, -0.0332,  ..., -0.0043, -0.6268, -0.1629],\n",
      "          [ 0.0865,  0.4763,  0.8282,  ..., -0.3346,  0.1674,  0.3750],\n",
      "          [ 0.0434, -0.3642,  0.6712,  ...,  0.0088, -0.5426, -0.2819],\n",
      "          ...,\n",
      "          [-0.0887, -0.4255, -0.2389,  ..., -0.5887, -0.0687,  0.5417],\n",
      "          [-0.4126,  0.3792,  0.3184,  ...,  0.4529,  0.0522, -0.0996],\n",
      "          [-0.1880, -0.1547,  0.4431,  ...,  0.4548,  0.2744, -0.0707]],\n",
      "\n",
      "         [[ 0.0756, -0.1996,  0.2204,  ..., -0.6018,  0.2539,  0.0377],\n",
      "          [ 0.4899,  0.2606,  0.4734,  ..., -0.7198,  0.0107,  0.0990],\n",
      "          [ 0.8663,  0.1350, -0.2639,  ..., -0.2333, -0.0175,  0.0819],\n",
      "          ...,\n",
      "          [ 0.6513,  0.9198, -0.4463,  ..., -0.9085, -0.2368,  0.1569],\n",
      "          [ 0.5159, -0.0583, -0.2155,  ..., -0.7161, -0.4919, -0.2218],\n",
      "          [ 0.6677,  0.0496,  0.4239,  ..., -0.3902,  0.3023, -0.2810]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.3776,  0.7315, -0.1188,  ...,  0.0940, -1.0454, -0.1338],\n",
      "          [ 0.0328, -0.1539, -0.0585,  ..., -0.0356, -0.4006, -0.3786],\n",
      "          [-0.0211, -0.7699,  0.6738,  ...,  0.1800, -1.0568,  0.0517],\n",
      "          ...,\n",
      "          [-0.0342, -0.4075,  0.0880,  ...,  0.1180, -0.6480,  0.5055],\n",
      "          [-0.2667, -0.5759,  0.6470,  ..., -0.2806, -0.6736, -0.0589],\n",
      "          [-0.2321, -0.7668,  0.3771,  ...,  0.1099, -0.7427, -0.3256]],\n",
      "\n",
      "         [[ 0.3376, -0.2081, -0.3444,  ...,  0.2393, -3.6649,  0.0240],\n",
      "          [ 0.0902, -0.1079,  0.2649,  ...,  0.5302,  0.1383, -0.0238],\n",
      "          [-0.3178,  0.0628,  0.1356,  ..., -0.2192,  0.2532, -0.2091],\n",
      "          ...,\n",
      "          [-0.0732,  0.1440,  0.0102,  ..., -0.1906, -0.4274, -0.1058],\n",
      "          [-0.2133,  0.3825,  0.2130,  ..., -0.1587,  0.1423,  0.4772],\n",
      "          [ 0.1446,  0.1939, -0.1032,  ...,  0.0140, -0.0516,  0.1508]],\n",
      "\n",
      "         [[ 0.0398, -0.1427, -0.0398,  ..., -0.1953,  0.2215, -0.0815],\n",
      "          [ 0.0916, -0.0555, -0.1958,  ...,  0.0073,  0.3752,  0.1137],\n",
      "          [-0.2358,  0.0136,  0.0561,  ...,  0.2342,  0.2812, -0.2284],\n",
      "          ...,\n",
      "          [ 0.0120, -0.0689,  0.1551,  ...,  0.1010,  0.2072,  0.2058],\n",
      "          [-0.1228,  0.3519,  0.2860,  ...,  0.0353,  0.2533,  0.0042],\n",
      "          [ 0.5045, -0.2603,  0.2926,  ..., -0.1420,  0.1495,  0.4216]]]],\n",
      "       grad_fn=<PermuteBackward0>)), (tensor([[[[-1.3064e-01, -1.0979e+00,  3.2432e-01,  ..., -6.3491e-01,\n",
      "           -1.5534e-01, -3.2244e-02],\n",
      "          [ 9.1356e-01, -1.9732e+00, -1.5342e+00,  ..., -4.0318e-01,\n",
      "            6.9114e-01, -2.0584e+00],\n",
      "          [-5.7985e-01, -3.2941e+00,  1.0398e+00,  ..., -3.1816e-01,\n",
      "            1.1943e+00,  1.0996e+00],\n",
      "          ...,\n",
      "          [-8.3410e-01, -3.3659e+00, -2.5620e-01,  ...,  1.2024e+00,\n",
      "            1.0393e+00,  4.3450e-01],\n",
      "          [ 6.3595e-01, -2.9348e+00,  1.1180e+00,  ...,  3.2539e-01,\n",
      "            1.3172e-01,  6.1427e-01],\n",
      "          [-8.8404e-02, -2.9582e+00, -1.8970e-01,  ...,  2.0508e+00,\n",
      "            3.9535e-01, -3.2704e-01]],\n",
      "\n",
      "         [[-5.1276e-01,  2.7387e-01, -4.6331e-01,  ...,  1.2137e+00,\n",
      "           -5.0740e-01, -4.4837e-01],\n",
      "          [-1.1845e+00,  2.5814e-03, -2.0988e+00,  ...,  7.7741e-01,\n",
      "            1.1046e+00, -4.0853e-01],\n",
      "          [-1.5110e+00,  3.0193e-01, -9.7447e-01,  ...,  1.4578e+00,\n",
      "           -7.7149e-01,  9.8541e-01],\n",
      "          ...,\n",
      "          [-1.2197e+00,  8.6280e-01, -1.4175e+00,  ...,  5.2127e-01,\n",
      "            1.4455e-01, -7.3104e-01],\n",
      "          [-1.6232e+00,  1.1155e+00, -6.6773e-01,  ...,  6.5509e-01,\n",
      "           -3.3087e-01, -1.6180e-01],\n",
      "          [-1.7753e+00, -9.2439e-01, -1.9809e+00,  ...,  3.0107e-01,\n",
      "            2.1093e+00, -4.1915e-01]],\n",
      "\n",
      "         [[ 1.2301e+00,  3.0167e+00,  3.7457e+00,  ...,  6.4829e-01,\n",
      "            1.6559e+00, -7.5835e-01],\n",
      "          [-3.5332e+00,  2.7201e+00, -3.1271e+00,  ..., -2.3347e+00,\n",
      "            3.7236e+00,  1.3975e+00],\n",
      "          [-2.2434e+00,  7.7272e-01, -4.2915e+00,  ..., -3.2902e+00,\n",
      "            3.3329e+00, -2.6203e-02],\n",
      "          ...,\n",
      "          [-3.0278e+00,  2.1481e-01, -4.6373e+00,  ..., -3.4573e+00,\n",
      "            2.3757e+00,  2.2482e-01],\n",
      "          [-2.2168e+00, -1.3730e-01, -3.6287e+00,  ..., -3.0111e+00,\n",
      "            2.5621e+00,  8.2467e-01],\n",
      "          [-2.4950e+00, -1.7693e+00, -4.1322e+00,  ..., -3.8049e+00,\n",
      "            2.4405e+00,  6.3619e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.3653e+00, -2.7223e+00, -2.6976e+00,  ...,  9.1960e-01,\n",
      "            4.5927e-01,  2.6761e+00],\n",
      "          [-1.8721e+00,  2.2937e+00,  6.3503e-01,  ..., -5.0201e-01,\n",
      "           -2.4214e+00,  1.9656e-01],\n",
      "          [-2.1684e+00,  2.0264e+00,  2.1374e+00,  ...,  2.4647e-01,\n",
      "           -2.1852e+00, -5.3933e-01],\n",
      "          ...,\n",
      "          [-3.5112e+00,  3.0812e+00,  2.3877e+00,  ...,  5.9291e-01,\n",
      "           -1.6327e+00, -1.6200e+00],\n",
      "          [-2.3861e+00,  2.2758e+00,  1.2010e+00,  ...,  3.5292e-01,\n",
      "           -2.5653e+00, -2.0206e+00],\n",
      "          [-3.6080e+00,  3.7016e+00,  1.1661e+00,  ..., -2.8118e-01,\n",
      "           -2.2032e+00, -8.3960e-01]],\n",
      "\n",
      "         [[ 1.7415e+00,  4.4209e-01,  9.3136e-01,  ..., -2.7851e-03,\n",
      "           -9.8478e-01, -3.0201e-01],\n",
      "          [ 1.9936e+00,  1.1219e+00,  6.8835e-01,  ...,  3.0822e-02,\n",
      "           -1.7473e+00, -1.4582e+00],\n",
      "          [ 2.0865e+00,  5.9878e-01,  1.7819e+00,  ...,  2.9807e-01,\n",
      "           -1.3333e+00, -9.8664e-01],\n",
      "          ...,\n",
      "          [ 2.2854e+00,  4.0623e-01,  1.0110e+00,  ..., -4.5140e-02,\n",
      "           -1.3934e+00, -1.3925e+00],\n",
      "          [ 2.3561e+00,  1.0713e-01,  1.4692e+00,  ...,  5.9521e-02,\n",
      "           -9.1071e-01, -9.8251e-01],\n",
      "          [ 1.7382e+00,  3.0453e-01,  8.0566e-01,  ..., -8.2713e-02,\n",
      "           -1.7958e+00, -7.0052e-01]],\n",
      "\n",
      "         [[-2.6354e-01,  1.7693e-01, -5.4590e-01,  ...,  2.5397e-01,\n",
      "            2.7925e-01,  1.6319e-01],\n",
      "          [-9.4149e-01,  7.3126e-01,  1.2778e-01,  ..., -3.4195e-02,\n",
      "            7.7102e-01, -2.5772e-01],\n",
      "          [-8.9125e-02,  6.2769e-01, -5.2680e-01,  ..., -4.9708e-01,\n",
      "            1.6790e-01, -4.8594e-02],\n",
      "          ...,\n",
      "          [ 1.8897e-01,  4.1793e-01,  4.2095e-03,  ..., -8.7246e-02,\n",
      "            9.2267e-01,  5.2892e-01],\n",
      "          [-5.0142e-01, -7.0971e-02, -2.9293e-01,  ...,  7.1730e-01,\n",
      "            3.6349e-01, -3.1808e-01],\n",
      "          [-8.2282e-01,  2.1863e-01, -4.0295e-01,  ...,  2.3457e-01,\n",
      "            6.5272e-01,  1.1903e+00]]]], grad_fn=<PermuteBackward0>), tensor([[[[-5.0393e-04,  4.8796e-03, -1.4501e-01,  ..., -7.6289e-03,\n",
      "           -2.8119e-02, -5.4427e-01],\n",
      "          [ 4.1592e-01, -1.6358e-01, -6.1516e-01,  ..., -1.1610e+00,\n",
      "           -3.3906e-01,  7.3473e-01],\n",
      "          [-2.5723e-01, -1.9903e-01,  3.4377e-01,  ..., -4.7447e-01,\n",
      "           -5.2443e-01,  8.9744e-01],\n",
      "          ...,\n",
      "          [-6.2955e-01, -5.1045e-01,  2.7003e-01,  ..., -5.5983e-01,\n",
      "            3.9542e-02,  9.8375e-01],\n",
      "          [-3.2224e-01, -3.4890e-01,  3.3485e-01,  ..., -5.4965e-01,\n",
      "            8.0939e-02,  3.8376e-01],\n",
      "          [-7.0190e-02,  9.2515e-01, -1.7987e+00,  ...,  4.0547e-01,\n",
      "            2.7042e-01, -7.3333e-01]],\n",
      "\n",
      "         [[ 3.9716e-02,  1.0030e-02,  3.6718e-02,  ..., -4.8265e-02,\n",
      "           -3.5171e-03,  1.6775e-02],\n",
      "          [-1.1896e-01,  1.7423e-01,  1.9564e-02,  ..., -6.3828e-01,\n",
      "            1.1594e-01, -1.3074e-01],\n",
      "          [-1.0151e+00,  3.3278e-01, -6.7921e-01,  ..., -7.7678e-01,\n",
      "            1.6370e-01,  1.2506e-01],\n",
      "          ...,\n",
      "          [-8.0226e-01,  3.4991e-01,  1.7780e-01,  ...,  1.2278e-01,\n",
      "           -9.2416e-01, -3.9670e-01],\n",
      "          [-1.6534e+00,  1.1237e+00,  7.8151e-02,  ..., -7.4000e-02,\n",
      "            6.6094e-01, -8.2608e-01],\n",
      "          [-2.7947e-01,  8.7644e-01, -3.3824e-01,  ..., -5.5729e-01,\n",
      "            2.5327e-01, -4.9211e-01]],\n",
      "\n",
      "         [[ 3.3389e-02, -7.5867e-01, -7.8789e-02,  ...,  5.0789e-02,\n",
      "            6.6967e-03, -5.6933e-02],\n",
      "          [ 1.8743e-01, -8.2430e-01,  1.6054e-01,  ...,  5.8465e-01,\n",
      "           -3.7226e-01,  4.5724e-01],\n",
      "          [ 5.8520e-01, -1.8240e+00, -1.0297e-01,  ..., -9.3858e-01,\n",
      "            6.2460e-01,  3.4501e-01],\n",
      "          ...,\n",
      "          [ 4.8532e-01, -1.8520e+00,  6.5008e-01,  ..., -2.8849e-01,\n",
      "           -7.2474e-02, -5.5462e-02],\n",
      "          [ 6.3006e-02, -1.6277e+00,  4.7184e-01,  ...,  3.1680e-01,\n",
      "            3.3567e-01, -1.3281e+00],\n",
      "          [ 5.5559e-01, -1.4183e+00,  1.0968e-01,  ..., -3.6159e-01,\n",
      "           -1.5538e-02,  1.5086e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.6744e-02, -7.3889e-02,  1.3169e+00,  ..., -4.2897e-02,\n",
      "            1.9233e-01, -2.8341e-02],\n",
      "          [ 1.1231e-01, -5.2662e-01,  2.1252e+00,  ...,  4.1341e-01,\n",
      "            1.5472e-01,  4.6660e-01],\n",
      "          [ 1.3814e-02, -2.3841e-01,  4.1973e-01,  ...,  8.1055e-02,\n",
      "           -2.3287e-03, -2.9085e-01],\n",
      "          ...,\n",
      "          [ 8.7570e-03, -9.0433e-01,  6.3584e-01,  ...,  1.1827e-01,\n",
      "           -5.4032e-02, -1.7269e-01],\n",
      "          [-2.4978e-01, -3.2650e-01,  1.1344e+00,  ..., -2.3940e-01,\n",
      "           -1.8000e-01,  2.4618e-01],\n",
      "          [ 6.0731e-02, -5.9978e-01,  2.2221e+00,  ...,  4.9747e-01,\n",
      "           -2.2863e-01,  6.7705e-01]],\n",
      "\n",
      "         [[ 3.3704e-03, -1.1483e-01, -1.7243e-01,  ...,  1.5501e-01,\n",
      "            1.0659e-01,  1.5490e-01],\n",
      "          [ 7.2209e-01, -7.6462e-02, -4.1955e-01,  ...,  8.6717e-02,\n",
      "           -1.2081e+00, -7.5173e-01],\n",
      "          [ 4.8515e-01, -4.2426e-01,  2.5429e-01,  ...,  6.2008e-01,\n",
      "            9.0937e-03,  4.3552e-01],\n",
      "          ...,\n",
      "          [ 8.6042e-02, -3.4666e-01, -4.2675e-02,  ...,  5.8158e-01,\n",
      "           -4.3553e-01,  5.4511e-02],\n",
      "          [ 7.2782e-02, -6.6633e-01,  2.6930e-02,  ...,  1.1203e-01,\n",
      "           -6.7174e-01, -4.6135e-01],\n",
      "          [ 1.3095e+00, -2.0998e-01,  4.5598e-01,  ..., -4.9698e-01,\n",
      "           -8.3722e-01,  4.0662e-01]],\n",
      "\n",
      "         [[ 1.4329e-02,  2.1596e-02,  2.4439e-02,  ..., -6.4790e-02,\n",
      "            2.3542e-01,  4.6180e-03],\n",
      "          [-1.7783e-01, -5.5750e-01,  5.4701e-01,  ..., -7.5747e-01,\n",
      "           -1.9474e+00,  4.7546e-01],\n",
      "          [-3.3409e-02, -1.3789e-01, -1.1980e+00,  ..., -1.9490e-01,\n",
      "           -1.7439e+00, -6.0625e-02],\n",
      "          ...,\n",
      "          [ 9.9703e-01,  2.0257e-01, -3.1060e-01,  ..., -1.5530e-01,\n",
      "           -2.1347e+00, -6.1646e-03],\n",
      "          [ 1.7343e-01, -3.4995e-02, -3.2649e-01,  ..., -7.3843e-01,\n",
      "           -1.4943e+00,  4.6113e-01],\n",
      "          [ 5.1815e-01, -3.5678e-01, -1.0977e-01,  ...,  1.2455e-01,\n",
      "           -1.1751e+00, -2.0073e-01]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[ 2.7788e-02, -2.2848e-01,  1.7585e-01,  ..., -8.7202e-01,\n",
      "            7.3388e-01, -1.2083e+00],\n",
      "          [-3.3307e-01, -2.6727e-01, -1.0965e+00,  ...,  7.6214e-02,\n",
      "           -3.7527e-01,  1.0802e+00],\n",
      "          [ 7.6970e-01, -5.8101e-01,  1.5723e+00,  ..., -1.4236e+00,\n",
      "            1.5276e+00,  2.4899e+00],\n",
      "          ...,\n",
      "          [ 1.8970e+00,  1.9054e+00,  4.7631e-02,  ..., -4.4136e-01,\n",
      "           -1.7720e+00,  3.5283e+00],\n",
      "          [-2.4493e+00,  1.9428e+00, -1.1049e+00,  ...,  1.1314e+00,\n",
      "           -2.8493e-01,  6.7003e-01],\n",
      "          [-3.4411e-03, -1.3296e+00, -3.7462e-02,  ...,  1.9412e+00,\n",
      "            1.1197e+00,  7.8791e-01]],\n",
      "\n",
      "         [[ 7.8763e-01,  2.0943e-01,  4.4604e-02,  ..., -1.4038e-01,\n",
      "           -1.0861e+00, -1.8975e-01],\n",
      "          [ 4.4435e-01, -1.7008e+00,  2.0655e+00,  ...,  2.3038e+00,\n",
      "            4.3517e+00,  1.6300e+00],\n",
      "          [ 2.1174e-01, -1.4767e+00, -2.4156e-01,  ..., -1.4703e-01,\n",
      "            4.8965e+00,  9.0495e-01],\n",
      "          ...,\n",
      "          [ 1.7149e+00,  4.2309e-01, -1.0179e+00,  ..., -5.5805e-02,\n",
      "            6.2501e+00, -6.2374e-01],\n",
      "          [ 7.7476e-01, -9.5950e-01, -1.2123e+00,  ..., -2.0367e-01,\n",
      "            5.3426e+00,  8.8866e-01],\n",
      "          [-1.8800e+00, -4.5295e-01,  1.7126e-01,  ...,  4.5545e-01,\n",
      "            4.7357e+00,  1.3549e+00]],\n",
      "\n",
      "         [[ 3.6749e-01, -3.5005e-01, -3.4176e-01,  ...,  3.4573e-01,\n",
      "            1.4536e+00,  2.4569e-01],\n",
      "          [ 8.2784e-01, -5.4502e+00, -1.4256e+00,  ..., -3.0112e+00,\n",
      "           -1.8108e+00, -5.9609e+00],\n",
      "          [-1.2803e+00, -5.3884e+00, -1.4840e+00,  ..., -2.8317e+00,\n",
      "           -1.7284e+00, -6.3506e+00],\n",
      "          ...,\n",
      "          [-4.8669e-01, -7.3041e+00, -1.2751e+00,  ..., -5.8802e+00,\n",
      "           -2.5512e+00, -6.5385e+00],\n",
      "          [-2.3958e+00, -6.5567e+00, -1.4496e+00,  ..., -4.7557e+00,\n",
      "           -3.1316e+00, -5.6689e+00],\n",
      "          [-2.1095e+00, -6.4414e+00, -2.1893e+00,  ..., -3.4015e+00,\n",
      "           -1.6757e+00, -5.1771e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.2113e-01,  1.7470e+00,  5.1675e-01,  ...,  2.4330e-01,\n",
      "            4.6468e-01, -1.6733e+00],\n",
      "          [ 7.5471e-01, -5.9362e+00,  1.2807e+00,  ..., -2.8052e+00,\n",
      "           -1.8204e+00,  4.9073e+00],\n",
      "          [-1.2388e+00, -5.3100e+00,  1.1507e+00,  ..., -2.4889e+00,\n",
      "           -1.9652e+00,  6.6657e+00],\n",
      "          ...,\n",
      "          [ 1.0864e+00, -7.9558e+00,  4.6129e-01,  ..., -1.0731e+00,\n",
      "           -2.1434e+00,  7.3219e+00],\n",
      "          [ 1.3286e+00, -5.3189e+00,  8.2856e-01,  ..., -1.3376e+00,\n",
      "           -2.5929e+00,  6.7799e+00],\n",
      "          [ 4.1255e+00, -7.5861e+00,  2.3945e+00,  ..., -8.7181e-01,\n",
      "           -2.0500e+00,  4.2582e+00]],\n",
      "\n",
      "         [[ 4.3694e-02, -3.8561e-02,  1.5395e-01,  ..., -1.0313e-01,\n",
      "           -9.3783e-02, -1.5308e-01],\n",
      "          [-1.5399e+00, -1.4477e+00, -9.0096e-02,  ..., -1.8788e+00,\n",
      "            1.7003e-01, -1.3554e+00],\n",
      "          [ 1.7225e+00, -1.2175e+00,  8.0361e-01,  ..., -3.1351e-01,\n",
      "           -7.9156e-01,  8.2051e-02],\n",
      "          ...,\n",
      "          [ 1.0463e+00, -6.0492e-01, -1.1385e+00,  ..., -8.6565e-01,\n",
      "           -1.4969e+00, -3.9215e-01],\n",
      "          [ 1.3005e+00,  2.6865e-01, -2.1182e-01,  ..., -1.3155e+00,\n",
      "            2.1500e-01,  6.3746e-01],\n",
      "          [-2.3935e-02, -3.5993e-02, -5.1257e-01,  ..., -2.0523e+00,\n",
      "            8.1780e-01, -5.0735e-01]],\n",
      "\n",
      "         [[ 4.1389e-01, -5.0654e-02,  1.8796e+00,  ..., -2.2928e-01,\n",
      "           -1.9405e-01, -1.0033e+00],\n",
      "          [ 3.9689e+00,  2.8258e+00, -2.5711e+00,  ...,  1.5037e+00,\n",
      "            1.1603e+00,  2.3677e+00],\n",
      "          [ 3.5356e+00,  4.3206e-01, -2.1687e+00,  ...,  8.8892e-01,\n",
      "            1.7043e-01,  3.7651e+00],\n",
      "          ...,\n",
      "          [ 3.1921e+00, -1.5396e-01, -1.7423e+00,  ...,  5.8445e-01,\n",
      "            7.8061e-01,  5.4974e+00],\n",
      "          [ 2.7578e+00,  1.3180e+00, -1.8304e-01,  ...,  1.2056e+00,\n",
      "            9.0558e-01,  4.5920e+00],\n",
      "          [ 3.2101e+00,  1.7057e+00, -3.0014e+00,  ...,  1.5700e-01,\n",
      "            1.2103e+00,  2.9143e+00]]]], grad_fn=<PermuteBackward0>), tensor([[[[ 0.0393,  0.0640, -0.0040,  ...,  0.0148,  0.1003,  0.0364],\n",
      "          [ 0.1899, -1.2731, -0.0927,  ...,  0.4180, -1.1752, -0.7597],\n",
      "          [ 0.6639, -1.2155,  0.6802,  ...,  0.3044, -0.5365, -0.6152],\n",
      "          ...,\n",
      "          [ 0.6925, -1.0998, -0.0908,  ..., -0.9177, -0.3616,  0.7024],\n",
      "          [ 0.8738, -0.0030,  0.6249,  ...,  0.4040, -0.2402, -0.3529],\n",
      "          [ 0.5926, -0.4416,  0.4759,  ...,  0.2889, -1.1714,  0.1500]],\n",
      "\n",
      "         [[-0.0396,  0.0046,  0.0802,  ..., -0.0323, -0.0323, -0.0479],\n",
      "          [ 0.4534,  0.3375,  0.0344,  ..., -0.1345,  0.0738,  0.3358],\n",
      "          [ 0.1075, -0.3013, -0.0287,  ..., -0.7180,  0.1979, -0.3366],\n",
      "          ...,\n",
      "          [ 0.7735, -0.4434,  0.7290,  ...,  0.3555, -0.0562,  0.6400],\n",
      "          [ 0.3798, -0.1745, -0.5894,  ..., -0.6654,  0.3393, -0.1447],\n",
      "          [-0.0732,  0.1401, -0.0279,  ...,  0.3760,  0.4056,  0.3141]],\n",
      "\n",
      "         [[ 0.0398, -0.1029, -0.0551,  ..., -0.0279,  0.0944, -0.1543],\n",
      "          [-0.5191, -0.1946, -0.9514,  ...,  0.2838, -0.1794,  0.0183],\n",
      "          [-0.0972,  0.1324, -0.2935,  ..., -0.2863, -0.1100,  0.2025],\n",
      "          ...,\n",
      "          [-0.2279, -0.1897, -0.0984,  ..., -0.2617, -0.0540, -0.1865],\n",
      "          [-0.3661,  0.4980, -0.1153,  ...,  0.2046, -0.4417, -0.5688],\n",
      "          [ 0.2646,  0.1311, -0.2495,  ..., -0.0970, -0.0932,  0.2982]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0199,  0.1196,  0.0043,  ..., -0.0280,  0.0703, -0.0397],\n",
      "          [ 0.3740, -0.5725,  0.5402,  ...,  0.3354, -0.2783,  0.1177],\n",
      "          [-0.4676,  0.4910, -0.5381,  ..., -0.6379, -0.0212, -0.3652],\n",
      "          ...,\n",
      "          [ 0.3420, -0.4254, -0.8352,  ..., -1.0714, -0.5262, -0.1558],\n",
      "          [ 0.4620, -0.3259, -0.3333,  ..., -0.1631, -1.1375,  0.1938],\n",
      "          [ 0.8135, -0.6944,  0.8459,  ...,  0.9354, -0.0821, -0.9512]],\n",
      "\n",
      "         [[-0.1650, -0.1273, -0.0911,  ..., -0.2380, -0.0235, -0.0439],\n",
      "          [ 0.2423, -0.1982, -0.3798,  ...,  1.2084,  0.1282,  0.4970],\n",
      "          [-0.2471, -0.0481,  0.6488,  ...,  0.2297,  0.0877,  0.2649],\n",
      "          ...,\n",
      "          [ 0.4882, -0.4170, -1.1254,  ..., -0.0257,  0.3899,  0.6498],\n",
      "          [ 0.0378, -0.1978, -0.7505,  ...,  0.6408,  0.3061,  0.0650],\n",
      "          [ 0.3428,  0.2477, -0.0302,  ...,  0.3336, -0.3238,  0.7069]],\n",
      "\n",
      "         [[ 0.1198, -0.0662, -0.0280,  ..., -0.0150, -0.0899, -0.1058],\n",
      "          [ 0.0870,  0.7072, -0.0679,  ..., -0.2282, -0.5942,  0.2226],\n",
      "          [ 0.4126,  0.5676,  0.4628,  ...,  0.3485,  0.4450,  0.4159],\n",
      "          ...,\n",
      "          [ 0.1018,  1.0225,  0.8144,  ...,  1.0189, -0.7810,  0.2011],\n",
      "          [-0.1640,  0.4718,  0.7213,  ..., -0.4266,  0.7220, -0.5376],\n",
      "          [ 0.7442,  0.1339,  0.1364,  ..., -0.2110, -0.1334,  0.5818]]]],\n",
      "       grad_fn=<PermuteBackward0>)), (tensor([[[[-8.7030e-01, -1.3107e-01,  3.3352e-01,  ..., -9.5781e-01,\n",
      "            2.2612e-02, -2.9464e+00],\n",
      "          [ 8.7902e-01,  8.4963e-01, -2.3842e+00,  ..., -1.0832e+00,\n",
      "           -3.3621e+00,  7.3047e+00],\n",
      "          [ 4.2046e-01, -1.6989e+00, -3.6359e+00,  ..., -1.8860e+00,\n",
      "           -1.3055e+00,  8.7708e+00],\n",
      "          ...,\n",
      "          [ 1.1597e+00, -5.5044e-02, -2.9820e+00,  ..., -2.9855e+00,\n",
      "           -8.0059e-01,  9.0167e+00],\n",
      "          [ 1.3003e+00, -1.8427e+00, -2.5466e+00,  ..., -2.2680e+00,\n",
      "           -4.9269e-01,  9.0153e+00],\n",
      "          [ 1.4810e+00, -2.1189e+00, -3.1684e+00,  ..., -1.4982e+00,\n",
      "           -7.4555e-01,  7.5856e+00]],\n",
      "\n",
      "         [[ 3.9207e-01, -8.7198e-02,  4.5695e-01,  ..., -1.4772e-01,\n",
      "           -5.9226e-02, -2.2404e+00],\n",
      "          [-9.2611e-01, -1.0826e+00,  2.9565e+00,  ..., -3.3355e-01,\n",
      "            6.7191e-02,  5.2791e+00],\n",
      "          [-2.0138e+00,  2.5610e+00,  2.7312e+00,  ..., -8.6938e-01,\n",
      "           -9.2203e-01,  6.5544e+00],\n",
      "          ...,\n",
      "          [-2.3277e+00,  3.0192e+00,  2.2181e+00,  ..., -6.7362e-01,\n",
      "           -2.3619e+00,  5.3670e+00],\n",
      "          [-1.4857e+00,  1.1971e+00,  4.1599e+00,  ..., -1.2885e+00,\n",
      "            3.6474e-02,  5.8478e+00],\n",
      "          [-9.6462e-01,  1.8368e-01,  3.9543e+00,  ...,  8.6317e-02,\n",
      "            2.0718e-01,  5.9762e+00]],\n",
      "\n",
      "         [[ 1.3705e-01, -6.6146e-01, -2.0491e-01,  ...,  1.4911e-01,\n",
      "            2.5535e-01, -1.7442e-01],\n",
      "          [ 9.6288e-01,  7.6794e-01,  1.0388e+00,  ...,  1.1536e+00,\n",
      "            6.9593e-01,  6.7704e-02],\n",
      "          [ 6.9019e-01,  1.5791e+00, -1.1705e+00,  ..., -2.6818e+00,\n",
      "            2.7664e+00, -3.0953e-01],\n",
      "          ...,\n",
      "          [ 3.4966e-01,  2.8292e+00, -2.7694e-01,  ...,  1.9839e-01,\n",
      "           -2.9795e-01,  5.6709e-02],\n",
      "          [ 5.5502e-01,  2.8706e+00,  1.0689e+00,  ...,  4.6223e-01,\n",
      "            1.1534e+00, -3.3890e-01],\n",
      "          [-4.6586e-01,  3.5610e+00,  1.8373e+00,  ..., -1.9501e+00,\n",
      "            8.7908e-01,  5.4681e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.7748e-01,  2.7054e-02, -1.0139e-02,  ...,  1.2542e+00,\n",
      "            6.6676e-02,  1.7786e+00],\n",
      "          [ 8.4195e-01,  3.3218e-01, -3.7188e-01,  ..., -2.4526e+00,\n",
      "           -1.0317e+00, -1.7486e+00],\n",
      "          [ 1.4077e+00, -2.1439e+00,  1.9194e+00,  ..., -2.8310e+00,\n",
      "           -1.5155e+00, -4.6366e-01],\n",
      "          ...,\n",
      "          [ 6.3642e-01, -3.7890e-01,  4.1699e-01,  ..., -2.6958e+00,\n",
      "           -1.8645e+00, -1.5154e+00],\n",
      "          [ 5.6299e-01,  7.1263e-02,  1.9159e+00,  ..., -2.8884e+00,\n",
      "           -1.3942e+00, -2.9639e-01],\n",
      "          [-4.5055e-02, -1.7017e+00, -2.1927e+00,  ..., -2.8068e+00,\n",
      "           -1.4369e+00,  4.6139e-01]],\n",
      "\n",
      "         [[-3.3143e-01, -1.4473e-01,  2.1405e-01,  ...,  2.5461e-01,\n",
      "           -2.4395e-02,  9.5474e-03],\n",
      "          [ 1.0049e+00, -3.7632e-02,  1.0678e+00,  ...,  8.0685e-01,\n",
      "            1.1032e+00, -6.8820e-01],\n",
      "          [-1.2172e+00,  6.9030e-01,  1.5863e+00,  ..., -3.1340e-01,\n",
      "            7.7381e-01,  3.1862e-01],\n",
      "          ...,\n",
      "          [-1.4489e+00, -1.8195e+00,  2.2968e-01,  ..., -2.3629e-01,\n",
      "            5.0580e-01,  1.7240e+00],\n",
      "          [ 6.2354e-01, -4.7434e-01,  6.6617e-01,  ..., -4.2127e-01,\n",
      "            5.5778e-01,  1.3094e+00],\n",
      "          [-3.6481e-01, -8.1014e-01,  1.0905e+00,  ...,  2.8319e-01,\n",
      "           -3.4009e-01,  1.1526e+00]],\n",
      "\n",
      "         [[ 3.4086e+00,  2.1957e+00, -2.0634e+00,  ..., -2.8445e+00,\n",
      "           -3.8668e+00, -1.2118e+00],\n",
      "          [-3.9493e+00, -9.6459e-02,  6.3140e+00,  ..., -1.6963e+00,\n",
      "            9.7586e+00,  6.5454e-01],\n",
      "          [-2.2183e+00,  1.4802e+00,  8.0630e+00,  ..., -3.1303e-01,\n",
      "            9.6195e+00, -3.0811e+00],\n",
      "          ...,\n",
      "          [-1.2233e+00,  2.5477e-01,  1.0940e+01,  ..., -2.0287e+00,\n",
      "            9.2713e+00, -3.0632e+00],\n",
      "          [ 4.1926e-01, -1.9422e+00,  1.0884e+01,  ..., -2.4706e+00,\n",
      "            8.4180e+00, -2.6354e-01],\n",
      "          [-3.8019e+00, -2.9484e+00,  8.6298e+00,  ...,  3.9286e-01,\n",
      "            9.4200e+00, -3.5452e+00]]]], grad_fn=<PermuteBackward0>), tensor([[[[-8.1243e-03, -4.6875e-02,  2.8140e-02,  ...,  6.3446e-02,\n",
      "            3.7524e-02,  6.2208e-02],\n",
      "          [ 3.6318e-01,  4.0679e-01, -8.8457e-01,  ..., -8.7599e-01,\n",
      "           -6.2197e-01, -2.3575e-01],\n",
      "          [ 8.4704e-01, -2.4193e-01,  6.5432e-02,  ..., -4.8768e-02,\n",
      "           -3.2319e-01,  2.7234e-02],\n",
      "          ...,\n",
      "          [-7.1644e-01, -3.8051e-01,  3.1958e-01,  ..., -1.7277e-01,\n",
      "           -2.8744e-01,  6.4842e-01],\n",
      "          [ 6.3426e-02,  5.8646e-02,  1.5367e-01,  ..., -7.9727e-01,\n",
      "            2.1005e-01, -4.7221e-02],\n",
      "          [-2.8939e-01,  9.8619e-01, -1.2187e-01,  ...,  6.1459e-01,\n",
      "            1.2230e-01,  2.8980e-02]],\n",
      "\n",
      "         [[-7.9340e-02, -1.9839e-02, -1.3748e-01,  ..., -4.5362e-02,\n",
      "            4.7141e-02, -2.1514e-02],\n",
      "          [ 3.8464e-01,  4.4843e-01,  2.5690e-01,  ...,  6.0012e-02,\n",
      "           -7.9366e-01,  1.7406e-02],\n",
      "          [ 6.8222e-01,  4.3141e-01, -4.9966e-01,  ...,  1.2264e+00,\n",
      "            1.6478e-02,  3.0150e-01],\n",
      "          ...,\n",
      "          [-1.8152e-01,  1.0517e+00,  1.6914e-01,  ...,  2.2235e-01,\n",
      "           -1.4414e-01,  4.8126e-01],\n",
      "          [ 4.2601e-01,  8.4627e-01,  7.6066e-01,  ...,  3.1048e-01,\n",
      "           -9.5713e-01,  1.3933e-01],\n",
      "          [-7.9261e-01, -4.5893e-02,  2.7801e-01,  ..., -2.4856e-01,\n",
      "           -1.8497e-01,  1.4958e-01]],\n",
      "\n",
      "         [[ 6.4877e-02,  1.0147e-01,  9.0731e-02,  ...,  2.4853e-02,\n",
      "           -7.0408e-02,  2.1704e-03],\n",
      "          [-2.8727e-01,  1.0639e+00, -7.6742e-01,  ..., -1.6286e-01,\n",
      "           -2.7387e-02,  2.1888e-01],\n",
      "          [-2.2061e-01,  7.3867e-01,  2.6517e-01,  ...,  4.1131e-01,\n",
      "            1.1331e-01, -9.2833e-02],\n",
      "          ...,\n",
      "          [ 7.6469e-01,  4.5741e-01,  6.1440e-01,  ...,  2.1229e-01,\n",
      "           -6.9302e-01,  1.1526e+00],\n",
      "          [ 3.9825e-01,  1.6894e-01,  2.8217e-01,  ...,  9.9099e-03,\n",
      "            1.7956e-01, -1.8304e-01],\n",
      "          [ 9.5533e-01,  2.3099e-01, -3.7088e-01,  ...,  1.1159e-01,\n",
      "            4.1088e-01,  8.4991e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 6.9642e-04,  8.0578e-02, -8.2013e-02,  ...,  4.4803e-02,\n",
      "            3.8873e-02, -1.2982e-01],\n",
      "          [ 9.7045e-01,  7.6598e-01,  2.0733e-01,  ..., -2.7961e-02,\n",
      "            3.3511e-01,  4.8941e-01],\n",
      "          [ 2.9853e-01,  5.2318e-01, -3.8045e-02,  ...,  1.4244e-01,\n",
      "           -5.3408e-01,  7.4249e-01],\n",
      "          ...,\n",
      "          [ 9.5500e-02,  1.0807e-01, -1.7016e-01,  ..., -4.4687e-01,\n",
      "            4.3877e-01,  4.9143e-01],\n",
      "          [ 3.9956e-01, -4.1271e-02,  2.2884e-01,  ..., -1.1470e+00,\n",
      "           -7.0314e-01,  1.8757e-01],\n",
      "          [-1.4463e-01,  1.5225e-01, -3.5494e-01,  ...,  7.4277e-02,\n",
      "           -9.4380e-01, -7.6884e-01]],\n",
      "\n",
      "         [[-1.2790e-01, -5.0633e-02,  1.0964e-01,  ..., -7.0734e-02,\n",
      "            5.3396e-02, -2.0372e-02],\n",
      "          [-3.0156e-01, -9.6380e-01, -5.5456e-02,  ...,  3.8136e-02,\n",
      "           -1.4608e-01,  3.0372e-01],\n",
      "          [-1.6844e+00, -1.1151e+00, -1.4948e+00,  ...,  1.0683e+00,\n",
      "            1.8470e-02, -4.7352e-01],\n",
      "          ...,\n",
      "          [ 1.2311e+00, -2.5707e-01, -1.1381e+00,  ...,  1.1436e+00,\n",
      "           -7.3893e-01, -7.7884e-01],\n",
      "          [-1.2139e-01, -9.8972e-02,  1.2058e-01,  ...,  1.8321e-01,\n",
      "           -8.0374e-02, -1.4073e+00],\n",
      "          [ 1.2635e-01,  2.2019e-01, -6.2877e-01,  ..., -5.4051e-01,\n",
      "           -9.4094e-01,  2.5185e-01]],\n",
      "\n",
      "         [[-1.7268e-02, -1.3456e-02, -2.0527e-02,  ..., -2.6838e-02,\n",
      "            9.7050e-03, -1.7938e-02],\n",
      "          [-7.7853e-01, -6.9514e-01,  2.7593e-02,  ...,  9.3043e-03,\n",
      "           -2.1480e-01,  2.6231e-02],\n",
      "          [-5.7314e-01,  1.7547e-01, -3.4330e-01,  ..., -2.4895e-01,\n",
      "            8.2850e-02, -1.6592e+00],\n",
      "          ...,\n",
      "          [ 4.2328e-01, -1.7767e-01, -4.9537e-01,  ..., -8.8111e-02,\n",
      "           -5.4085e-01, -7.2555e-01],\n",
      "          [ 3.9623e-01, -4.5017e-01, -4.6419e-02,  ...,  9.6666e-02,\n",
      "            1.4161e-01, -9.9378e-02],\n",
      "          [ 1.9659e-01, -3.5031e-01, -2.1810e-01,  ..., -5.2627e-01,\n",
      "            4.4161e-01,  7.5955e-02]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[ 2.4765e-02, -2.8574e-01,  2.1615e-01,  ...,  1.6883e+00,\n",
      "           -2.1490e-01, -7.0574e-02],\n",
      "          [ 4.3964e-01,  2.0785e+00,  7.6777e-01,  ..., -4.4799e+00,\n",
      "            5.3344e-01, -9.9032e-01],\n",
      "          [-1.1761e-01,  2.1367e+00, -4.9687e-02,  ..., -2.7405e+00,\n",
      "            1.1945e+00, -7.2273e-01],\n",
      "          ...,\n",
      "          [ 5.0301e-01,  3.5190e-01, -1.3136e+00,  ..., -4.5201e+00,\n",
      "           -9.0387e-01, -1.7619e+00],\n",
      "          [ 1.0892e+00,  5.2051e-01, -1.6928e+00,  ..., -4.5051e+00,\n",
      "           -1.8976e+00, -1.8637e+00],\n",
      "          [ 1.2572e+00,  9.3484e-01, -1.9021e+00,  ..., -4.9509e+00,\n",
      "           -1.0847e+00, -1.5218e+00]],\n",
      "\n",
      "         [[ 1.8058e-01,  9.8178e-01, -1.4240e+00,  ..., -1.2947e-01,\n",
      "            2.6640e-01,  9.1793e-01],\n",
      "          [-7.9507e-01, -5.2798e+00,  6.3241e-01,  ..., -1.5922e+00,\n",
      "            1.8192e+00, -1.4376e+00],\n",
      "          [ 1.6657e+00, -3.2325e+00, -9.2456e-02,  ..., -1.7967e-01,\n",
      "            3.6682e-01, -4.4002e-01],\n",
      "          ...,\n",
      "          [-7.5780e-01, -4.0911e+00,  1.8216e+00,  ..., -1.1872e+00,\n",
      "           -5.4371e-01,  1.0322e+00],\n",
      "          [ 7.2514e-01, -3.8931e+00,  3.0938e+00,  ...,  2.0449e-01,\n",
      "           -2.3604e+00, -2.4342e+00],\n",
      "          [ 6.4305e-02, -3.6256e+00,  4.5494e+00,  ...,  2.9568e-01,\n",
      "           -1.2645e-01, -1.4457e+00]],\n",
      "\n",
      "         [[-6.6871e-01,  2.4777e-01, -4.1925e-02,  ...,  1.8080e-01,\n",
      "            4.8689e-02, -2.9203e-01],\n",
      "          [ 1.5419e+00, -1.1425e+00, -5.7651e-01,  ..., -1.4225e+00,\n",
      "           -1.7641e-01, -5.7116e-01],\n",
      "          [ 2.3532e+00, -1.6485e+00, -2.2027e-02,  ..., -4.1829e-01,\n",
      "           -1.3139e-03,  2.7159e-01],\n",
      "          ...,\n",
      "          [ 2.0948e-01, -1.4102e+00,  3.0157e-01,  ..., -6.8001e-01,\n",
      "            1.4259e+00,  6.4480e-01],\n",
      "          [ 4.2430e-01, -2.7407e-01, -8.3983e-01,  ...,  4.3362e-01,\n",
      "            1.8679e+00, -5.5671e-01],\n",
      "          [ 2.0665e+00, -4.3276e-01, -6.5379e-01,  ..., -1.0391e+00,\n",
      "           -4.5373e-01,  6.5413e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-4.0074e-02,  1.1457e-01,  1.4191e-01,  ..., -9.7227e-02,\n",
      "            2.0769e-02,  1.5848e-01],\n",
      "          [-1.8722e-01,  1.7946e-01, -5.2563e-01,  ...,  2.2159e+00,\n",
      "           -1.6231e-01, -2.3987e-01],\n",
      "          [ 2.7649e-01, -3.0254e-01,  1.1789e+00,  ...,  5.1044e-01,\n",
      "           -4.8698e-02,  1.3377e+00],\n",
      "          ...,\n",
      "          [ 1.8748e+00, -7.8544e-01,  2.8115e-02,  ...,  1.2942e+00,\n",
      "           -8.9220e-01,  1.3853e+00],\n",
      "          [ 2.2372e+00, -1.1087e-01, -1.8988e-01,  ...,  5.5902e-01,\n",
      "           -3.8751e-01,  8.4666e-01],\n",
      "          [ 2.6981e+00, -5.9885e-01, -8.1044e-06,  ...,  9.8385e-01,\n",
      "           -8.5825e-01,  6.7094e-01]],\n",
      "\n",
      "         [[-3.0022e+00,  3.9174e-01, -1.1223e-02,  ..., -4.7967e-01,\n",
      "           -3.3196e-01,  1.2360e+00],\n",
      "          [ 5.4688e+00,  7.2843e-01, -3.0646e-01,  ..., -6.9659e-01,\n",
      "            2.0842e+00, -3.1209e-01],\n",
      "          [ 4.6728e+00,  8.9727e-01, -1.0430e+00,  ..., -1.5629e-01,\n",
      "           -1.8246e+00,  8.4182e-01],\n",
      "          ...,\n",
      "          [ 3.9265e+00,  8.9734e-01, -3.0358e-01,  ..., -1.8324e-01,\n",
      "            1.8761e+00, -3.6983e-01],\n",
      "          [ 5.6626e+00,  1.6019e+00, -1.1044e+00,  ...,  1.5657e+00,\n",
      "            3.1486e-01,  4.5444e-01],\n",
      "          [ 4.7640e+00,  4.0294e-01, -9.1716e-01,  ..., -1.1075e+00,\n",
      "           -1.3904e-01, -1.9700e-01]],\n",
      "\n",
      "         [[-7.1129e-04, -2.4458e-01,  2.4069e-03,  ..., -1.7966e-01,\n",
      "            3.2623e-01,  7.5991e-02],\n",
      "          [ 4.7459e-01, -2.1584e+00,  5.7037e-01,  ..., -1.8233e+00,\n",
      "            1.2086e+00, -1.3540e+00],\n",
      "          [ 1.7912e-01, -1.6093e+00, -4.0369e-01,  ..., -1.9495e+00,\n",
      "           -8.3254e-01, -3.7306e-02],\n",
      "          ...,\n",
      "          [-2.8004e-01, -3.7315e-01,  4.7064e-01,  ...,  7.7200e-01,\n",
      "           -9.6399e-02, -6.2465e-02],\n",
      "          [-1.3913e-01, -3.9404e-01, -8.9042e-01,  ..., -1.5056e+00,\n",
      "            6.0884e-01,  1.4535e+00],\n",
      "          [ 5.2569e-01, -9.1115e-01, -3.1540e-03,  ...,  2.6696e-01,\n",
      "            5.5103e-01, -7.7528e-01]]]], grad_fn=<PermuteBackward0>), tensor([[[[-1.6154e-02, -2.4324e-02,  4.0457e-03,  ..., -5.0684e-03,\n",
      "           -4.3276e-02,  3.5572e-01],\n",
      "          [ 2.1075e+00,  3.3239e-01,  7.0590e-01,  ..., -6.4442e-01,\n",
      "            2.0325e-01, -1.1933e+00],\n",
      "          [ 2.5973e-01,  1.4189e+00, -6.4489e-01,  ..., -1.0163e+00,\n",
      "            6.2315e-01, -1.7562e+00],\n",
      "          ...,\n",
      "          [ 1.4778e+00,  8.3814e-02, -6.8671e-01,  ..., -1.1482e+00,\n",
      "           -2.9591e-01, -2.1776e-01],\n",
      "          [ 2.2378e-01,  5.5076e-02, -8.2289e-02,  ..., -8.1783e-01,\n",
      "           -7.1799e-02, -1.1758e+00],\n",
      "          [ 3.6818e-01,  6.0979e-01, -9.6424e-01,  ..., -5.0657e-02,\n",
      "           -6.6968e-02, -5.9430e-01]],\n",
      "\n",
      "         [[ 2.8516e-03, -2.1652e-02,  2.0662e-02,  ..., -1.2961e-02,\n",
      "            2.0125e-02, -4.7159e-03],\n",
      "          [ 1.0375e+00, -2.5208e-01,  1.1858e+00,  ..., -1.1227e-01,\n",
      "            1.6027e+00, -5.9035e-01],\n",
      "          [ 2.0631e-01, -2.9578e-01, -2.2777e-01,  ..., -1.5720e+00,\n",
      "            1.5820e+00, -4.7028e-01],\n",
      "          ...,\n",
      "          [ 8.3342e-01, -6.2943e-01,  8.9513e-01,  ..., -7.9693e-01,\n",
      "            1.5179e+00, -3.8773e-01],\n",
      "          [ 1.2586e+00, -5.0771e-01, -5.4175e-01,  ...,  2.8752e-01,\n",
      "            1.6020e+00, -6.1038e-01],\n",
      "          [-6.5668e-01, -5.4717e-01,  5.5254e-01,  ..., -7.9152e-01,\n",
      "            1.3110e+00, -8.4972e-01]],\n",
      "\n",
      "         [[-5.5537e-02,  4.8931e-03, -4.0918e-02,  ..., -3.8937e-02,\n",
      "            3.6257e-04, -8.0482e-02],\n",
      "          [-1.2110e+00, -5.9457e-01, -7.2941e-01,  ..., -6.5566e-01,\n",
      "            1.4824e-01, -1.2141e+00],\n",
      "          [-1.7351e+00, -4.9327e-01,  3.0575e-01,  ...,  4.4212e-01,\n",
      "            6.7854e-01,  1.6899e-01],\n",
      "          ...,\n",
      "          [-8.1976e-01,  3.9100e-03,  4.3525e-01,  ...,  7.0418e-01,\n",
      "           -2.0418e-01, -3.1862e-01],\n",
      "          [ 6.5627e-01, -9.4792e-01,  1.2928e+00,  ...,  4.0322e-01,\n",
      "            3.8351e-01, -3.1098e-01],\n",
      "          [ 1.3689e-01,  6.0351e-01,  1.0294e+00,  ...,  4.0633e-01,\n",
      "            1.6941e-01, -2.8906e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.2976e-01, -2.0310e-01, -7.0779e-02,  ..., -4.7672e-01,\n",
      "            2.2137e-01,  9.6389e-02],\n",
      "          [ 1.2666e+00, -1.3649e+00, -9.1768e-01,  ...,  3.2598e+00,\n",
      "            1.7509e-01,  4.1348e-02],\n",
      "          [ 1.0642e+00, -2.8721e-01, -1.0862e+00,  ...,  1.0351e+00,\n",
      "           -1.2148e-01, -1.6960e+00],\n",
      "          ...,\n",
      "          [ 2.6311e-01, -1.0183e+00, -6.5221e-01,  ...,  2.8931e-01,\n",
      "           -3.1788e-01, -6.2820e-01],\n",
      "          [ 1.2252e+00, -8.1788e-01, -2.3993e-01,  ...,  1.0185e+00,\n",
      "            6.0988e-01, -1.2439e+00],\n",
      "          [ 3.7232e+00, -5.0314e-01, -4.4315e-01,  ...,  1.1738e+00,\n",
      "           -1.7832e-02, -6.0587e-01]],\n",
      "\n",
      "         [[-8.1666e-02, -1.3310e-01, -4.5392e-02,  ..., -1.8094e-01,\n",
      "           -1.4460e-01,  1.1990e-01],\n",
      "          [ 1.9767e-01, -2.0686e-02,  7.5451e-03,  ..., -7.3771e-01,\n",
      "           -3.5278e-01, -1.8229e-01],\n",
      "          [-1.0677e-01, -1.4265e-01, -4.1139e-01,  ..., -5.4943e-02,\n",
      "            1.0752e+00, -1.8821e-03],\n",
      "          ...,\n",
      "          [-3.8723e-01, -1.7497e-01,  1.7842e-01,  ..., -1.2700e+00,\n",
      "            3.4986e-01, -1.2791e-01],\n",
      "          [ 2.8225e-01,  9.2754e-01, -5.8493e-01,  ..., -1.2929e-01,\n",
      "            5.9180e-01, -9.5633e-02],\n",
      "          [-5.2601e-01, -1.5829e-01, -5.5380e-01,  ..., -2.2086e-01,\n",
      "           -8.9115e-01, -8.7194e-02]],\n",
      "\n",
      "         [[-3.1941e-02, -3.7102e-02,  8.7104e-02,  ...,  8.3825e-02,\n",
      "           -3.5994e-02,  2.2552e-02],\n",
      "          [-6.7529e-02,  7.2829e-01, -6.5636e-01,  ..., -1.1950e+00,\n",
      "           -3.0989e-01,  5.2998e-01],\n",
      "          [ 3.7581e-01, -1.7782e-01, -5.9921e-01,  ..., -1.0169e+00,\n",
      "           -1.3930e+00,  9.3052e-01],\n",
      "          ...,\n",
      "          [ 1.5450e-01,  2.0267e-01,  5.5618e-01,  ..., -4.9346e-01,\n",
      "           -1.4179e-01,  9.6428e-01],\n",
      "          [-1.4963e+00,  5.2494e-01,  8.0477e-01,  ...,  6.7948e-01,\n",
      "           -1.1102e+00,  2.0383e+00],\n",
      "          [-1.8790e+00,  2.2361e-01, -4.0352e-01,  ...,  4.1492e-01,\n",
      "           -3.1719e-01,  3.2742e-01]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[-3.3482e-01,  8.5828e-01, -1.5853e-01,  ...,  1.1293e+00,\n",
      "           -1.6286e-01,  1.3779e-01],\n",
      "          [-7.3385e-01, -5.1293e+00, -1.3420e-01,  ..., -3.2350e+00,\n",
      "            1.4706e+00,  7.1443e-01],\n",
      "          [ 1.1276e+00, -3.5899e+00, -2.8202e-01,  ..., -2.4571e+00,\n",
      "            1.2787e-01,  1.4592e+00],\n",
      "          ...,\n",
      "          [-1.8962e-01, -3.6263e+00, -3.1238e-01,  ..., -3.1505e+00,\n",
      "            3.7385e-01, -9.1244e-02],\n",
      "          [-4.2835e-01, -4.0420e+00,  8.1813e-01,  ..., -3.4046e+00,\n",
      "           -8.4866e-01,  9.5906e-01],\n",
      "          [ 6.9933e-01, -4.5508e+00,  3.1178e-01,  ..., -3.7826e+00,\n",
      "            1.2011e+00,  8.3313e-02]],\n",
      "\n",
      "         [[ 4.7401e-02,  8.7476e-01, -6.4117e-01,  ..., -2.1939e-02,\n",
      "            2.8363e-01,  1.4145e-02],\n",
      "          [ 6.1275e-01,  1.2520e+00,  1.3016e+00,  ...,  1.1617e+00,\n",
      "           -1.7080e+00, -1.0072e+00],\n",
      "          [-2.1421e+00,  7.3557e-01,  1.5809e+00,  ...,  1.4965e+00,\n",
      "            1.2102e-02, -4.7807e-01],\n",
      "          ...,\n",
      "          [-2.9112e+00, -8.5782e-01,  7.4075e-01,  ...,  1.8909e+00,\n",
      "            7.5693e-01, -1.9168e-01],\n",
      "          [-1.4354e+00,  1.7861e-01,  1.0104e+00,  ...,  1.7918e+00,\n",
      "            8.0827e-01, -1.8521e+00],\n",
      "          [-4.8108e-01,  6.7107e-01,  1.9948e+00,  ...,  1.3903e+00,\n",
      "            6.5300e-01, -4.7520e-01]],\n",
      "\n",
      "         [[-3.1181e-01,  1.4279e-01, -9.9188e-01,  ..., -3.5612e-01,\n",
      "           -4.8890e-02, -1.3194e-01],\n",
      "          [-6.0227e-01, -6.2857e-01,  3.5674e+00,  ..., -4.8142e-01,\n",
      "           -1.8370e-01, -8.9275e-02],\n",
      "          [ 1.2670e-01,  2.0276e-01,  4.0301e+00,  ...,  8.4419e-01,\n",
      "           -1.1873e+00, -6.9582e-01],\n",
      "          ...,\n",
      "          [-3.0766e-01, -2.5999e-01,  2.7491e+00,  ...,  1.0202e+00,\n",
      "            4.8657e-01,  6.0294e-01],\n",
      "          [-4.9440e-01,  1.1039e+00,  2.7289e+00,  ...,  9.9205e-01,\n",
      "           -5.3449e-01, -4.7665e-02],\n",
      "          [-7.9895e-01,  5.7926e-01,  1.6596e+00,  ...,  1.7603e-01,\n",
      "            7.6233e-01,  4.0313e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.8220e-01,  7.4177e-02, -5.5519e-02,  ..., -4.9372e-02,\n",
      "            2.2708e-01,  1.6501e-02],\n",
      "          [-7.3055e-02, -8.0805e-01,  1.4979e+00,  ..., -3.6545e-01,\n",
      "            1.4223e+00, -3.9321e-01],\n",
      "          [-1.0276e+00, -1.5622e+00, -4.0911e-01,  ..., -1.1011e-01,\n",
      "            7.7091e-01, -7.4014e-01],\n",
      "          ...,\n",
      "          [ 1.0894e+00, -6.7380e-01, -5.2268e-01,  ...,  1.3527e+00,\n",
      "            3.2484e-01, -2.3670e+00],\n",
      "          [-1.6279e+00, -2.4458e-01,  1.5869e-01,  ...,  6.4984e-02,\n",
      "            1.7585e-01,  4.8370e-01],\n",
      "          [-5.9251e-01,  5.7276e-02, -6.9373e-01,  ...,  1.5550e+00,\n",
      "           -2.1464e-01,  8.7871e-01]],\n",
      "\n",
      "         [[ 2.0809e-01,  6.1432e-02,  3.1973e-01,  ...,  4.1752e-01,\n",
      "            9.5741e-03,  2.3014e-01],\n",
      "          [ 1.4143e-01,  6.4253e-01,  6.2522e-01,  ..., -1.5353e+00,\n",
      "            1.8101e-01, -3.8889e-01],\n",
      "          [ 2.5442e+00,  7.3574e-01,  3.2120e-01,  ..., -1.6549e+00,\n",
      "           -1.0973e+00,  9.1917e-01],\n",
      "          ...,\n",
      "          [ 2.4018e+00, -4.1278e-01,  1.8692e+00,  ..., -1.9528e+00,\n",
      "           -1.4570e+00,  1.2236e+00],\n",
      "          [ 2.3562e+00,  7.3343e-01,  1.6958e+00,  ..., -1.6585e+00,\n",
      "           -1.0312e+00,  5.8289e-01],\n",
      "          [ 2.6394e+00, -7.8490e-01,  1.5319e+00,  ..., -6.6944e-01,\n",
      "           -1.7523e-01,  1.0960e+00]],\n",
      "\n",
      "         [[-3.0148e+00,  5.4584e-01,  5.5762e-01,  ..., -9.4472e-01,\n",
      "            3.3175e-01,  1.9976e-01],\n",
      "          [ 7.4210e+00,  1.1603e+00, -2.3412e+00,  ...,  7.0589e-01,\n",
      "           -6.4264e-01,  9.8195e-01],\n",
      "          [ 7.2441e+00,  2.0441e-02, -3.2675e+00,  ...,  1.3967e+00,\n",
      "            1.6647e+00, -7.0157e-01],\n",
      "          ...,\n",
      "          [ 8.2699e+00,  2.2170e-01, -3.2565e+00,  ...,  2.0599e+00,\n",
      "            4.4595e-01,  3.7351e-03],\n",
      "          [ 7.8367e+00, -7.8646e-01, -2.3990e+00,  ...,  1.2168e+00,\n",
      "            1.5381e+00, -9.4327e-02],\n",
      "          [ 8.5858e+00, -1.2253e+00, -2.0003e+00,  ...,  2.7713e+00,\n",
      "           -8.6141e-01, -3.6364e-01]]]], grad_fn=<PermuteBackward0>), tensor([[[[ 3.6623e-02, -5.2531e-02,  1.9742e-02,  ..., -7.7511e-02,\n",
      "            7.2725e-04, -8.8710e-02],\n",
      "          [-9.7288e-01,  9.7550e-02, -1.2793e-01,  ..., -8.6592e-01,\n",
      "            9.6055e-01,  9.4705e-01],\n",
      "          [ 6.0470e-01, -7.4197e-01,  2.8492e-01,  ..., -8.1862e-01,\n",
      "           -2.5122e-01,  3.3637e-01],\n",
      "          ...,\n",
      "          [ 8.4885e-02, -9.5376e-01,  8.2301e-01,  ..., -1.2013e+00,\n",
      "           -2.1742e-01,  4.6894e-01],\n",
      "          [-5.5673e-01, -3.4668e-01, -1.2498e-01,  ...,  8.4895e-02,\n",
      "            1.6590e-01, -2.4329e-01],\n",
      "          [-5.6699e-02, -8.2961e-02, -2.7019e-01,  ...,  1.3005e-01,\n",
      "            6.9359e-01, -1.0336e+00]],\n",
      "\n",
      "         [[ 6.4450e-02,  2.1530e-02, -2.1496e-02,  ..., -2.4472e-02,\n",
      "            1.2992e-02, -4.4305e-03],\n",
      "          [ 2.3571e-01,  2.0766e-02, -1.3801e+00,  ...,  1.3692e+00,\n",
      "            4.9315e-01,  9.9846e-01],\n",
      "          [-1.6701e-01, -9.2045e-02, -1.2245e-01,  ...,  6.0058e-01,\n",
      "            1.2820e+00, -3.5395e-01],\n",
      "          ...,\n",
      "          [-2.2864e-01, -3.1889e-01, -4.3726e-01,  ..., -6.8220e-01,\n",
      "            5.4563e-01, -1.3924e-01],\n",
      "          [-6.7477e-01,  8.5847e-01,  1.2225e+00,  ..., -2.7894e-01,\n",
      "            2.9505e-02, -3.2619e-01],\n",
      "          [-1.8376e-02,  1.3465e-01, -3.8302e-01,  ..., -1.9936e-01,\n",
      "           -1.8827e-01,  5.6655e-01]],\n",
      "\n",
      "         [[ 7.8684e-02,  1.9472e-02,  6.3902e-03,  ...,  1.7769e-02,\n",
      "           -6.6467e-02, -6.4331e-02],\n",
      "          [ 6.6447e-01,  1.5695e-01, -3.0897e-01,  ..., -2.5934e-02,\n",
      "            7.7875e-02,  7.9969e-01],\n",
      "          [-9.1101e-01,  8.4699e-03, -6.1100e-01,  ...,  3.1032e-01,\n",
      "            1.3060e+00,  2.3035e+00],\n",
      "          ...,\n",
      "          [-6.4365e-02, -2.7944e-02, -7.5075e-03,  ...,  2.8388e-01,\n",
      "            1.0270e-01,  1.1703e+00],\n",
      "          [ 4.1661e-01,  2.3543e-01, -3.0824e-01,  ..., -6.0406e-01,\n",
      "            2.0804e+00,  1.1219e+00],\n",
      "          [ 9.0022e-01,  2.2899e-01,  6.6242e-01,  ...,  3.1266e-01,\n",
      "           -1.2449e-01,  2.6343e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 5.2003e-03,  2.2984e-02,  2.6918e-02,  ..., -8.5574e-02,\n",
      "           -2.0333e-02,  2.4534e-02],\n",
      "          [-6.2101e-01, -3.4177e-01, -2.3836e-01,  ...,  1.5253e+00,\n",
      "           -5.2830e-01,  6.2624e-01],\n",
      "          [-1.1685e+00, -2.0089e+00,  3.0326e-01,  ...,  2.1701e-01,\n",
      "           -4.1754e-01,  6.0309e-02],\n",
      "          ...,\n",
      "          [-1.4094e+00, -7.6163e-01, -5.2874e-01,  ...,  8.4009e-01,\n",
      "           -3.0532e-01,  4.1513e-01],\n",
      "          [-3.2228e-01, -6.5825e-01,  3.5221e-01,  ..., -3.5284e-01,\n",
      "            8.1324e-01, -3.6595e-01],\n",
      "          [-2.6580e-01, -1.8477e+00, -6.4743e-01,  ...,  8.9569e-02,\n",
      "           -6.2916e-01,  1.3052e+00]],\n",
      "\n",
      "         [[ 3.6519e-02, -1.5498e-02,  2.4342e-02,  ...,  3.0825e-02,\n",
      "           -7.8301e-03, -2.5398e-03],\n",
      "          [ 2.1977e-02,  5.4030e-01, -1.2397e+00,  ..., -5.6331e-02,\n",
      "            8.2130e-01, -8.4140e-01],\n",
      "          [ 1.7633e-01,  1.2647e+00, -2.8253e-01,  ...,  1.0400e+00,\n",
      "           -9.3328e-01, -4.1916e-03],\n",
      "          ...,\n",
      "          [ 1.1627e+00,  1.5116e+00,  5.4693e-01,  ...,  1.0771e+00,\n",
      "            5.1534e-01, -3.3569e-01],\n",
      "          [ 1.3085e+00,  1.7962e+00,  7.1871e-02,  ...,  1.7234e+00,\n",
      "            1.5339e-01, -7.4687e-03],\n",
      "          [-4.2539e-01,  8.1347e-01, -1.2454e+00,  ...,  2.5234e-01,\n",
      "            1.4081e+00, -1.6478e+00]],\n",
      "\n",
      "         [[ 7.5280e-02, -1.9492e-01, -7.6429e-02,  ..., -1.8599e-02,\n",
      "            1.9642e-01, -4.4872e-02],\n",
      "          [-5.2066e-01, -4.2676e-01, -1.4494e+00,  ...,  5.5261e-02,\n",
      "           -7.3229e-01,  3.5040e-01],\n",
      "          [ 6.6238e-01, -9.1902e-01,  4.5529e-01,  ...,  8.7319e-02,\n",
      "           -5.3366e-01,  7.2359e-02],\n",
      "          ...,\n",
      "          [ 3.3716e-01, -9.0171e-02, -7.1401e-02,  ...,  7.6054e-01,\n",
      "           -2.1491e-01,  6.3359e-01],\n",
      "          [ 6.3230e-01, -6.5134e-01, -3.7061e-01,  ...,  5.7909e-01,\n",
      "           -2.8124e-01,  1.0037e-01],\n",
      "          [-9.5346e-01, -8.4655e-01, -8.3134e-01,  ..., -1.9914e-01,\n",
      "            9.2537e-03,  5.0407e-01]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[ 1.0622e+00, -2.4323e-01, -1.3085e-01,  ...,  6.2919e-01,\n",
      "            7.1191e-01, -2.9884e-01],\n",
      "          [-3.5855e+00, -2.5353e+00,  7.7037e-01,  ...,  8.2905e-01,\n",
      "           -5.8205e+00,  8.9020e-01],\n",
      "          [-3.5743e+00, -1.1161e+00,  1.4131e+00,  ..., -7.5875e-01,\n",
      "           -5.7886e+00, -2.3789e-01],\n",
      "          ...,\n",
      "          [-5.0010e+00, -1.0487e+00,  1.3998e+00,  ..., -1.1483e+00,\n",
      "           -3.9165e+00, -1.4244e+00],\n",
      "          [-3.5776e+00, -2.1408e+00,  2.6862e-01,  ..., -1.2902e+00,\n",
      "           -5.8713e+00, -1.5573e+00],\n",
      "          [-3.4254e+00, -1.1625e+00,  7.0866e-01,  ..., -2.0576e-01,\n",
      "           -4.5088e+00, -7.9936e-01]],\n",
      "\n",
      "         [[-1.4688e-01, -6.8372e-02,  1.6291e-01,  ..., -4.7366e-02,\n",
      "           -8.7257e-01, -2.0452e-01],\n",
      "          [ 1.4799e-01, -2.0140e-01, -4.6879e-01,  ..., -9.2302e-02,\n",
      "           -2.8963e-01, -1.5943e+00],\n",
      "          [-4.4538e-01,  7.4565e-01,  5.1560e-02,  ..., -4.7482e-01,\n",
      "            4.2682e-01,  1.3532e+00],\n",
      "          ...,\n",
      "          [-2.9737e-01, -6.4269e-01,  2.1584e+00,  ..., -8.0010e-02,\n",
      "           -3.7174e-01,  9.9664e-02],\n",
      "          [-5.9062e-03,  1.1669e-01,  1.5913e+00,  ..., -3.5517e-01,\n",
      "           -2.5326e-01,  5.4763e-01],\n",
      "          [ 3.3248e-01,  1.6614e-01,  1.2786e+00,  ..., -1.5105e+00,\n",
      "            4.8657e-02, -1.0194e+00]],\n",
      "\n",
      "         [[ 1.9251e-01,  2.9957e-01,  1.1201e+00,  ..., -4.5921e-01,\n",
      "            4.3891e-01, -5.0343e-01],\n",
      "          [-1.1269e+00, -1.0395e+00, -2.0093e+00,  ..., -1.8079e+00,\n",
      "           -1.2817e+00, -1.2740e-01],\n",
      "          [-6.0963e-01, -1.1446e+00, -5.9599e-01,  ..., -5.5590e-01,\n",
      "           -1.7295e+00,  2.1497e+00],\n",
      "          ...,\n",
      "          [-3.7749e-01, -1.0703e+00, -1.7492e+00,  ...,  1.1796e+00,\n",
      "           -1.7194e+00,  1.4549e+00],\n",
      "          [-6.0639e-01, -7.6828e-01, -2.4446e+00,  ...,  1.4839e-01,\n",
      "           -2.4104e+00,  1.4604e+00],\n",
      "          [-1.7869e-02, -9.0636e-02, -2.9331e+00,  ..., -1.1622e+00,\n",
      "           -1.4258e+00,  3.1314e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.4201e-01,  7.9865e-02, -2.2244e-01,  ...,  6.8028e-03,\n",
      "            1.5653e-01,  1.8582e-02],\n",
      "          [-2.2436e+00, -3.8184e-01, -1.8959e+00,  ...,  1.4808e+00,\n",
      "           -1.6620e+00, -1.2042e+00],\n",
      "          [-3.2492e+00, -8.1916e-02, -7.9666e-01,  ...,  8.2335e-01,\n",
      "           -1.2223e-01, -4.0923e-01],\n",
      "          ...,\n",
      "          [-2.1759e+00, -1.0737e+00,  1.0706e+00,  ...,  1.1066e+00,\n",
      "           -1.5231e+00, -6.1068e-01],\n",
      "          [-3.1344e+00,  6.0297e-01, -3.4324e-01,  ...,  5.2749e-01,\n",
      "           -1.4747e-01, -1.8511e+00],\n",
      "          [-9.4911e-01,  1.7301e+00, -3.7859e-01,  ..., -1.9842e-01,\n",
      "           -9.7358e-01, -1.7117e+00]],\n",
      "\n",
      "         [[-3.5605e-01, -2.1814e+00,  1.2363e-01,  ..., -8.8427e-02,\n",
      "           -4.9201e-02,  9.1038e-01],\n",
      "          [ 1.0831e+00,  3.0001e+00,  1.4076e-02,  ..., -6.5359e-01,\n",
      "            2.6181e-01, -1.4213e+00],\n",
      "          [ 2.6392e-01,  2.9411e+00,  3.7823e-01,  ...,  3.8646e-01,\n",
      "           -8.4837e-01,  2.3346e-01],\n",
      "          ...,\n",
      "          [-2.9887e-01,  4.3147e+00,  2.9049e+00,  ...,  2.5529e-01,\n",
      "            6.0600e-02,  2.2023e+00],\n",
      "          [ 5.0034e-01,  3.0863e+00,  1.3099e+00,  ...,  1.1425e+00,\n",
      "           -1.5001e+00, -1.4241e-01],\n",
      "          [ 4.6364e-01,  1.9611e+00, -2.6739e-01,  ...,  6.5261e-01,\n",
      "           -2.6123e+00,  7.5221e-01]],\n",
      "\n",
      "         [[ 3.7338e-01,  6.6345e-02, -1.4785e-01,  ...,  6.4116e-01,\n",
      "            1.3627e-01,  2.6404e-01],\n",
      "          [-9.3303e-01, -1.3735e+00, -6.3368e-01,  ..., -1.2171e+00,\n",
      "            1.0816e+00, -1.2383e+00],\n",
      "          [ 8.6730e-01, -3.6169e-01,  4.8072e-01,  ...,  3.3191e-01,\n",
      "            4.7317e-01, -2.8533e-02],\n",
      "          ...,\n",
      "          [ 2.0663e-01,  2.9921e-01, -9.5355e-01,  ...,  1.3655e+00,\n",
      "            1.5235e+00,  4.1158e-01],\n",
      "          [-2.5282e+00,  1.3888e+00,  5.9832e-01,  ...,  5.6720e-01,\n",
      "            1.0474e+00,  5.6120e-01],\n",
      "          [-1.6388e+00,  3.3981e+00, -1.9926e-03,  ..., -2.8102e-01,\n",
      "            7.4000e-01, -7.9052e-01]]]], grad_fn=<PermuteBackward0>), tensor([[[[-3.4548e-02,  5.3397e-02, -6.4730e-02,  ..., -2.3077e-02,\n",
      "            5.7571e-05,  2.0528e-02],\n",
      "          [ 1.6396e+00, -4.8811e-01, -9.9508e-03,  ...,  1.7677e-01,\n",
      "           -4.7795e-01, -1.0079e+00],\n",
      "          [-2.8697e-01,  1.1344e-01, -4.9888e-01,  ...,  6.7768e-03,\n",
      "           -4.4369e-01, -2.8803e-01],\n",
      "          ...,\n",
      "          [-2.6596e-01, -1.0271e-01, -3.9434e-01,  ...,  3.8971e-01,\n",
      "           -5.0229e-01,  3.0224e-01],\n",
      "          [-2.5514e-01, -1.0779e-01, -2.2035e-01,  ...,  1.8765e-01,\n",
      "           -4.9432e-01,  7.5763e-01],\n",
      "          [-5.2694e-01,  5.6019e-01, -1.6829e-01,  ..., -7.4336e-01,\n",
      "           -1.9421e-01, -2.9857e-01]],\n",
      "\n",
      "         [[ 1.1421e-02, -2.3286e-02,  2.4031e-02,  ...,  1.9458e-02,\n",
      "           -5.3157e-02,  1.4023e-02],\n",
      "          [-3.8006e-01, -9.7725e-01, -1.2923e+00,  ...,  7.0721e-01,\n",
      "            3.2788e-01, -4.0794e-01],\n",
      "          [ 3.8605e-01,  1.0538e+00,  4.5280e-01,  ...,  4.6869e-01,\n",
      "           -1.7468e-01, -6.9764e-02],\n",
      "          ...,\n",
      "          [-1.5275e-01,  6.4661e-02, -1.3746e+00,  ...,  3.6357e-01,\n",
      "           -1.6532e-01, -7.0445e-01],\n",
      "          [ 1.5834e+00,  4.7908e-01, -2.1951e+00,  ..., -7.1123e-01,\n",
      "            4.9889e-01,  1.8200e-01],\n",
      "          [ 6.6054e-01,  2.0032e-01, -1.3051e+00,  ...,  1.0871e+00,\n",
      "            9.5369e-01,  5.8053e-01]],\n",
      "\n",
      "         [[ 4.1737e-02, -2.4639e-02,  5.5960e-02,  ...,  2.9685e-02,\n",
      "           -5.1147e-03, -2.1416e-03],\n",
      "          [-3.6082e-01,  1.1219e+00,  2.5839e-01,  ..., -1.1847e+00,\n",
      "            4.4165e-01, -3.9130e-01],\n",
      "          [ 2.6447e-02,  8.1012e-01, -1.2078e+00,  ...,  1.2943e+00,\n",
      "           -2.7931e-01,  1.5785e+00],\n",
      "          ...,\n",
      "          [-6.7659e-01, -5.7791e-01,  1.3315e-01,  ...,  5.2287e-01,\n",
      "            5.0035e-01, -7.4863e-02],\n",
      "          [ 1.4293e+00, -1.6677e+00,  4.0782e-01,  ...,  8.3133e-01,\n",
      "            4.0289e-02,  2.8899e-01],\n",
      "          [-2.0438e-01, -2.4394e-01,  7.0181e-01,  ..., -8.0756e-01,\n",
      "           -6.4599e-02,  1.7995e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.8734e-01,  7.9658e-02,  5.4667e-02,  ...,  4.3881e-02,\n",
      "            4.9607e-02, -1.2179e-01],\n",
      "          [-2.4291e-01, -2.3118e-01, -1.4433e-01,  ..., -2.6102e-01,\n",
      "            8.7648e-01,  1.4179e+00],\n",
      "          [ 1.5885e-01,  5.0671e-01, -5.5144e-01,  ...,  6.0489e-01,\n",
      "            4.8461e-01,  4.1415e-01],\n",
      "          ...,\n",
      "          [ 1.4839e+00,  8.9808e-01, -4.7551e-01,  ...,  1.3906e+00,\n",
      "            5.0490e-01,  8.4300e-01],\n",
      "          [-6.6189e-01,  8.5713e-01, -2.2219e+00,  ...,  9.9652e-01,\n",
      "           -6.0912e-01,  1.4610e-01],\n",
      "          [ 6.4920e-01,  7.5266e-02, -1.2200e+00,  ..., -2.1853e-01,\n",
      "            6.9125e-01,  8.3183e-01]],\n",
      "\n",
      "         [[-5.9185e-01, -1.7405e-05,  4.2870e-02,  ..., -1.6821e-02,\n",
      "            1.4827e-02, -6.8937e-03],\n",
      "          [-1.4579e+00, -1.5723e-01, -5.7404e-01,  ..., -1.8376e-01,\n",
      "            9.4737e-01,  4.8910e-01],\n",
      "          [-1.1835e+00, -1.0474e+00, -4.9270e-01,  ...,  7.0043e-01,\n",
      "            4.6974e-01, -1.6851e+00],\n",
      "          ...,\n",
      "          [-1.5753e+00,  5.2141e-01, -6.8034e-01,  ...,  9.2943e-01,\n",
      "           -1.8067e-01, -1.1223e-01],\n",
      "          [-1.6615e+00, -7.2187e-01, -1.3928e+00,  ..., -6.9692e-01,\n",
      "           -7.6497e-01, -1.2867e+00],\n",
      "          [-2.3243e+00,  7.3114e-01, -5.8834e-01,  ...,  3.8245e-01,\n",
      "            5.9070e-01,  8.7980e-01]],\n",
      "\n",
      "         [[-2.5289e-03,  8.2326e-02, -4.4994e-02,  ...,  5.7166e-02,\n",
      "            3.3988e-02, -4.4381e-02],\n",
      "          [-1.8215e+00,  6.7447e-01,  1.4626e-01,  ..., -2.0834e-01,\n",
      "           -6.8011e-01, -9.1083e-02],\n",
      "          [-1.7768e-01, -6.7572e-01, -1.1613e+00,  ...,  4.3582e-01,\n",
      "           -9.2206e-01, -1.0331e-02],\n",
      "          ...,\n",
      "          [-6.1787e-01, -4.2050e-01,  1.1709e+00,  ..., -3.2984e-01,\n",
      "           -1.1966e+00, -1.3459e-01],\n",
      "          [ 5.2116e-01, -7.1142e-01,  8.7801e-02,  ...,  2.5684e-01,\n",
      "           -1.5464e+00,  9.0607e-01],\n",
      "          [-4.4611e-01, -3.0615e-02,  9.0717e-01,  ..., -3.4028e-01,\n",
      "            6.6750e-01,  1.4592e+00]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[-3.7207e-02, -2.3316e+00,  1.6703e-01,  ..., -2.2565e-01,\n",
      "           -1.9570e-01,  7.1027e-02],\n",
      "          [-1.1617e+00,  5.7819e+00,  4.6491e-01,  ...,  8.7886e-01,\n",
      "           -4.3816e-01,  4.5943e-01],\n",
      "          [-4.2979e-01,  4.2397e+00, -1.1994e+00,  ...,  3.5809e-01,\n",
      "           -1.4994e+00,  6.7624e-01],\n",
      "          ...,\n",
      "          [ 3.2663e-01,  4.1137e+00,  7.4487e-01,  ...,  1.0355e-01,\n",
      "           -7.1495e-01,  2.4164e-01],\n",
      "          [ 4.3679e-01,  4.4497e+00, -3.8439e-01,  ..., -8.8988e-01,\n",
      "           -2.2552e+00,  9.7584e-01],\n",
      "          [-1.5527e+00,  3.7683e+00,  9.7823e-01,  ..., -6.7632e-01,\n",
      "           -4.9476e-01, -1.1217e-01]],\n",
      "\n",
      "         [[-7.9816e-01,  2.2714e-01,  4.6665e-01,  ..., -5.2178e-01,\n",
      "            1.0731e+00,  1.1169e+00],\n",
      "          [ 6.0558e-01,  5.8535e-01,  7.0104e-01,  ..., -1.7125e-02,\n",
      "            1.8413e+00, -2.3594e+00],\n",
      "          [ 5.2231e-02, -7.0835e-01,  6.8880e-01,  ...,  6.1963e-01,\n",
      "            1.3369e+00, -7.8483e-02],\n",
      "          ...,\n",
      "          [ 1.5261e+00,  1.1631e+00,  1.0220e+00,  ...,  9.6928e-01,\n",
      "            6.1072e-01,  8.9004e-01],\n",
      "          [ 5.1967e-01,  9.8946e-01, -4.1932e-01,  ...,  9.8320e-01,\n",
      "            2.7651e+00,  8.3823e-02],\n",
      "          [ 1.0032e+00,  6.3827e-01,  6.4666e-01,  ...,  1.7658e-01,\n",
      "            2.5239e+00,  3.5147e-01]],\n",
      "\n",
      "         [[-8.5172e-01,  4.6461e-01,  2.0544e-02,  ...,  4.9692e-01,\n",
      "           -2.3368e-01,  1.1502e+00],\n",
      "          [ 4.4905e-01, -2.2600e+00, -1.6843e-01,  ..., -7.2168e-01,\n",
      "            4.8860e-01,  7.7545e-01],\n",
      "          [ 2.3175e+00,  5.5396e-01, -5.1860e-02,  ...,  2.2043e+00,\n",
      "           -3.3749e-01,  7.5064e-01],\n",
      "          ...,\n",
      "          [ 1.1443e+00, -5.4907e-01, -1.3156e-02,  ...,  1.0815e+00,\n",
      "            4.3771e-02,  5.5785e-01],\n",
      "          [ 2.2766e+00, -1.2869e-01, -9.5749e-04,  ...,  1.3608e+00,\n",
      "           -2.9476e-01,  2.6347e-01],\n",
      "          [ 5.3784e-01, -5.2174e-01, -6.0712e-01,  ...,  4.7690e-01,\n",
      "            3.6724e-01,  8.0862e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.0616e-01, -1.4787e-01,  1.3669e-01,  ...,  1.8873e-01,\n",
      "            1.7284e+00, -2.8581e+00],\n",
      "          [-3.8468e-01, -2.4209e+00, -2.0211e+00,  ..., -4.2279e-02,\n",
      "           -5.5258e+00,  6.0159e+00],\n",
      "          [ 4.7562e-01, -5.9414e-01,  2.3329e-01,  ...,  4.6957e-01,\n",
      "           -4.2637e+00,  4.0592e+00],\n",
      "          ...,\n",
      "          [ 4.0728e-01,  1.1867e-01,  1.8816e-01,  ..., -1.8224e-01,\n",
      "           -3.8168e+00,  4.0095e+00],\n",
      "          [-7.5770e-02,  2.7630e-01, -4.2721e-01,  ..., -4.6696e-01,\n",
      "           -3.8459e+00,  3.5987e+00],\n",
      "          [-9.9748e-01, -1.5461e+00,  1.5974e-01,  ...,  1.7373e-01,\n",
      "           -5.0581e+00,  5.8954e+00]],\n",
      "\n",
      "         [[ 1.8310e-01,  3.6814e-01,  2.2611e-01,  ..., -2.1254e-01,\n",
      "            2.7709e-02, -1.4778e-01],\n",
      "          [-1.0061e+00,  1.3695e-01, -7.0975e-01,  ...,  1.4483e+00,\n",
      "            3.8641e-01,  1.5701e-01],\n",
      "          [-1.7147e+00, -4.0835e-01, -9.1268e-01,  ..., -1.2419e+00,\n",
      "            5.9121e-01,  1.6165e-01],\n",
      "          ...,\n",
      "          [-1.1759e+00,  3.2263e-01, -9.3597e-01,  ...,  1.4734e-01,\n",
      "            1.8565e+00, -1.4806e-01],\n",
      "          [-2.2313e+00,  1.1912e-02, -8.5720e-01,  ..., -2.7444e-01,\n",
      "            4.0173e-01, -1.1884e-01],\n",
      "          [-6.2753e-02,  5.1046e-01, -5.3795e-01,  ...,  1.4632e+00,\n",
      "            7.0864e-02, -3.8610e-02]],\n",
      "\n",
      "         [[ 3.6597e-01,  1.1065e-01,  6.1236e-01,  ...,  5.2692e-01,\n",
      "            5.7217e-01, -3.3553e-01],\n",
      "          [-6.9139e-01, -1.7580e+00, -1.0766e+00,  ..., -9.4145e-01,\n",
      "           -3.9056e+00,  5.2198e-01],\n",
      "          [ 1.8071e+00,  3.3615e-01, -7.1925e-01,  ..., -1.4615e+00,\n",
      "           -3.1846e+00,  8.6247e-01],\n",
      "          ...,\n",
      "          [ 1.8149e+00, -3.2714e-01, -2.1500e-03,  ...,  5.8732e-01,\n",
      "           -4.5655e+00,  1.1112e+00],\n",
      "          [ 1.0082e+00,  4.4798e-01, -1.2183e+00,  ...,  3.0995e-01,\n",
      "           -4.4124e+00, -9.2812e-02],\n",
      "          [ 5.2579e-02, -6.6877e-01,  4.3503e-01,  ..., -7.5357e-01,\n",
      "           -4.1590e+00,  5.1833e-01]]]], grad_fn=<PermuteBackward0>), tensor([[[[ 5.4477e-02, -1.0079e-02, -7.1998e-03,  ...,  1.2584e-01,\n",
      "           -6.7115e-02, -3.4897e-02],\n",
      "          [-1.3886e+00,  1.2505e-01,  1.5337e+00,  ...,  2.8176e-02,\n",
      "            1.3031e+00,  8.0599e-01],\n",
      "          [-9.7120e-01,  2.3579e-01,  2.0918e-01,  ..., -1.3661e-01,\n",
      "           -1.3911e-01,  3.0666e-01],\n",
      "          ...,\n",
      "          [-1.5130e+00,  1.3099e+00,  3.7635e-01,  ...,  1.0479e-01,\n",
      "           -1.5613e-01, -3.7426e-01],\n",
      "          [-2.2269e+00,  5.4880e-01, -1.6555e-01,  ..., -1.9958e-01,\n",
      "            5.1361e-01,  1.3968e+00],\n",
      "          [-7.2444e-01,  2.3468e-01,  1.2650e-01,  ...,  7.0135e-01,\n",
      "           -9.8300e-01, -5.6617e-01]],\n",
      "\n",
      "         [[-6.2821e-04,  3.5647e-02,  4.8210e-02,  ..., -7.1052e-03,\n",
      "           -7.6824e-03,  1.6442e-03],\n",
      "          [-6.4987e-01, -7.6724e-01,  9.8571e-01,  ..., -1.4795e+00,\n",
      "           -9.2898e-01, -9.5713e-01],\n",
      "          [-8.5396e-01,  1.1841e+00,  9.4399e-01,  ..., -1.3180e+00,\n",
      "           -1.4074e+00,  1.7183e+00],\n",
      "          ...,\n",
      "          [-8.4935e-01,  6.0559e-01, -1.3319e-01,  ...,  7.4473e-02,\n",
      "            3.4940e-01,  3.1393e-01],\n",
      "          [-7.7478e-01, -1.1393e-01, -8.1701e-01,  ...,  8.2333e-01,\n",
      "            2.7810e-01, -5.0121e-01],\n",
      "          [-1.2348e+00,  2.9091e-02, -2.1150e-01,  ..., -6.8804e-01,\n",
      "           -3.4438e-01, -2.4307e-01]],\n",
      "\n",
      "         [[ 5.2694e-02, -4.0038e-02,  6.1282e-02,  ...,  4.4516e-02,\n",
      "           -6.2781e-02, -6.8190e-02],\n",
      "          [-4.9701e-01, -1.0709e-01,  4.2327e-01,  ...,  2.6530e-01,\n",
      "            1.9708e-01,  9.3207e-01],\n",
      "          [-1.3805e+00, -7.6684e-02,  7.9675e-01,  ..., -1.3720e+00,\n",
      "           -4.6293e-01,  1.4489e-01],\n",
      "          ...,\n",
      "          [-7.9975e-01,  2.5503e-01,  4.0074e-01,  ...,  3.4839e-01,\n",
      "            1.9658e-01,  1.0246e-01],\n",
      "          [-2.0658e+00,  1.2563e+00,  1.1194e+00,  ..., -1.2582e+00,\n",
      "           -8.3095e-01, -7.5707e-01],\n",
      "          [-5.6611e-01,  3.4331e-01,  5.5608e-01,  ..., -1.0000e+00,\n",
      "            2.0445e-01,  4.4600e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-9.5199e-02, -4.4339e-02,  4.9220e-02,  ..., -8.3539e-02,\n",
      "            5.0388e-02,  7.1123e-03],\n",
      "          [-1.0189e-01, -8.8846e-01,  2.4153e+00,  ..., -4.0560e-02,\n",
      "            1.2145e+00, -5.5087e-01],\n",
      "          [ 1.0599e+00,  1.3258e+00, -5.2393e-01,  ..., -6.0868e-01,\n",
      "            9.2151e-01,  5.4656e-04],\n",
      "          ...,\n",
      "          [ 8.0069e-01,  1.1239e+00, -1.6939e+00,  ..., -5.6439e-02,\n",
      "            8.9284e-01, -2.1120e-01],\n",
      "          [-1.0811e+00,  5.4340e-01, -4.4467e-01,  ...,  7.1408e-01,\n",
      "            1.3834e+00,  4.1413e-02],\n",
      "          [-1.3071e-01, -1.3592e-01,  5.2612e-01,  ...,  2.2801e-02,\n",
      "           -3.6289e-01,  1.5982e-02]],\n",
      "\n",
      "         [[ 1.3284e-01, -7.5168e-02,  1.2040e-01,  ...,  5.7788e-02,\n",
      "            2.8678e-02, -1.3395e-01],\n",
      "          [-1.6283e+00,  2.6347e-01, -5.2707e-01,  ...,  7.6813e-02,\n",
      "            5.4004e-01,  2.7725e-02],\n",
      "          [-1.0973e+00,  9.4864e-02, -7.5633e-01,  ...,  8.6267e-01,\n",
      "           -9.3799e-01,  3.8082e-01],\n",
      "          ...,\n",
      "          [ 4.8343e-01,  1.2756e+00, -1.1119e+00,  ..., -4.3046e-01,\n",
      "           -2.2356e+00,  1.0161e+00],\n",
      "          [ 7.1817e-01,  1.7461e+00,  4.0816e-02,  ..., -1.8153e+00,\n",
      "           -2.3039e+00,  1.2382e+00],\n",
      "          [-6.2281e-01,  5.6792e-01, -1.6507e+00,  ..., -1.7121e+00,\n",
      "           -2.5794e-01,  7.3141e-01]],\n",
      "\n",
      "         [[ 2.1119e-01, -4.4821e-02, -5.3789e-02,  ...,  4.3649e-02,\n",
      "            5.0222e-02,  2.0442e-02],\n",
      "          [-3.4639e-01, -1.1319e+00,  3.8547e-01,  ...,  6.3541e-03,\n",
      "           -9.0637e-01,  9.0626e-02],\n",
      "          [ 1.1983e+00,  9.1804e-01,  1.4268e+00,  ..., -9.3963e-01,\n",
      "            1.5999e+00, -7.6144e-01],\n",
      "          ...,\n",
      "          [-6.2813e-03,  1.3180e-01,  1.6269e+00,  ..., -9.7581e-01,\n",
      "           -5.5297e-01, -2.2723e-01],\n",
      "          [ 9.8678e-01,  1.9777e-01,  4.6067e-02,  ..., -3.6008e-01,\n",
      "           -1.3234e+00, -8.0070e-01],\n",
      "          [ 7.7375e-01,  1.1837e+00,  9.3078e-01,  ..., -6.4503e-03,\n",
      "           -2.8351e-01,  9.0372e-01]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[ 2.0048e-02, -2.4519e-01, -4.4249e-01,  ...,  3.2120e-01,\n",
      "            3.1733e-01,  3.7760e-01],\n",
      "          [-4.7956e-01,  2.0112e-01, -6.2867e-01,  ...,  1.5845e+00,\n",
      "           -7.7267e-01, -1.5101e+00],\n",
      "          [ 1.4418e+00, -9.8136e-01, -2.0040e+00,  ..., -1.5228e+00,\n",
      "           -7.8220e-01,  1.1354e+00],\n",
      "          ...,\n",
      "          [ 5.4786e-01, -7.6154e-01, -1.3852e+00,  ..., -9.7375e-01,\n",
      "            9.4619e-01,  1.6077e+00],\n",
      "          [ 1.1833e+00, -1.2055e+00, -1.2465e+00,  ..., -2.1387e+00,\n",
      "            8.1566e-01,  7.6337e-01],\n",
      "          [ 8.7936e-01, -7.2027e-01, -3.9889e-01,  ..., -7.7523e-02,\n",
      "           -7.5766e-01,  7.2990e-01]],\n",
      "\n",
      "         [[-2.8365e-01,  1.6541e-01,  1.1532e-01,  ...,  4.8897e-02,\n",
      "           -1.1488e+00, -1.5094e-01],\n",
      "          [ 4.8388e-01, -1.4985e+00, -3.7533e-01,  ...,  9.8088e-01,\n",
      "            4.3544e-02, -2.4499e-01],\n",
      "          [-1.7166e+00,  6.6593e-01, -6.7068e-01,  ...,  1.1564e+00,\n",
      "           -2.8418e-02,  1.9959e+00],\n",
      "          ...,\n",
      "          [-7.3802e-01, -3.9069e-01,  1.1501e+00,  ...,  9.3964e-01,\n",
      "           -3.6733e-01,  2.0080e+00],\n",
      "          [-1.1246e+00,  2.2817e-01, -6.7411e-01,  ...,  1.1683e+00,\n",
      "           -4.9866e-01,  2.5428e+00],\n",
      "          [-2.0976e+00,  7.2382e-01, -1.9695e-01,  ..., -2.5367e-01,\n",
      "            2.0174e+00,  1.7711e-01]],\n",
      "\n",
      "         [[-1.2462e+00, -1.0842e-01,  5.5454e-01,  ..., -6.5962e-01,\n",
      "            4.7463e-01, -2.7957e-01],\n",
      "          [ 1.2219e+00, -1.3565e-02, -1.8268e-01,  ..., -1.0927e-01,\n",
      "            6.8062e-01, -3.8454e-01],\n",
      "          [ 6.2731e-01,  1.3723e+00,  1.0831e+00,  ...,  1.2130e+00,\n",
      "           -9.4955e-01,  3.9851e-02],\n",
      "          ...,\n",
      "          [ 8.9982e-01,  6.2094e-01, -6.5763e-01,  ...,  1.9411e-01,\n",
      "            1.2959e-01,  8.5158e-01],\n",
      "          [ 5.6330e-01,  1.1120e+00,  3.3892e-01,  ..., -1.8766e-01,\n",
      "            2.4930e-01,  1.6307e+00],\n",
      "          [ 1.5157e+00,  1.2802e+00,  1.7649e-01,  ..., -1.7989e-01,\n",
      "           -3.1743e-01,  1.3446e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 7.8925e-01, -9.1099e-01, -3.9517e-01,  ..., -1.0467e+00,\n",
      "           -4.1457e-01,  4.9478e-01],\n",
      "          [ 1.1100e+00, -1.0900e+00, -1.2293e+00,  ...,  2.4606e-02,\n",
      "            1.3240e+00, -9.5905e-01],\n",
      "          [-2.3523e-01, -1.2066e+00, -2.2501e+00,  ..., -6.2365e-01,\n",
      "           -1.5222e-01, -9.8355e-01],\n",
      "          ...,\n",
      "          [ 8.3029e-01, -8.9178e-02, -1.4560e+00,  ..., -9.6452e-01,\n",
      "           -1.5364e+00, -1.5963e+00],\n",
      "          [ 1.3894e+00, -1.0217e+00, -1.8544e+00,  ..., -3.2119e-01,\n",
      "           -1.7235e+00, -9.1478e-01],\n",
      "          [ 1.4120e+00, -1.8935e-01, -8.6996e-01,  ...,  1.5584e+00,\n",
      "           -2.3751e-01, -4.0418e-01]],\n",
      "\n",
      "         [[-9.1097e-01,  2.5492e+00,  3.2162e-01,  ...,  3.5468e-01,\n",
      "            1.9495e+00, -5.2428e-01],\n",
      "          [-6.1980e-04, -4.8603e+00, -6.1537e-02,  ...,  8.3505e-01,\n",
      "           -3.5077e+00,  2.9900e+00],\n",
      "          [ 1.6124e-01, -3.3156e+00,  6.9126e-01,  ..., -1.3009e-01,\n",
      "           -2.5798e+00,  1.1434e+00],\n",
      "          ...,\n",
      "          [ 5.1392e-01, -2.3876e+00,  8.6221e-01,  ...,  5.4030e-01,\n",
      "           -3.3747e+00,  5.4270e-01],\n",
      "          [ 1.5037e-01, -3.5029e+00,  7.2642e-01,  ...,  8.1861e-01,\n",
      "           -2.9153e+00, -1.6575e-01],\n",
      "          [ 1.0648e-02, -3.2802e+00,  1.2746e-01,  ..., -3.4161e-02,\n",
      "           -3.9063e+00,  9.6344e-01]],\n",
      "\n",
      "         [[-2.0122e+00, -3.6749e-01, -1.1128e+00,  ..., -3.8888e-01,\n",
      "            5.8251e-02,  2.5243e-01],\n",
      "          [ 3.7357e+00,  4.8627e-01,  2.6720e+00,  ...,  2.7053e-01,\n",
      "            9.4404e-01,  1.5985e+00],\n",
      "          [ 1.6714e+00,  2.0198e-01,  6.9839e-01,  ..., -2.9328e-01,\n",
      "           -4.2131e-01,  9.0508e-01],\n",
      "          ...,\n",
      "          [ 3.0977e+00, -7.2126e-01,  6.1069e-01,  ..., -2.7743e+00,\n",
      "            1.9017e-01, -4.2569e-01],\n",
      "          [ 2.2055e+00, -5.9940e-01,  1.4640e+00,  ..., -6.1568e-01,\n",
      "           -3.8462e-02, -1.0012e+00],\n",
      "          [ 3.6170e+00,  3.1186e-01,  2.2378e+00,  ..., -1.7102e+00,\n",
      "           -6.8550e-01, -6.7543e-02]]]], grad_fn=<PermuteBackward0>), tensor([[[[-5.5811e-02, -8.1161e-02,  3.5665e-02,  ...,  1.0902e-01,\n",
      "           -2.7879e-02,  2.5281e-02],\n",
      "          [-9.0960e-01, -3.8608e-01,  1.2446e-01,  ...,  3.0909e-01,\n",
      "            1.1499e+00,  3.5506e-01],\n",
      "          [-7.4317e-01, -3.7660e-02,  2.5584e-01,  ...,  3.3733e-01,\n",
      "           -4.3523e-03,  1.0116e-01],\n",
      "          ...,\n",
      "          [-7.7130e-01,  3.1267e-02, -6.8241e-01,  ...,  4.8133e-01,\n",
      "            8.2489e-01, -4.7745e-01],\n",
      "          [ 6.5649e-01, -4.4844e-01, -9.6256e-01,  ..., -1.0070e+00,\n",
      "            2.6519e-01,  1.2390e+00],\n",
      "          [ 1.2554e-01, -6.2507e-01, -8.0511e-01,  ...,  2.2184e-01,\n",
      "            1.4366e-03, -7.6317e-01]],\n",
      "\n",
      "         [[ 1.6063e-02,  7.9707e-03, -3.7657e-02,  ...,  2.3601e-02,\n",
      "            1.4850e-02,  4.8284e-02],\n",
      "          [-6.0881e-02, -1.3131e+00, -6.0873e-01,  ...,  1.8584e-01,\n",
      "            3.6074e-01, -3.8274e-01],\n",
      "          [-1.5878e+00,  2.0528e+00, -4.2970e-01,  ..., -7.5051e-01,\n",
      "            5.4841e-01,  2.2237e+00],\n",
      "          ...,\n",
      "          [ 8.7220e-02,  6.0580e-01, -1.6239e+00,  ...,  1.3238e+00,\n",
      "            5.5440e-02, -1.7474e-01],\n",
      "          [-3.6177e-02,  1.5872e+00, -5.4881e-01,  ...,  3.8778e-01,\n",
      "            7.6063e-01, -1.2272e+00],\n",
      "          [-6.9744e-01,  1.4850e-02,  4.5608e-02,  ...,  8.4212e-01,\n",
      "            6.5791e-01, -4.5026e-01]],\n",
      "\n",
      "         [[ 4.5020e-02,  4.0703e-02, -8.5913e-02,  ..., -2.0362e-03,\n",
      "           -1.8462e-02,  1.6405e-04],\n",
      "          [ 1.6720e-01,  4.8971e-01,  3.9579e-01,  ..., -2.8610e-03,\n",
      "           -6.2666e-01, -1.1864e+00],\n",
      "          [ 4.9711e-02, -4.5269e-01, -1.5426e+00,  ...,  6.3945e-02,\n",
      "            7.7366e-01, -3.9650e-01],\n",
      "          ...,\n",
      "          [ 7.2350e-01, -1.4624e-01, -2.3485e-02,  ..., -7.4050e-01,\n",
      "           -6.9769e-02, -5.2391e-01],\n",
      "          [ 4.8700e-01,  4.1583e-01, -1.2641e+00,  ..., -1.7825e+00,\n",
      "           -3.8960e-01, -3.9705e-01],\n",
      "          [ 1.9475e+00,  6.2432e-01, -2.6233e-01,  ..., -1.4964e-01,\n",
      "            4.1958e-01, -6.7410e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.0051e-03,  3.0031e-02, -6.7098e-03,  ..., -2.2071e-02,\n",
      "           -2.5694e-02,  3.6145e-02],\n",
      "          [ 5.0417e-01,  1.1350e+00,  3.9957e-01,  ...,  5.8380e-02,\n",
      "           -1.8627e-01,  5.7473e-01],\n",
      "          [-6.6580e-01,  3.3979e-01, -5.0136e-02,  ...,  9.6055e-01,\n",
      "            8.5829e-02, -4.9096e-01],\n",
      "          ...,\n",
      "          [ 2.9450e-01, -6.1851e-02, -4.3595e-01,  ..., -1.0011e+00,\n",
      "           -6.6519e-01, -2.4951e-01],\n",
      "          [ 4.7731e-01,  5.6563e-01,  2.0658e-01,  ..., -1.5315e+00,\n",
      "            5.8052e-01, -9.5938e-01],\n",
      "          [ 7.7956e-01,  1.3807e-01,  7.8524e-01,  ..., -7.3850e-01,\n",
      "            3.1660e-01, -9.8107e-01]],\n",
      "\n",
      "         [[-6.5377e-02, -4.5243e-02,  2.9972e-02,  ...,  1.7289e-02,\n",
      "           -4.0501e-02, -1.0150e-01],\n",
      "          [ 3.3697e-01,  1.1677e-01, -8.0097e-01,  ...,  7.6040e-01,\n",
      "            1.3982e-01, -1.2177e-01],\n",
      "          [-1.5240e-01, -8.5122e-01,  2.2739e-01,  ...,  3.9852e-02,\n",
      "           -6.3272e-01, -1.0419e-01],\n",
      "          ...,\n",
      "          [ 3.2864e-01, -3.8063e-01, -1.1536e+00,  ...,  2.8569e-01,\n",
      "           -4.8180e-01,  4.8347e-01],\n",
      "          [-3.7271e-02, -7.7108e-01, -1.2393e+00,  ..., -1.8304e-01,\n",
      "           -4.1910e-01, -1.1904e-01],\n",
      "          [-1.6454e-01, -2.6413e-02,  8.6452e-02,  ...,  2.2575e+00,\n",
      "            1.9995e-01,  1.4322e+00]],\n",
      "\n",
      "         [[-1.1829e-02,  3.3011e-02, -4.7851e-02,  ..., -7.7910e-03,\n",
      "            1.3503e-02,  1.1945e-02],\n",
      "          [ 9.6119e-02, -3.9244e-01, -6.0998e-01,  ..., -1.6956e-01,\n",
      "            1.0601e+00, -3.7701e-01],\n",
      "          [-1.0333e+00, -1.0666e+00, -1.9146e+00,  ...,  1.6704e+00,\n",
      "           -1.3921e+00,  1.6433e-01],\n",
      "          ...,\n",
      "          [ 8.8033e-01,  9.6307e-02, -2.6994e-01,  ...,  6.8796e-01,\n",
      "           -2.2137e-01, -6.1842e-01],\n",
      "          [-1.2989e+00, -9.3259e-01, -2.5020e-01,  ...,  8.7767e-01,\n",
      "           -8.2240e-01,  1.2617e+00],\n",
      "          [-3.5339e-01, -1.2871e+00, -7.6167e-01,  ...,  5.0202e-01,\n",
      "            6.3385e-01, -5.9363e-01]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[-0.5105,  0.4888, -0.8554,  ..., -1.0267, -1.3161,  0.2115],\n",
      "          [ 0.2877,  1.1726,  0.3502,  ...,  1.8822,  0.3735, -1.5771],\n",
      "          [-0.1310,  0.0584, -0.3570,  ...,  0.7754, -0.5054, -0.8701],\n",
      "          ...,\n",
      "          [-0.3379,  0.4844,  0.7754,  ...,  1.7417,  0.4086, -0.3045],\n",
      "          [-1.1033,  2.2741, -0.3488,  ...,  1.3230,  0.8851,  0.1877],\n",
      "          [ 0.0484,  0.5209,  0.5654,  ...,  1.4024,  0.4582,  0.1081]],\n",
      "\n",
      "         [[ 0.8641, -2.0632,  0.1507,  ...,  0.2358, -2.4628, -0.4365],\n",
      "          [ 0.9688,  2.2874,  0.0156,  ...,  0.7563,  1.3243,  0.4979],\n",
      "          [ 0.4390,  1.8610,  0.4000,  ...,  0.7267, -2.4052,  0.4363],\n",
      "          ...,\n",
      "          [ 1.1937,  0.3409,  0.4068,  ...,  1.2676, -0.6946, -0.2601],\n",
      "          [ 1.7088,  0.9094, -0.6384,  ...,  1.7225, -1.6495, -0.9003],\n",
      "          [ 1.3253,  1.3612, -0.6570,  ...,  0.7820,  1.2845, -1.1651]],\n",
      "\n",
      "         [[ 1.0047,  0.3677, -0.1759,  ..., -0.8085, -1.3894, -0.3705],\n",
      "          [-1.7129, -0.7999, -0.7384,  ...,  1.3115, -0.2055,  0.0450],\n",
      "          [-0.5662, -0.0284, -0.2150,  ..., -0.1283, -0.9736,  0.2904],\n",
      "          ...,\n",
      "          [ 0.3430,  0.4525, -0.4825,  ...,  1.0501,  0.3081, -0.8408],\n",
      "          [-1.0427,  0.2273, -1.1049,  ..., -0.2994, -1.1820,  0.0543],\n",
      "          [-0.2672, -0.5198, -0.0518,  ...,  0.7361,  0.0610, -0.7269]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.2270, -0.5682,  0.4831,  ..., -0.6640,  1.0591,  0.2766],\n",
      "          [-0.4265,  1.3822, -1.1272,  ..., -2.1121, -1.1342, -0.3716],\n",
      "          [-0.6802,  0.5943, -1.7053,  ..., -2.2653, -0.9940,  1.9404],\n",
      "          ...,\n",
      "          [-1.6890,  1.9767, -1.9055,  ..., -1.9564, -1.3578,  1.3325],\n",
      "          [-2.2248,  1.9432, -1.5527,  ..., -2.1565, -1.1160,  1.3684],\n",
      "          [-0.8544,  0.4039, -0.9816,  ..., -3.4208, -0.6918,  0.5010]],\n",
      "\n",
      "         [[ 0.2557,  0.5486,  0.5427,  ...,  0.7337,  0.0735,  0.8409],\n",
      "          [-0.2630,  3.1089,  0.0968,  ...,  1.1111, -1.2343,  0.9462],\n",
      "          [-0.5651,  0.4287, -0.6963,  ..., -0.4415,  1.4793,  1.4501],\n",
      "          ...,\n",
      "          [-1.1638,  0.4917, -0.2724,  ...,  0.2877,  1.7233,  0.9012],\n",
      "          [-1.8936,  0.1102, -0.2262,  ...,  0.3156,  0.7080,  1.3191],\n",
      "          [ 0.6021,  0.8863,  0.7705,  ...,  0.0679, -0.1352,  0.9846]],\n",
      "\n",
      "         [[-0.7236,  0.3119, -1.5837,  ..., -0.3841,  0.2253, -1.3224],\n",
      "          [-1.1716, -0.2108, -0.5625,  ..., -1.0099,  0.2504,  0.5097],\n",
      "          [ 0.9856, -0.7506,  0.1756,  ..., -0.9063, -0.6341, -1.6732],\n",
      "          ...,\n",
      "          [ 0.2208, -0.4965,  0.4488,  ..., -1.8295, -0.8895,  1.1919],\n",
      "          [ 0.0954, -0.5029,  0.2003,  ..., -0.8505, -0.6648,  0.9491],\n",
      "          [-0.6941,  0.4661,  0.6553,  ...,  1.0357,  0.8860, -1.2790]]]],\n",
      "       grad_fn=<PermuteBackward0>), tensor([[[[ 1.8845e-02,  4.6500e-02, -6.7226e-02,  ...,  6.0936e-02,\n",
      "           -2.3287e-02, -9.3406e-02],\n",
      "          [-9.4938e-01, -7.0013e-01,  1.5670e+00,  ..., -7.2562e-01,\n",
      "           -3.4044e-01,  7.6454e-02],\n",
      "          [ 7.5843e-01, -4.9189e-01,  6.0218e-01,  ..., -2.8840e+00,\n",
      "           -9.1578e-01, -1.3063e+00],\n",
      "          ...,\n",
      "          [ 1.7519e-01,  1.0409e+00, -7.1487e-01,  ..., -1.8131e+00,\n",
      "            2.2427e-02,  4.4781e-01],\n",
      "          [-1.0125e+00,  5.8842e-01,  9.9389e-01,  ..., -2.6583e-01,\n",
      "            5.7453e-01, -1.9235e-01],\n",
      "          [ 1.1273e-01,  8.2475e-01,  6.5481e-01,  ..., -6.1369e-01,\n",
      "            3.0219e-01, -9.7520e-01]],\n",
      "\n",
      "         [[ 5.9533e-02,  7.3237e-04,  3.6023e-02,  ..., -3.3076e-02,\n",
      "           -2.6139e-02, -2.8997e-03],\n",
      "          [ 7.9582e-01, -4.8594e-02, -4.5047e-02,  ...,  2.7073e-01,\n",
      "            3.3371e-01,  7.6484e-01],\n",
      "          [ 3.0589e-01,  3.7174e-01,  1.6971e+00,  ..., -5.9791e-01,\n",
      "           -1.3786e+00,  1.1129e-01],\n",
      "          ...,\n",
      "          [-1.6560e-01,  2.7446e-01,  1.1149e+00,  ..., -4.8972e-01,\n",
      "            1.2120e-01, -2.4574e-01],\n",
      "          [-1.8769e+00,  3.0057e-01,  7.6704e-01,  ...,  5.1949e-01,\n",
      "           -1.4190e-01, -7.1114e-01],\n",
      "          [ 4.1969e-03,  4.3631e-01,  5.6476e-01,  ...,  1.6068e-01,\n",
      "           -7.5989e-02,  8.5783e-01]],\n",
      "\n",
      "         [[-1.0822e-02,  1.1039e-02, -2.9605e-02,  ...,  1.4061e-02,\n",
      "            3.5705e-02,  5.5029e-02],\n",
      "          [ 2.8252e-02,  1.3149e-01,  6.0442e-01,  ...,  2.2485e-01,\n",
      "           -9.9927e-01, -2.6869e-01],\n",
      "          [-1.0952e-01,  1.5982e+00,  5.3234e-01,  ..., -5.4922e-01,\n",
      "            7.4813e-01, -3.4615e-01],\n",
      "          ...,\n",
      "          [ 4.7358e-01,  2.8728e-01,  7.8927e-01,  ..., -3.9230e-01,\n",
      "           -1.0877e+00, -8.7388e-01],\n",
      "          [ 9.9395e-01, -6.1588e-01, -1.6218e+00,  ..., -4.8930e-02,\n",
      "           -5.9601e-02,  1.5459e+00],\n",
      "          [-2.2850e-01, -9.3027e-01,  3.4326e-01,  ..., -2.5795e-01,\n",
      "            5.3608e-01, -6.2419e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-9.8938e-02,  3.7375e-02, -1.7196e-03,  ..., -4.7671e-02,\n",
      "           -2.1496e-02,  2.7531e-02],\n",
      "          [ 1.6518e-01,  7.4135e-01, -7.2497e-01,  ...,  2.1972e-01,\n",
      "           -4.6066e-01,  1.2030e-01],\n",
      "          [ 5.9424e-01,  5.5340e-01, -2.6494e-01,  ...,  7.4360e-01,\n",
      "           -7.8462e-01, -4.4225e-01],\n",
      "          ...,\n",
      "          [ 5.2224e-02, -9.4100e-01,  8.2801e-01,  ...,  2.8265e-01,\n",
      "           -5.7627e-02,  4.8287e-01],\n",
      "          [-1.5872e-01,  1.2591e-02,  2.9803e-02,  ..., -8.9281e-02,\n",
      "            4.3614e-01, -2.1890e-01],\n",
      "          [-5.4043e-01, -1.0715e+00, -4.7456e-01,  ...,  5.4266e-01,\n",
      "           -1.1797e+00,  3.2639e-01]],\n",
      "\n",
      "         [[ 8.4550e-02,  2.7683e-02,  6.4263e-02,  ...,  2.7156e-02,\n",
      "            4.6119e-02,  9.7695e-03],\n",
      "          [-6.1208e-01,  3.6774e-01,  5.9525e-01,  ..., -2.3135e+00,\n",
      "            6.9887e-01, -1.3180e+00],\n",
      "          [ 3.9861e+00, -1.3436e+00, -9.6040e-01,  ..., -1.5569e+00,\n",
      "           -3.5084e-01,  1.4959e+00],\n",
      "          ...,\n",
      "          [ 6.8438e-01, -2.2958e-01,  1.3876e-01,  ..., -1.2215e+00,\n",
      "            1.2698e+00, -8.8786e-01],\n",
      "          [ 1.0611e+00, -1.0908e+00, -9.6433e-01,  ...,  6.7415e-01,\n",
      "           -1.7808e+00, -4.8283e-01],\n",
      "          [-1.3237e-01,  9.3720e-01,  1.9473e+00,  ..., -6.9671e-02,\n",
      "           -2.0320e-01, -1.6678e+00]],\n",
      "\n",
      "         [[-1.1082e-01,  2.3628e-02, -5.7385e-02,  ..., -9.5812e-02,\n",
      "            5.7014e-02, -1.9716e-02],\n",
      "          [ 1.0451e+00,  1.6790e-01, -2.4514e-01,  ...,  7.5939e-01,\n",
      "            1.0304e+00,  7.7789e-01],\n",
      "          [ 2.3197e-01,  2.6045e-01,  1.0641e+00,  ..., -6.8515e-01,\n",
      "            4.4432e-01, -1.3401e+00],\n",
      "          ...,\n",
      "          [-1.0766e-01, -5.5228e-02,  1.8174e+00,  ..., -9.1997e-01,\n",
      "            2.2831e+00, -2.3189e-01],\n",
      "          [ 8.4494e-01,  2.4680e-01,  6.5136e-01,  ..., -1.7919e+00,\n",
      "            6.9252e-01, -1.9504e-01],\n",
      "          [-1.0843e-01,  3.2181e-01,  2.1127e-01,  ..., -7.0103e-01,\n",
      "            1.1408e-01, -1.0622e-01]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[-1.7216, -0.3013, -0.2952,  ...,  0.1865,  0.3256, -0.5023],\n",
      "          [-0.1957, -0.1786, -0.9112,  ...,  1.1172, -0.7191, -0.0284],\n",
      "          [ 0.0220,  0.1776,  0.0591,  ...,  1.7701, -0.4619, -0.2454],\n",
      "          ...,\n",
      "          [ 0.2176, -0.3232, -0.6081,  ...,  1.5550, -1.2430, -0.3198],\n",
      "          [ 0.9524, -0.5294, -0.5705,  ...,  1.8280, -1.2653, -0.1666],\n",
      "          [ 1.3661,  0.5509,  0.1335,  ...,  0.6294, -2.2567, -1.2231]],\n",
      "\n",
      "         [[ 0.1073, -0.0800,  2.2955,  ...,  0.2432,  0.0857, -0.1948],\n",
      "          [ 0.9808, -0.6278, -0.4527,  ...,  0.3213,  0.1807, -0.0346],\n",
      "          [ 1.0343, -1.1682, -1.6209,  ..., -0.2954,  0.0743, -0.4127],\n",
      "          ...,\n",
      "          [ 1.1312, -1.4746, -0.3128,  ...,  0.2481, -0.3518, -1.7267],\n",
      "          [ 0.7582, -2.8503, -1.2797,  ...,  0.0118,  0.3566, -2.5543],\n",
      "          [ 1.0603, -1.5495, -1.0175,  ...,  0.3707,  1.0936, -0.3179]],\n",
      "\n",
      "         [[-0.1928,  1.0482,  0.4753,  ..., -0.5411,  0.3269, -0.1044],\n",
      "          [-0.1273,  0.0276, -0.0614,  ...,  1.1025,  0.4218, -0.4889],\n",
      "          [-0.6753, -0.1420,  1.1495,  ...,  0.0532,  0.0279,  0.1422],\n",
      "          ...,\n",
      "          [-0.3856,  0.7099,  0.3387,  ...,  0.5485,  0.4601, -1.2318],\n",
      "          [ 0.0920,  1.3385,  0.6113,  ...,  1.0003,  0.4814, -0.5297],\n",
      "          [-0.1687,  0.9535, -0.1802,  ...,  0.9236,  0.5945, -0.3767]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5510,  0.9702, -0.8676,  ..., -0.7088,  0.6934,  0.8508],\n",
      "          [-0.1047,  0.5283, -1.5482,  ..., -0.6607, -0.7164,  0.1849],\n",
      "          [ 0.5435,  0.8508, -0.4913,  ..., -0.8345, -0.6720,  1.3088],\n",
      "          ...,\n",
      "          [ 0.1160,  2.7588, -0.5009,  ..., -0.0066,  0.4824,  1.0076],\n",
      "          [-0.2585,  1.3606, -0.4349,  ...,  0.3187,  0.1408,  1.2981],\n",
      "          [-0.1738,  0.0186, -0.2303,  ...,  0.1310,  0.0513,  0.1596]],\n",
      "\n",
      "         [[-0.4068,  0.3957,  0.3603,  ...,  0.6988,  0.0296, -0.0826],\n",
      "          [-1.4076,  1.4544, -1.0819,  ...,  0.3654, -0.2819, -0.1160],\n",
      "          [-0.1411,  0.1701, -0.5690,  ..., -0.2093, -1.0324, -1.7581],\n",
      "          ...,\n",
      "          [-0.8491,  1.4285,  0.3278,  ...,  0.2814,  0.2993, -0.2850],\n",
      "          [-1.0304,  2.7150, -2.0548,  ..., -0.2626,  0.2016, -1.0827],\n",
      "          [-1.1077,  1.0719, -1.4121,  ..., -0.1051,  0.4943, -0.5894]],\n",
      "\n",
      "         [[-0.7434, -0.0046,  0.4467,  ..., -0.1008,  0.0175, -0.0680],\n",
      "          [ 0.7038, -0.6238,  0.3741,  ..., -0.4137, -1.0309,  1.0379],\n",
      "          [ 0.4699,  0.0304,  0.7853,  ...,  0.2075, -1.7587,  0.4150],\n",
      "          ...,\n",
      "          [ 0.5061, -0.2991, -0.0411,  ..., -1.2583, -0.4008,  0.8131],\n",
      "          [ 0.1593,  0.4109,  0.9283,  ..., -0.3009, -0.3262,  2.1224],\n",
      "          [ 0.1634, -0.5846,  1.9271,  ...,  0.5845,  0.7323,  0.9993]]]],\n",
      "       grad_fn=<PermuteBackward0>), tensor([[[[ 0.1065, -0.1222, -0.1562,  ..., -0.2670,  0.2449, -0.1406],\n",
      "          [ 1.4053,  0.1191,  0.3313,  ...,  3.0563, -1.4882,  1.4536],\n",
      "          [-1.0129,  1.1642,  0.9386,  ...,  1.2820,  0.2608,  0.7850],\n",
      "          ...,\n",
      "          [ 1.2474,  0.3010,  2.3936,  ...,  2.6949, -1.1334,  1.9415],\n",
      "          [-0.0950,  0.0124,  1.3814,  ...,  2.3656, -1.5637,  2.3256],\n",
      "          [-0.8091,  1.0129,  1.1685,  ...,  2.3549, -2.6491,  0.1552]],\n",
      "\n",
      "         [[ 0.1237, -0.0431,  0.0194,  ..., -0.0261, -0.0917,  0.1659],\n",
      "          [ 0.0162, -0.0761, -0.3063,  ...,  0.3631,  0.3114, -0.7951],\n",
      "          [ 0.4270,  0.3540, -0.6077,  ...,  0.7230, -1.9099, -1.3288],\n",
      "          ...,\n",
      "          [ 0.5493, -1.1214, -1.3512,  ...,  1.3561, -0.0735, -0.9123],\n",
      "          [ 0.3282, -1.0373, -3.9200,  ...,  0.3016,  0.2430,  0.5087],\n",
      "          [ 0.9810,  0.5636, -0.4291,  ...,  0.9421,  0.1170,  0.9924]],\n",
      "\n",
      "         [[-0.0164,  0.0347, -0.0670,  ...,  0.0290,  0.0150,  0.0685],\n",
      "          [ 1.5384,  1.3917,  0.9124,  ..., -0.3786,  0.0426,  0.4816],\n",
      "          [ 0.6235, -0.1767, -0.1205,  ...,  0.1296,  1.1016,  2.4075],\n",
      "          ...,\n",
      "          [ 0.2905,  0.1605,  0.8631,  ..., -1.3392, -0.5427,  0.5099],\n",
      "          [ 0.8795, -0.2968, -0.5038,  ..., -2.0592,  1.3690, -0.1484],\n",
      "          [ 0.7729, -0.7343, -0.2226,  ..., -0.5945,  2.3027,  0.7000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0089, -0.0111,  0.0988,  ...,  0.0838, -0.0212,  0.0478],\n",
      "          [-0.4070,  0.7785,  0.7239,  ...,  0.9680, -0.0801,  0.2763],\n",
      "          [-1.3666,  1.5033,  1.2126,  ..., -0.2791, -0.1582, -0.9694],\n",
      "          ...,\n",
      "          [-1.5264,  0.9931, -0.1975,  ..., -0.2901,  0.0562,  0.4341],\n",
      "          [ 0.4481,  0.4224,  0.9034,  ..., -0.0635, -1.7770,  0.2598],\n",
      "          [ 0.5037, -1.0197,  0.4674,  ..., -0.0421,  1.0225,  0.1864]],\n",
      "\n",
      "         [[-0.1965, -0.0647,  0.0663,  ..., -0.0456,  0.0493, -0.0815],\n",
      "          [-0.8762,  0.2020, -0.3455,  ...,  0.2209,  0.7596, -0.1498],\n",
      "          [ 0.1031, -1.5917, -1.6884,  ...,  0.1600,  1.8610,  0.1595],\n",
      "          ...,\n",
      "          [ 0.0825, -0.3949, -0.0894,  ..., -0.7238,  0.3111,  0.3884],\n",
      "          [ 1.1359,  1.0477, -1.2507,  ..., -0.2242, -1.1359, -0.2188],\n",
      "          [-1.3058, -0.1418,  1.1550,  ..., -0.9057,  0.3632, -0.0210]],\n",
      "\n",
      "         [[ 0.1040, -0.1469,  0.1752,  ..., -0.1528,  0.0050, -0.1816],\n",
      "          [-0.5424, -0.4902,  1.4413,  ..., -0.7550,  0.1687, -0.0194],\n",
      "          [-0.0250, -0.4402, -0.1225,  ..., -2.2199,  0.5366, -1.1056],\n",
      "          ...,\n",
      "          [ 0.0880, -0.2506,  1.9353,  ..., -1.0922,  0.9209, -0.5325],\n",
      "          [ 0.4116, -1.0799,  0.5464,  ..., -0.7013,  0.3254, -0.4542],\n",
      "          [-0.1713,  0.5032, -0.1787,  ...,  0.0351, -0.0544,  0.3726]]]],\n",
      "       grad_fn=<PermuteBackward0>))), hidden_states=None, attentions=None, cross_attentions=None)\n"
     ]
    }
   ],
   "source": [
    "text = \"What is Huggingface Transformers?\"\n",
    "\n",
    "gpt2_model = AutoModel.from_pretrained(\"gpt2\")\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "encoded_input = gpt2_tokenizer(text, return_tensors=\"pt\")\n",
    "print(f\"gpt2 encoded input : {encoded_input}\")\n",
    "\n",
    "gpt2_output = gpt2_model(**encoded_input)\n",
    "print(f\"gpt2 output : {gpt2_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backbone Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"klue/roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaModel(\n",
      "  (embeddings): RobertaEmbeddings(\n",
      "    (word_embeddings): Embedding(32000, 768, padding_idx=1)\n",
      "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "    (token_type_embeddings): Embedding(1, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): RobertaEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x RobertaLayer(\n",
      "        (attention): RobertaAttention(\n",
      "          (self): RobertaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): RobertaSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): RobertaIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): RobertaOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): RobertaPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_id = \"klue/roberta-base\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_id)\n",
    "print(config)\n",
    "\n",
    "model = AutoModel.from_pretrained(model_id)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backbone + Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"SamLowe/roberta-base-go_emotions\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"admiration\",\n",
      "    \"1\": \"amusement\",\n",
      "    \"2\": \"anger\",\n",
      "    \"3\": \"annoyance\",\n",
      "    \"4\": \"approval\",\n",
      "    \"5\": \"caring\",\n",
      "    \"6\": \"confusion\",\n",
      "    \"7\": \"curiosity\",\n",
      "    \"8\": \"desire\",\n",
      "    \"9\": \"disappointment\",\n",
      "    \"10\": \"disapproval\",\n",
      "    \"11\": \"disgust\",\n",
      "    \"12\": \"embarrassment\",\n",
      "    \"13\": \"excitement\",\n",
      "    \"14\": \"fear\",\n",
      "    \"15\": \"gratitude\",\n",
      "    \"16\": \"grief\",\n",
      "    \"17\": \"joy\",\n",
      "    \"18\": \"love\",\n",
      "    \"19\": \"nervousness\",\n",
      "    \"20\": \"optimism\",\n",
      "    \"21\": \"pride\",\n",
      "    \"22\": \"realization\",\n",
      "    \"23\": \"relief\",\n",
      "    \"24\": \"remorse\",\n",
      "    \"25\": \"sadness\",\n",
      "    \"26\": \"surprise\",\n",
      "    \"27\": \"neutral\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"admiration\": 0,\n",
      "    \"amusement\": 1,\n",
      "    \"anger\": 2,\n",
      "    \"annoyance\": 3,\n",
      "    \"approval\": 4,\n",
      "    \"caring\": 5,\n",
      "    \"confusion\": 6,\n",
      "    \"curiosity\": 7,\n",
      "    \"desire\": 8,\n",
      "    \"disappointment\": 9,\n",
      "    \"disapproval\": 10,\n",
      "    \"disgust\": 11,\n",
      "    \"embarrassment\": 12,\n",
      "    \"excitement\": 13,\n",
      "    \"fear\": 14,\n",
      "    \"gratitude\": 15,\n",
      "    \"grief\": 16,\n",
      "    \"joy\": 17,\n",
      "    \"love\": 18,\n",
      "    \"nervousness\": 19,\n",
      "    \"neutral\": 27,\n",
      "    \"optimism\": 20,\n",
      "    \"pride\": 21,\n",
      "    \"realization\": 22,\n",
      "    \"relief\": 23,\n",
      "    \"remorse\": 24,\n",
      "    \"sadness\": 25,\n",
      "    \"surprise\": 26\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"multi_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "RobertaForSequenceClassification(\n",
      "  (roberta): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): RobertaClassificationHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (out_proj): Linear(in_features=768, out_features=28, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_id = \"SamLowe/roberta-base-go_emotions\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_id)\n",
    "print(config)\n",
    "\n",
    "classification_model = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "print(classification_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backbone + head(downstream task finetuning )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaForSequenceClassification(\n",
      "  (roberta): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(32000, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): RobertaClassificationHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_id = \"klue/roberta-base\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_id)\n",
    "classification_model = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "\n",
    "print(classification_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertTokenizerFast(name_or_path='klue/roberta-base', vocab_size=32000, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t4: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "model_id = \"klue/roberta-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [0, 9157, 7461, 2190, 2259, 8509, 2138, 1793, 2855, 5385, 2200, 20950, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['[CLS]', '', '##', '##', '##', '', '##', '', '##', '', '##', '', '[SEP]']\n",
      "[CLS]      [SEP]\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "text = \"    \"\n",
    "tokenized = tokenizer(text)\n",
    "print(tokenized)\n",
    "\n",
    "##  id()   \n",
    "print(tokenizer.convert_ids_to_tokens(tokenized['input_ids']))\n",
    "\n",
    "##  id   \n",
    "print(tokenizer.decode(tokenized['input_ids']))\n",
    "\n",
    "##  id   .  \n",
    "print(tokenizer.decode(tokenized['input_ids'], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[0, 1656, 2517, 3135, 6265, 2], [0, 864, 2517, 3135, 2346, 2121, 2]], 'token_type_ids': [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1]]}\n",
      "{'input_ids': [[0, 1656, 2517, 3135, 6265, 2, 864, 2517, 3135, 2346, 2121, 2]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "tokenized = tokenizer([' ', ''])\n",
    "print(tokenized)\n",
    "\n",
    "## 2             .\n",
    "## 2           .\n",
    "tokenized = tokenizer([[' ', '']])\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]   [SEP]', '[CLS]  [SEP]']\n",
      "['[CLS]   [SEP]  [SEP]']\n"
     ]
    }
   ],
   "source": [
    "first_tokenized_result = tokenizer([' ', ''])['input_ids']\n",
    "print(tokenizer.batch_decode(first_tokenized_result))\n",
    "\n",
    "second_tokenized_result = tokenizer([[' ', '']])['input_ids']\n",
    "print(tokenizer.batch_decode(second_tokenized_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[2, 1656, 2517, 3135, 6265, 3], [2, 864, 2517, 3135, 6265, 3]], 'token_type_ids': [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}\n",
      "{'input_ids': [[2, 1656, 2517, 3135, 6265, 3, 864, 2517, 3135, 6265, 3]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")\n",
    "\n",
    "tokenized = bert_tokenizer([' ', ' '])\n",
    "print(tokenized)\n",
    "\n",
    "tokenized = bert_tokenizer([[' ', ' ']])\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[0, 1656, 2517, 3135, 6265, 2], [0, 864, 2517, 3135, 6265, 2]], 'token_type_ids': [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}\n",
      "{'input_ids': [[0, 1656, 2517, 3135, 6265, 2, 864, 2517, 3135, 6265, 2]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "roberta_tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-base\")\n",
    "\n",
    "tokenized = roberta_tokenizer([' ', ' '])\n",
    "print(tokenized)\n",
    "\n",
    "tokenized = roberta_tokenizer([[' ', ' ']])\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 1656, 2517, 3135, 6265, 2069, 1599, 2062, 2, 1, 1, 1, 1, 1, 1, 1, 1], [0, 864, 2517, 3135, 6265, 2073, 1656, 2517, 3135, 6265, 2178, 2062, 831, 647, 10283, 18, 2]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")\n",
    "tokenizer(['  ', '     .'], padding='longest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['title', 'context', 'news_category', 'source', 'guid', 'is_impossible', 'question_type', 'question', 'answers'],\n",
      "        num_rows: 17554\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['title', 'context', 'news_category', 'source', 'guid', 'is_impossible', 'question_type', 'question', 'answers'],\n",
      "        num_rows: 5841\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "klue_mrc_dataset = load_dataset('klue', 'mrc')\n",
    "print(klue_mrc_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Unnamed: 0', 'Ticker Symbol', 'Period Ending', 'Accounts Payable', 'Accounts Receivable', \"Add'l income/expense items\", 'After Tax ROE', 'Capital Expenditures', 'Capital Surplus', 'Cash Ratio', 'Cash and Cash Equivalents', 'Changes in Inventories', 'Common Stocks', 'Cost of Revenue', 'Current Ratio', 'Deferred Asset Charges', 'Deferred Liability Charges', 'Depreciation', 'Earnings Before Interest and Tax', 'Earnings Before Tax', 'Effect of Exchange Rate', 'Equity Earnings/Loss Unconsolidated Subsidiary', 'Fixed Assets', 'Goodwill', 'Gross Margin', 'Gross Profit', 'Income Tax', 'Intangible Assets', 'Interest Expense', 'Inventory', 'Investments', 'Liabilities', 'Long-Term Debt', 'Long-Term Investments', 'Minority Interest', 'Misc. Stocks', 'Net Borrowings', 'Net Cash Flow', 'Net Cash Flow-Operating', 'Net Cash Flows-Financing', 'Net Cash Flows-Investing', 'Net Income', 'Net Income Adjustments', 'Net Income Applicable to Common Shareholders', 'Net Income-Cont. Operations', 'Net Receivables', 'Non-Recurring Items', 'Operating Income', 'Operating Margin', 'Other Assets', 'Other Current Assets', 'Other Current Liabilities', 'Other Equity', 'Other Financing Activities', 'Other Investing Activities', 'Other Liabilities', 'Other Operating Activities', 'Other Operating Items', 'Pre-Tax Margin', 'Pre-Tax ROE', 'Profit Margin', 'Quick Ratio', 'Research and Development', 'Retained Earnings', 'Sale and Purchase of Stock', 'Sales, General and Admin.', 'Short-Term Debt / Current Portion of Long-Term Debt', 'Short-Term Investments', 'Total Assets', 'Total Current Assets', 'Total Current Liabilities', 'Total Equity', 'Total Liabilities', 'Total Liabilities & Equity', 'Total Revenue', 'Treasury Stock', 'For Year', 'Earnings Per Share', 'Estimated Shares Outstanding'],\n",
      "        num_rows: 1781\n",
      "    })\n",
      "})\n",
      "Dataset({\n",
      "    features: ['a'],\n",
      "    num_rows: 3\n",
      "})\n",
      "Dataset({\n",
      "    features: ['a'],\n",
      "    num_rows: 3\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files=\"/home/pervinco/Datasets/NYSE/fundamentals.csv\")\n",
    "print(dataset)\n",
    "\n",
    "my_dict = {\"a\" : [1, 2, 3]}\n",
    "dataset = Dataset.from_dict(my_dict)\n",
    "print(dataset)\n",
    "\n",
    "df = pd.DataFrame({\"a\" : [1, 2, 3]})\n",
    "dataset = Dataset.from_pandas(df)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['guid', 'title', 'label', 'url', 'date'],\n",
      "    num_rows: 45678\n",
      "})\n",
      "Dataset({\n",
      "    features: ['guid', 'title', 'label', 'url', 'date'],\n",
      "    num_rows: 9107\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "klue_tc_train = load_dataset(\"klue\", \"ynat\", split=\"train\")\n",
    "klue_tc_eval = load_dataset(\"klue\", \"ynat\", split=\"validation\")\n",
    "\n",
    "print(klue_tc_train)\n",
    "print(klue_tc_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'guid': 'ynat-v1_train_00000',\n",
       " 'title': '  2    ',\n",
       " 'label': 3,\n",
       " 'url': 'https://news.naver.com/main/read.nhn?mode=LS2D&mid=shm&sid1=105&sid2=227&oid=001&aid=0008508947',\n",
       " 'date': '2016.06.30.  10:36'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "klue_tc_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IT', '', '', '', '', '', '']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "klue_tc_train.features['label'].names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['title', 'label'],\n",
      "    num_rows: 45678\n",
      "})\n",
      "Dataset({\n",
      "    features: ['title', 'label'],\n",
      "    num_rows: 9107\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "klue_tc_train = klue_tc_train.remove_columns(['guid', 'url', 'date'])\n",
    "klue_tc_eval = klue_tc_eval.remove_columns(['guid', 'url', 'date'])\n",
    "\n",
    "print(klue_tc_train)\n",
    "print(klue_tc_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassLabel(names=['IT', '', '', '', '', '', ''], id=None)\n",
      "{'title': '  2    ', 'label': 3, 'label_str': ''}\n"
     ]
    }
   ],
   "source": [
    "klue_tc_label = klue_tc_train.features['label']\n",
    "print(klue_tc_label)\n",
    "\n",
    "def make_str_label(batch):\n",
    "    batch['label_str'] = klue_tc_label.int2str(batch['label'])\n",
    "    return batch\n",
    "\n",
    "klue_tc_train = klue_tc_train.map(make_str_label, batched=True, batch_size=1000)\n",
    "print(klue_tc_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['title', 'label', 'label_str'],\n",
      "        num_rows: 35678\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['title', 'label', 'label_str'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "total_train_dataset = klue_tc_train.train_test_split(test_size=10000, shuffle=True, seed=42)\n",
    "print(total_train_dataset)\n",
    "\n",
    "train_dataset = total_train_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = klue_tc_eval.train_test_split(test_size=1000, shuffle=True, seed=42)\n",
    "\n",
    "test_dataset = dataset['test']\n",
    "valid_dataset = dataset['train'].train_test_split(test_size=1000, shuffle=True, seed=42)['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pervinco/miniconda3/envs/nlp-project/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_id = \"klue/roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=len(train_dataset.features['label'].names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0f79687a371452dabe4e068a60c604e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(rows):\n",
    "    return tokenizer(rows['title'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "valid_dataset = valid_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pervinco/miniconda3/envs/nlp-project/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./train_results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    push_to_hub=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    return {\"accuracy\" : (predictions == labels).mean()}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3750' max='3750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3750/3750 06:53, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.600700</td>\n",
       "      <td>0.893434</td>\n",
       "      <td>0.771000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.389000</td>\n",
       "      <td>0.659277</td>\n",
       "      <td>0.842000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.240700</td>\n",
       "      <td>0.706096</td>\n",
       "      <td>0.851000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6370279788970947,\n",
       " 'eval_accuracy': 0.856,\n",
       " 'eval_runtime': 4.2223,\n",
       " 'eval_samples_per_second': 236.84,\n",
       " 'eval_steps_per_second': 29.605,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer API   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AdamW\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pervinco/miniconda3/envs/nlp-project/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=7, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id = \"klue/roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=len(train_dataset.features['label'].names))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53fe376153694155b821a38f9d34403a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(raws):\n",
    "    return tokenizer(raws['title'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "def make_dataloader(dataset, batch_size, shuffle=True):\n",
    "    ##   dataset tokenization\n",
    "    dataset = dataset.map(tokenize_function, batched=True).with_format(\"torch\")\n",
    "    \n",
    "    ##   \n",
    "    dataset = dataset.rename_column(\"label\", \"labels\")\n",
    "\n",
    "    ##   \n",
    "    dataset = dataset.remove_columns(column_names=['title'])\n",
    "\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "train_dataloader = make_dataloader(train_dataset, batch_size=8, shuffle=True)\n",
    "valid_dataloader = make_dataloader(valid_dataset, batch_size=8, shuffle=False)\n",
    "test_dataloader = make_dataloader(test_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = np.mean(np.array(predictions) == np.array(true_labels))\n",
    "\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pervinco/miniconda3/envs/nlp-project/lib/python3.9/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e4fda02a92e499193ac59561a2b51c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss : 0.6306635253489018\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccc9f0649d534a08b64bfa0bf967cd9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss : 0.5637158288359642\n",
      "Validation Accuracy : 0.818\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2581f2cb94e74bf9a816f0899b36f29c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss : 0.4288061813563108\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8acc918c87ca4439a3774991387b75d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss : 0.8216227255761623\n",
      "Validation Accuracy : 0.749\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd87bd1d4eb647dc8e3ad512533d9c99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss : 0.3170539458371699\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d0ac35619d74866b49886ada0e362c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss : 0.7159613126516342\n",
      "Validation Accuracy : 0.8\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    \n",
    "    train_loss = train(model, train_dataloader, optimizer)\n",
    "    print(f\"Training Loss : {train_loss}\")\n",
    "\n",
    "    valid_loss, valid_accuracy = evaluate(model, valid_dataloader)\n",
    "    print(f\"Validation Loss : {valid_loss}\")\n",
    "    print(f\"Validation Accuracy : {valid_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c057497f81c4aaf82c9baa14d94125d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy : 0.792\n"
     ]
    }
   ],
   "source": [
    "_, test_accuracy = evaluate(model, test_dataloader)\n",
    "print(f\"Test Accuracy : {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pipline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.pipelines because of the following error (look up to see its traceback):\noperator torchvision::nms does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/nlp-project/lib/python3.9/site-packages/transformers/utils/import_utils.py:1603\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1602\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1603\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp-project/lib/python3.9/importlib/__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1030\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:986\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:680\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:850\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:228\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp-project/lib/python3.9/site-packages/transformers/pipelines/__init__.py:26\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PreTrainedFeatureExtractor\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_processing_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseImageProcessor\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_auto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoConfig\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp-project/lib/python3.9/site-packages/transformers/image_processing_utils.py:21\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_processing_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BatchFeature, ImageProcessingMixin\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_transforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m center_crop, normalize, rescale\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChannelDimension\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp-project/lib/python3.9/site-packages/transformers/image_transforms.py:22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     23\u001b[0m     ChannelDimension,\n\u001b[1;32m     24\u001b[0m     ImageInput,\n\u001b[1;32m     25\u001b[0m     get_channel_dimension_axis,\n\u001b[1;32m     26\u001b[0m     get_image_size,\n\u001b[1;32m     27\u001b[0m     infer_channel_dimension_format,\n\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExplicitEnum, TensorType, is_jax_tensor, is_tf_tensor, is_torch_tensor\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp-project/lib/python3.9/site-packages/transformers/image_utils.py:58\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torchvision_available():\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InterpolationMode\n\u001b[1;32m     60\u001b[0m     pil_torch_interpolation_mapping \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     61\u001b[0m         PILImageResampling\u001b[38;5;241m.\u001b[39mNEAREST: InterpolationMode\u001b[38;5;241m.\u001b[39mNEAREST,\n\u001b[1;32m     62\u001b[0m         PILImageResampling\u001b[38;5;241m.\u001b[39mBOX: InterpolationMode\u001b[38;5;241m.\u001b[39mBOX,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m         PILImageResampling\u001b[38;5;241m.\u001b[39mLANCZOS: InterpolationMode\u001b[38;5;241m.\u001b[39mLANCZOS,\n\u001b[1;32m     67\u001b[0m     }\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp-project/lib/python3.9/site-packages/torchvision/__init__.py:6\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp-project/lib/python3.9/site-packages/torchvision/_meta_registrations.py:164\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m grad\u001b[38;5;241m.\u001b[39mnew_empty((batch_size, channels, height, width))\n\u001b[1;32m    163\u001b[0m \u001b[38;5;129;43m@torch\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_custom_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimpl_abstract\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtorchvision::nms\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m--> 164\u001b[0m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43mmeta_nms\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miou_threshold\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mboxes should be a 2d tensor, got \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43mD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp-project/lib/python3.9/site-packages/torch/library.py:654\u001b[0m, in \u001b[0;36mregister_fake.<locals>.register\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m    653\u001b[0m     use_lib \u001b[38;5;241m=\u001b[39m lib\n\u001b[0;32m--> 654\u001b[0m \u001b[43muse_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_register_fake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacklevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstacklevel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp-project/lib/python3.9/site-packages/torch/library.py:154\u001b[0m, in \u001b[0;36mLibrary._register_fake\u001b[0;34m(self, op_name, fn, _stacklevel)\u001b[0m\n\u001b[1;32m    152\u001b[0m     func_to_register \u001b[38;5;241m=\u001b[39m fn\n\u001b[0;32m--> 154\u001b[0m handle \u001b[38;5;241m=\u001b[39m \u001b[43mentry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabstract_impl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregister\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc_to_register\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_registration_handles\u001b[38;5;241m.\u001b[39mappend(handle)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp-project/lib/python3.9/site-packages/torch/_library/abstract_impl.py:31\u001b[0m, in \u001b[0;36mAbstractImplHolder.register\u001b[0;34m(self, func, source)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregister_fake(...): the operator \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqualname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malready has an fake impl registered at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel\u001b[38;5;241m.\u001b[39msource\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     30\u001b[0m     )\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch_has_kernel_for_dispatch_key\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqualname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMeta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregister_fake(...): the operator \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqualname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malready has an DispatchKey::Meta implementation via a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregister_fake.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     38\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: operator torchvision::nms does not exist",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m      3\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mklue/roberta-base\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m model_pipeline \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-classification\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel_id)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1055\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp-project/lib/python3.9/site-packages/transformers/utils/import_utils.py:1593\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1591\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n\u001b[1;32m   1592\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m-> 1593\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1594\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1595\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp-project/lib/python3.9/site-packages/transformers/utils/import_utils.py:1605\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1603\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   1604\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1605\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1606\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1607\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1608\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.pipelines because of the following error (look up to see its traceback):\noperator torchvision::nms does not exist"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_id = \"klue/roberta-base\"\n",
    "model_pipeline = pipeline(\"text-classification\", model=model_id)\n",
    "\n",
    "model_pipeline(dataset['title'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "class CustomPipeline:\n",
    "    def __init__(self, model_id):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "        self.model.eval()\n",
    "\n",
    "    def __call__(self, texts):\n",
    "        tokenized = self.tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**tokenized)\n",
    "            logits = outputs.logits\n",
    "\n",
    "        probabilities = softmax(logits, dim=-1)\n",
    "        scores, labels = torch.max(probabilities, dim=-1)\n",
    "        labels_str = [self.model.config.id2label[label_idx] for label_idx in labels.tolist()]\n",
    "\n",
    "        return [{'label' : label, 'score' : score.item()} for label, score in zip(labels_str, scores)]\n",
    "    \n",
    "custom_pipeline = CustomPipeline(model_id)\n",
    "custom_pipeline(dataset['title'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
