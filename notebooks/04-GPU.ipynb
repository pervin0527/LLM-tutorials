{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers datasets accelerate peft bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 메모리 사용량 : 0.0000GB\n"
     ]
    }
   ],
   "source": [
    "def print_gpu_utilization():\n",
    "    if torch.cuda.is_available():\n",
    "        used_memory = torch.cuda.memory_allocated() / (1024 ** 3)\n",
    "        print(f\"GPU 메모리 사용량 : {used_memory:.4f}GB\")\n",
    "\n",
    "    else:\n",
    "        print(\"GPU 환경이 아닙니다.\")\n",
    "\n",
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pervinco/miniconda3/envs/nlp-project/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ba325ca24ae4c27a781eeedbfe71909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 메모리 사용량 : 2.5989GB\n",
      "모델 파라미터 데이터 타입 :  torch.float16\n"
     ]
    }
   ],
   "source": [
    "def load_model_and_tokenizer(model_id, peft=None):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    if peft is None:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=\"auto\", device_map={\"\": 0})\n",
    "\n",
    "    print_gpu_utilization()\n",
    "    return model, tokenizer\n",
    "\n",
    "model_id = \"EleutherAI/polyglot-ko-1.3b\"\n",
    "model, tokenizer = load_model_and_tokenizer(model_id)\n",
    "\n",
    "# 모델의 파라미터 데이터 타입 출력\n",
    "first_param = next(model.parameters())\n",
    "print(\"모델 파라미터 데이터 타입 : \", first_param.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_memory_of_gradients(model):\n",
    "    total_memory = 0  # 총 메모리 저장 변수 초기화\n",
    "    for param in model.parameters():  # 모델의 모든 파라미터에 대해 반복\n",
    "        if param.grad is not None:  # 파라미터에 그레디언트가 있을 때만 계산\n",
    "            # 각 그레디언트 텐서의 요소 개수와 요소 크기(바이트) 곱을 추가\n",
    "            total_memory += param.grad.nelement() * param.grad.element_size()\n",
    "    return total_memory  # 총 메모리 반환\n",
    "\n",
    "def estimate_memory_of_optimizer(optimizer):\n",
    "    total_memory = 0  # 총 메모리 저장 변수 초기화\n",
    "    for state in optimizer.state.values():  # 옵티마이저의 모든 상태에 대해 반복\n",
    "        for k, v in state.items():  # 각 상태의 키와 값을 반복\n",
    "            if torch.is_tensor(v):  # 값이 텐서인지 확인\n",
    "                # 각 텐서의 요소 개수와 요소 크기(바이트) 곱을 추가\n",
    "                total_memory += v.nelement() * v.element_size()\n",
    "    return total_memory  # 총 메모리 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataset, training_args):\n",
    "    if training_args.gradient_checkpointing:\n",
    "        model.gradient_checkpointing_enable()\n",
    "\n",
    "    train_dataloader = DataLoader(dataset, batch_size=training_args.per_device_train_batch_size)\n",
    "    optimizer = AdamW(model.parameters())\n",
    "    \n",
    "    model.train()\n",
    "    gpu_utilization_printed = False\n",
    "    for step, batch in enumerate(train_dataloader, start=1):\n",
    "        batch = {k : v.to(model.device) for k, v in batch.items()}\n",
    "\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss = loss / training_args.gradient_accumulation_steps\n",
    "        loss.backward()\n",
    "\n",
    "        if step % training_args.gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            gradients_memory = estimate_memory_of_gradients(model)\n",
    "            optimizer_memory = estimate_memory_of_optimizer(optimizer)\n",
    "\n",
    "            if not gpu_utilization_printed:\n",
    "                print_gpu_utilization()\n",
    "                gpu_utilization_printed = True\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    print(f\"옵티마이저 상태의 메모리 사용량 : {optimizer_memory / (1024 ** 3):.4f}GB\")\n",
    "    print(f\"그레디언트 메모리 사용량 : {gradients_memory / (1024 ** 3):.4f}GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dummy_dataset():\n",
    "    seq_len, dataset_size = 256, 64\n",
    "    dummy_data = {\n",
    "        \"input_ids\" : np.random.randint(100, 30000, (dataset_size, seq_len)),\n",
    "        \"labels\" : np.random.randint(100, 30000, (dataset_size, seq_len))\n",
    "    }\n",
    "\n",
    "    dataset = Dataset.from_dict(dummy_data)\n",
    "    dataset.set_format(\"pt\")\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup():\n",
    "    if 'model' in globals():\n",
    "        del globals()['model']\n",
    "\n",
    "    if 'dataset' in globals():\n",
    "        del globals()['dataset']\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_memory_experiment(batch_size,\n",
    "                          gradient_accumulation_steps=1,\n",
    "                          gradient_checkpointing=False,\n",
    "                          model_id=\"EleutherAI/polyglot-ko-1.3b\",\n",
    "                          peft=None):\n",
    "    \n",
    "    print(f\"배치 크기 : {batch_size}\")\n",
    "    model, tokenizer = load_model_and_tokenizer(model_id, peft=peft)\n",
    "\n",
    "    if gradient_checkpointing == True or peft == 'qlora':\n",
    "        model.config.use_cache = False\n",
    "\n",
    "    dataset = make_dummy_dataset()\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        gradient_checkpointing=gradient_checkpointing,\n",
    "        output_dir=\"./train_results\",\n",
    "        num_train_epochs=1\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        train_model(model, dataset, training_args)\n",
    "    except RuntimeError as e:\n",
    "        if \"CUDA out of memory\" in str(e):\n",
    "            print(e)\n",
    "        else:\n",
    "            raise e\n",
    "    finally:\n",
    "        del model, dataset\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 메모리 사용량 : 0.1152GB\n",
      "배치 크기 : 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "462b75b1f4114b36afdaaa614fdfa5df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 메모리 사용량 : 2.7141GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pervinco/miniconda3/envs/nlp-project/lib/python3.9/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 메모리 사용량 : 10.7011GB\n",
      "옵티마이저 상태의 메모리 사용량 : 4.9614GB\n",
      "그레디언트 메모리 사용량 : 2.4807GB\n",
      "GPU 메모리 사용량 : 0.1311GB\n",
      "배치 크기 : 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pervinco/miniconda3/envs/nlp-project/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2576fb5ab50042a2846c61ac481e5bb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 메모리 사용량 : 2.7300GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pervinco/miniconda3/envs/nlp-project/lib/python3.9/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 메모리 사용량 : 11.2277GB\n",
      "옵티마이저 상태의 메모리 사용량 : 4.9614GB\n",
      "그레디언트 메모리 사용량 : 2.4807GB\n",
      "GPU 메모리 사용량 : 0.1311GB\n",
      "배치 크기 : 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pervinco/miniconda3/envs/nlp-project/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b61c2d169a674745891a577407c14e82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 메모리 사용량 : 2.7300GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pervinco/miniconda3/envs/nlp-project/lib/python3.9/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 메모리 사용량 : 12.2795GB\n",
      "옵티마이저 상태의 메모리 사용량 : 4.9614GB\n",
      "그레디언트 메모리 사용량 : 2.4807GB\n",
      "GPU 메모리 사용량 : 0.1311GB\n"
     ]
    }
   ],
   "source": [
    "cleanup()\n",
    "print_gpu_utilization()\n",
    "\n",
    "for batch_size in [4, 8, 16]:\n",
    "    gpu_memory_experiment(batch_size)\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_id, peft=None):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "    if peft is None:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=\"auto\", device_map={\"\" : 0})\n",
    "\n",
    "    elif peft == \"lora\":\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=\"auto\", device_map={\"\" : 0})\n",
    "        lora_config = LoraConfig(r=8,\n",
    "                                 lora_alpha=32,\n",
    "                                 target_modules=['query_key_value'],\n",
    "                                 lora_dropout=0.05,\n",
    "                                 bias=\"none\",\n",
    "                                 task_type=\"CASUAL_LM\")\n",
    "        \n",
    "        model = get_peft_model(model, lora_config)\n",
    "        model.print_trainable_parameters()\n",
    "\n",
    "    print_gpu_utilization()\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 메모리 사용량 : 0.1311GB\n",
      "배치 크기 : 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pervinco/miniconda3/envs/nlp-project/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13b6da91f411451abcaec16252fe0c84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,572,864 || all params: 1,333,383,168 || trainable%: 0.1180\n",
      "GPU 메모리 사용량 : 2.7358GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pervinco/miniconda3/envs/nlp-project/lib/python3.9/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 메모리 사용량 : 4.8589GB\n",
      "옵티마이저 상태의 메모리 사용량 : 0.0117GB\n",
      "그레디언트 메모리 사용량 : 0.0059GB\n",
      "GPU 메모리 사용량 : 0.1311GB\n"
     ]
    }
   ],
   "source": [
    "cleanup()\n",
    "print_gpu_utilization()\n",
    "gpu_memory_experiment(batch_size=16, peft='lora')\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7af62841ef2435d9914d846642a8ec3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(load_in_4bit=True, ## 모델을 4비트로 불러올 것인가\n",
    "                                bnb_4bit_quant_type=\"nf4\", ## 4비트 데이터 타입\n",
    "                                bnb_4bit_use_double_quant=True, ## 2차 양자화\n",
    "                                bnb_4bit_compute_dtype=torch.bfloat16 ## 계산시 사용할 데이터 형식\n",
    "                                )\n",
    "\n",
    "model_nf4 = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=nf4_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 메모리 사용량 : 1.0604GB\n",
      "배치 크기 : 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pervinco/miniconda3/envs/nlp-project/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ba0222a512a402595a5ab1e2327bff0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,572,864 || all params: 1,333,383,168 || trainable%: 0.1180\n",
      "GPU 메모리 사용량 : 2.2273GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pervinco/miniconda3/envs/nlp-project/lib/python3.9/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/pervinco/miniconda3/envs/nlp-project/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/pervinco/miniconda3/envs/nlp-project/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 메모리 사용량 : 2.7664GB\n",
      "옵티마이저 상태의 메모리 사용량 : 0.0117GB\n",
      "그레디언트 메모리 사용량 : 0.0059GB\n",
      "GPU 메모리 사용량 : 1.0604GB\n"
     ]
    }
   ],
   "source": [
    "def load_model_and_tokenizer(model_id, peft=None):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "    if peft is None:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=\"auto\", device_map={\"\" : 0})\n",
    "\n",
    "    elif peft == 'lora':\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=\"auto\", device_map={\"\" : 0})\n",
    "        lora_config = LoraConfig(r=8,\n",
    "                                 lora_alpha=32,\n",
    "                                 target_modules=[\"query_key_value\"],\n",
    "                                 lora_dropout=0.05,\n",
    "                                 bias='none',\n",
    "                                 task_type=\"CASUAL_LM\")\n",
    "        \n",
    "        model = get_peft_model(model, lora_config)\n",
    "        model.print_trainable_parameters()\n",
    "\n",
    "    elif peft == 'qlora':\n",
    "        lora_config = LoraConfig(r=8,\n",
    "                                 lora_alpha=32,\n",
    "                                 target_modules=['query_key_value'],\n",
    "                                 lora_dropout=0.05,\n",
    "                                 bias='none',\n",
    "                                 task_type=\"CASUAL_LM\")\n",
    "        \n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True, ## 모델을 4비트로 불러올 것인가\n",
    "            bnb_4bit_quant_type=\"nf4\", ## 4비트 데이터 타입\n",
    "            bnb_4bit_use_double_quant=True, ## 2차 양자화\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16 ## 계산시 사용할 데이터 형식\n",
    "        )\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\" : 0})\n",
    "        model.gradient_checkpointing_enable()\n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "        model = get_peft_model(model, lora_config)\n",
    "        model.print_trainable_parameters()\n",
    "\n",
    "        print_gpu_utilization()\n",
    "        return model, tokenizer\n",
    "    \n",
    "cleanup()\n",
    "print_gpu_utilization()\n",
    "gpu_memory_experiment(batch_size=16, peft='qlora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
