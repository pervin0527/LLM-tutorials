{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import faiss\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: RobertaModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "word_embedding_model = models.Transformer(\"klue/roberta-base\")\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    \n",
    "    return sum_embeddings / sum_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    token_embeddings[input_mask_expanded == 0] = -1e9\n",
    "    return torch.max(token_embeddings, 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('snunlp/KR-SBERT-V40K-klueNLI-augSTS')\n",
    "\n",
    "embs = model.encode(['잠이 안 옵니다',\n",
    "                     '졸음이 옵니다',\n",
    "                     '기차가 옵니다'])\n",
    "\n",
    "cos_scores = util.cos_sim(embs, embs)\n",
    "print(cos_scores)\n",
    "# tensor([[1.0000, 0.6410, 0.1887],\n",
    "#         [0.6410, 1.0000, 0.2730],\n",
    "#         [0.1887, 0.2730, 1.0000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('clip-ViT-B-32')\n",
    "\n",
    "img_embs = model.encode([Image.open('dog.jpg'), Image.open('cat.jpg')])\n",
    "text_embs = model.encode(['A dog on grass', 'Brown cat on yellow background'])\n",
    "\n",
    "cos_scores = util.cos_sim(img_embs, text_embs)\n",
    "print(cos_scores)\n",
    "# tensor([[0.2771, 0.1509],\n",
    "#         [0.2071, 0.3180]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "klue_mrc_dataset = load_dataset('klue', 'mrc', split='train')\n",
    "sentence_model = SentenceTransformer('snunlp/KR-SBERT-V40K-klueNLI-augSTS')\n",
    "\n",
    "print(klue_mrc_dataset)\n",
    "print(sentence_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "klue_mrc_dataset = klue_mrc_dataset.train_test_split(train_size=1000, shuffle=False)\n",
    "print(klue_mrc_dataset)\n",
    "\n",
    "klue_mrc_dataset = klue_mrc_dataset['train']\n",
    "\n",
    "embeddings = sentence_model.encode(klue_mrc_dataset['context'])\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "index.add(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"이번 연도에는 언제 비가 많이 올까?\"\n",
    "query_embedding = sentence_model.encode([query])\n",
    "\n",
    "distances, indices = index.search(query_embedding, 3)\n",
    "print(distances)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in indices[0]:\n",
    "    print(klue_mrc_dataset['context'][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = klue_mrc_dataset[3]['question']\n",
    "print(query, '\\n')\n",
    "\n",
    "query_embedding = sentence_model.encode([query])\n",
    "distances, indices = index.search(query_embedding, 3)\n",
    "\n",
    "for idx in indices[0]:\n",
    "    print(\"=\" * 50)\n",
    "    print(idx)\n",
    "    print(klue_mrc_dataset['context'][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Document, VectorStoreIndex, ServiceContext, Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed_model = HuggingFaceEmbedding(model_name=\"intfloat/multilingual-e5-large-instruct\")\n",
    "# service_context = ServiceContext.from_defaults(embed_model=embed_model, llm=None)\n",
    "\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"intfloat/multilingual-e5-large-instruct\")\n",
    "Settings.llm = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = klue_mrc_dataset[:100]['context']\n",
    "documents = [Document(text=t) for t in text_list]\n",
    "\n",
    "index_llama = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from typing import List\n",
    "from collections import defaultdict\n",
    "from transformers import PreTrainedTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BM25:\n",
    "    def __init__(self, corpus, tokenizer):\n",
    "        \"\"\"\n",
    "        tokenizer : 토크나이저\n",
    "        corpus : 문서 데이터셋\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.corpus = corpus\n",
    "\n",
    "        ## 각 문서를 토큰화\n",
    "        self.tokenized_corpus = self.tokenizer(corpus, add_special_tokens=False)['input_ids']\n",
    "        \n",
    "        ## 토큰화된 문서들의 수\n",
    "        self.n_docs = len(self.tokenized_corpus)\n",
    "\n",
    "        ## 전체 문서들의 평균 길이\n",
    "        self.avg_doc_lens = sum(len(lst) for lst in self.tokenized_corpus) / len(self.tokenized_corpus)\n",
    "\n",
    "        ## IDF 계산\n",
    "        self.idf = self.calculate_idf()\n",
    "\n",
    "        ## TF 계산\n",
    "        self.term_freqs = self.calculate_term_freqs()\n",
    "\n",
    "    def calculate_idf(self):\n",
    "        idf = defaultdict(float)\n",
    "        ## 토큰화된 문서들을 순회\n",
    "        for doc in self.tokenized_corpus:\n",
    "            ## 토큰화된 문서의 토큰들의 등장 횟수를 카운팅하고 딕셔너리에 저장.\n",
    "            for token_id in set(doc):\n",
    "                idf[token_id] += 1\n",
    "\n",
    "        for token_id, doc_frequency in idf.items():\n",
    "            idf[token_id] = math.log(((self.n_docs - doc_frequency + 0.5) / (doc_frequency + 0.5)) + 1)\n",
    "\n",
    "        return idf\n",
    "    \n",
    "    def calculate_term_freqs(self):\n",
    "        term_freqs = [defaultdict(int) for _ in range(self.n_docs)]\n",
    "        \n",
    "        ## 토큰화된 문서들을 순회\n",
    "        for i, doc in enumerate(self.tokenized_corpus):\n",
    "            for token_id in doc:\n",
    "                term_freqs[i][token_id] += 1\n",
    "\n",
    "        return term_freqs\n",
    "    \n",
    "    def get_scores(self, query, k1=1.2, b=0.75):\n",
    "        query = self.tokenizer([query], add_special_tokens=False)['input_ids'][0]\n",
    "        scores = np.zeros(self.n_docs)\n",
    "\n",
    "        for q in query:\n",
    "            idf = self.idf[q]\n",
    "            for i, term_freq in enumerate(self.term_freqs):\n",
    "                q_frequency = term_freq[q]\n",
    "                doc_len = len(self.tokenized_corpus[i])\n",
    "                score_q = idf * (q_frequency * (k1 + 1)) / ((q_frequency) + k1 * (1 - b + b * (doc_len / self.avg_doc_lens)))\n",
    "                scores[i] += score_q\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    def get_top_k(self, query, k):\n",
    "        scores = self.get_scores(query)\n",
    "        top_k_indices = np.argsort(scores)[-k:][::-1]\n",
    "        top_k_scores = scores[top_k_indices]\n",
    "\n",
    "        return top_k_scores, top_k_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-base\")\n",
    "\n",
    "bm25 = BM25(['안녕하세요', '반갑습니다', '안녕 서울'], tokenizer)\n",
    "bm25.get_scores('안녕')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = BM25(klue_mrc_dataset['context'], tokenizer)\n",
    "\n",
    "query = '아번 연도에는 언제 비가 많이 올까?'\n",
    "_, bm25_search_ranking = bm25.get_top_k(query, 100)\n",
    "\n",
    "for idx in bm25_search_ranking[:3]:\n",
    "    print(klue_mrc_dataset['context'][idx][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = klue_mrc_dataset[3]['question']\n",
    "_, bm25_search_ranking = bm25.get_top_k(query, 100)\n",
    "\n",
    "for idx in bm25_search_ranking[:3]:\n",
    "    print(klue_mrc_dataset['context'][idx][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def reciprocal_rank_fusion(rankings, k=5):\n",
    "    rrf = defaultdict(float)\n",
    "    for ranking in rankings:\n",
    "        for i, doc_id in enumerate(ranking, 1):\n",
    "            rrf[doc_id] += 1.0 / (k + i)\n",
    "\n",
    "    return sorted(rrf.items(), key=lambda x : x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankings = [[1, 4, 3, 5, 6], [2, 1, 3, 6, 4]]\n",
    "reciprocal_rank_fusion(rankings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_vector_search(query, k):\n",
    "    query_embedding = sentence_model.encode([query])\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    \n",
    "    return distances[0], indices[0]\n",
    "\n",
    "def hybrid_search(query, k=20):\n",
    "    _, dense_search_ranking = dense_vector_search(query, 100)\n",
    "    _, bm25_search_ranking = bm25.get_top_k(query, 100)\n",
    "\n",
    "    results = reciprocal_rank_fusion([dense_search_ranking, bm25_search_ranking], k=k)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"이번 연도에는 언제 비가 많이 올까?\"\n",
    "print(\"검색 쿼리 문장: \", query)\n",
    "results = hybrid_search(query)\n",
    "for idx, score in results[:3]:\n",
    "  print(klue_mrc_dataset['context'][idx][:50])\n",
    "\n",
    "print(\"=\" * 80)\n",
    "query = klue_mrc_dataset[3]['question'] # 로버트 헨리 딕이 1946년에 매사추세츠 연구소에서 개발한 것은 무엇인가?\n",
    "print(\"검색 쿼리 문장: \", query)\n",
    "\n",
    "results = hybrid_search(query)\n",
    "for idx, score in results[:3]:\n",
    "  print(klue_mrc_dataset['context'][idx][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
