{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pervinco/LLM-tutorials/vllm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-12-23 08:27:10,426\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "\n",
    "from openai import OpenAI\n",
    "from dataclasses import dataclass\n",
    "from vllm import LLM, SamplingParams\n",
    "from typing import List, Dict, Optional\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../keys.env\")\n",
    "api_key = os.getenv('GRAVY_LAB_OPENAI')\n",
    "\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "응답: 소년의 아버지인 외과의사가 말한 대사를 해석하면, \"나는 수술을 할 수 없어! 이 아이는 내 아들이니까!\"라는 뜻입니다. 따라서, 이 외과의사는 소년의 아버지입니다.\n"
     ]
    }
   ],
   "source": [
    "def generate_completion(prompt: str, model: str = \"gpt-3.5-turbo\", max_tokens: int = 100) -> str:\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=0\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "\n",
    "# 프롬프트 입력\n",
    "prompt = '아이의 아버지인 외과의사가 말했어. \"난 수술 못해! 이 아이는 내 아들이라고!\" 이 외과의사는 소년에게 누구일까요?'\n",
    "\n",
    "# 응답 생성\n",
    "response = generate_completion(prompt)\n",
    "print(f\"응답: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LLM(model=\"meta-llama/Llama-3.2-1B\", dtype=\"float16\")\n",
    "# prompt = '아이의 아버지인 외과의사가 말했어. \"난 수술 못해! 이 아이는 내 아들이라고!\" 이 외과의사는 소년에게 누구일까요?'\n",
    "# model.generate(prompt, use_tqdm=False)[0].outputs[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from vllm import LLM, SamplingParams\n",
    "from typing import List, Dict, Optional\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Path:\n",
    "    reasoning_text: str\n",
    "    score: float\n",
    "    answer_span: str\n",
    "    num_path: int\n",
    "\n",
    "@dataclass\n",
    "class DecodingInfo:\n",
    "    question: str\n",
    "    paths: List[Path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_output_details(outputs):\n",
    "    \"\"\"\n",
    "    outputs의 각 요소를 자세히 출력하는 함수\n",
    "    \"\"\"\n",
    "    for i, output in enumerate(outputs, 1):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Output #{i}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # 1. 기본 정보\n",
    "        print(\"\\n[기본 정보]\")\n",
    "        print(f\"Request ID: {output.request_id}\")\n",
    "        print(f\"Prompt: {output.prompt}\")\n",
    "        print(f\"Finished: {output.finished}\")\n",
    "        \n",
    "        # 2. 생성된 텍스트\n",
    "        print(\"\\n[생성된 텍스트]\")\n",
    "        for out in output.outputs:\n",
    "            print(f\"Index: {out.index}\")\n",
    "            print(f\"Text: {out.text}\")\n",
    "            print(f\"Cumulative LogProb: {out.cumulative_logprob:.4f}\")\n",
    "            \n",
    "        # 3. 토큰별 상세 정보\n",
    "        print(\"\\n[토큰별 상세 정보]\")\n",
    "        print(f\"{'Token':^20} | {'LogProb':^10} | {'Alternative Token':^20} | {'Alt LogProb':^10}\")\n",
    "        print(\"-\" * 65)\n",
    "        \n",
    "        for out in output.outputs:\n",
    "            for logprob_dict in out.logprobs:\n",
    "                for token_id, prob_info in logprob_dict.items():\n",
    "                    # 메인 토큰 정보\n",
    "                    token = prob_info.decoded_token\n",
    "                    if token == '\\n': token = '\\\\n'\n",
    "                    if token == '\\t': token = '\\\\t'\n",
    "                    logprob = prob_info.logprob\n",
    "                    \n",
    "                    # 대안 토큰이 있는 경우 (항상 2개의 토큰이 있다고 가정)\n",
    "                    alt_token = \"\"\n",
    "                    alt_logprob = \"\"\n",
    "                    \n",
    "                    print(f\"{token:^20} | {logprob:^10.4f} | {alt_token:^20} | {alt_logprob:^10}\")\n",
    "        \n",
    "        print(\"\\n\")\n",
    "\n",
    "# 사용 예시:\n",
    "# outputs = decoder.generate_paths(prompts)\n",
    "# print_output_details(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoTDecoder:\n",
    "    \"\"\"\n",
    "    논문에서는 greedy decoding 대신 top-k를 이용하여 다양한 경로를 탐색하는 것을 권장합니다.\n",
    "    특히 각 경로에서 어떻게 생각하는지 평가할 수 있도록 여러 경로에서 다양한 토큰을 샘플링 해야한다고 설명합니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 model_name: str, ## 사용할 모델 이름\n",
    "                 device: str = 'cuda', \n",
    "                 max_new_tokens: int = 100, ## 생성할 토큰의 최대 길이\n",
    "                 topk: int = 5, ## 각 질문에 대해 탐색할 초기 토큰 개수\n",
    "                 stop: List[str] = ['\\n\\n질문', '질문', 'Q:', '\\n\\nQ:', '\\n\\nExercise'], ## 생성이 멈추는 특정 단어 리스트\n",
    "                 prompt: str = '', ## 프롬프트 텍스트\n",
    "                 pattern: str = r'[가-힣a-zA-Z0-9\\s]+'): ## 답변에서 추출할 텍스트 패턴(정규표현식)\n",
    "        \n",
    "        self.model = LLM(model=model_name, dtype='float16')\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.device = device\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "        self.stop = stop\n",
    "        self.topk = topk\n",
    "        self.model.llm_engine.model_config.max_logprobs = self.topk + 1\n",
    "        self.prompt = prompt\n",
    "        self.pattern = pattern\n",
    "\n",
    "\n",
    "    def format_prompt(self, raw_prompt: str) -> str:\n",
    "        return f'질문:{raw_prompt}\\n답변:{self.prompt}'\n",
    "    \n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def get_first_topk_tokens(self, prompt: str) -> Dict[str, List]:\n",
    "        \"\"\"\n",
    "        1단계 :  top-k 토큰 선택\n",
    "        formatted_prompt가 입력되어 모델이 예측하는 가장 가능성이 높은 top-k 단어를 계산\n",
    "          - n=1 : 동일한 프롬프트에서 몇 개의 결과(문장)를 생성하고 반환할 것인가? ex) n=5인 경우 하나의 입력에 대해 5개의 서로 다른 문장을 생성한다.\n",
    "          - top_p : 확률이 상위 100%에 포함되는 모든 단어를 고려. 단계별 토큰들의 확률 누적 합이 p를 넘지 않는 단어들만 선택한다.\n",
    "          - max_tokens : 모델이 출력할 문장의 최대 길이.\n",
    "          - logprobs : 상위 k개 단어의 로그 확률값 반환.\n",
    "\n",
    "        - output : 누적합이 top_p=1 이 되기 이전까지의 토큰들\n",
    "        \"\"\"\n",
    "        sampling_params = SamplingParams(n=1, \n",
    "                                         temperature=0, \n",
    "                                         top_p=1, \n",
    "                                         max_tokens=1, \n",
    "                                         logprobs=self.topk, \n",
    "                                         stop=self.stop)\n",
    "\n",
    "        # 모델이 입력된 prompt을 기준으로 prompt 뒤에 올 10개의 단어를 예측합니다. \n",
    "        outputs = self.model.generate(prompt, sampling_params, use_tqdm=False)[0].outputs[0].logprobs[0]\n",
    "\n",
    "        # decoded는 \"그\", \"\\n\\n\", \"소\", \"아이\", \"이\", \"아\", \"어\" 등의 단어가 생성되어 저장되어 있습니다. \n",
    "        # probs는 -2.064455270767212, -3.392580270767212 등의 로그 확률이 저정되어 있습니다. \n",
    "        # token_id는 당연히 token_id가 저장되어 있습니다. \n",
    "        topk_tokens = {'decoded': [], 'probs': [], 'token_id': [], 'logprobs': []}\n",
    "        for token_id, logprob_obj in outputs.items():\n",
    "            topk_tokens['logprobs'].append({token_id: logprob_obj})\n",
    "            topk_tokens['decoded'].append(logprob_obj.decoded_token)\n",
    "            topk_tokens['probs'].append(logprob_obj.logprob)\n",
    "            topk_tokens['token_id'].append(token_id)\n",
    "\n",
    "        # 로그 확률을 실제 확률로 변환합니다. \n",
    "        topk_tokens['probs'] = torch.exp(torch.tensor(topk_tokens['probs'])).tolist()\n",
    "\n",
    "        return topk_tokens\n",
    "    \n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def generate_paths(self, prompts: List[str]) -> Dict[int, Dict]:\n",
    "        \"\"\"\n",
    "        topk개의 토큰을 기반으로 다양한 경로를 생성해야 한다. 그 과정을 코드로 구현.\n",
    "\n",
    "        input : ['질문:아이의 아버지인 외과의사가 말했어. \"난 수술 못해! 이 아이는 내 아들이라고!\" 이 외과의사는 소년에게 누구일까요?\\n답변:아이', ...]\n",
    "        max_tokens = self.max_new_tokens : 최대 self.max_new_tokens인 길이의 문장이 생성된다.\n",
    "\n",
    "        generate 메서드는 배치 처리를 지원하므로, prompts의 각 원소는 독립적인 입력으로 간주되어 원소별 독립적인 답변을 추론하고 생성하게 된다.\n",
    "        \"\"\"\n",
    "        sampling_params = SamplingParams(n=1, temperature=0, top_p=1, max_tokens=self.max_new_tokens, logprobs=2, stop=self.stop)\n",
    "        \n",
    "        return self.model.generate(prompts, sampling_params, use_tqdm=False)\n",
    "    \n",
    "\n",
    "    # 질문과 Reasoning의 유사도 계산하는 함수\n",
    "    def calculate_question_similarity(self, question: str, reasoning: str) -> float:\n",
    "        \"\"\" 질문과 reasoning 간의 유사도를 계산하는 간단한 함수. 유사도가 높으면 패널티를 부여한다 \"\"\"\n",
    "        question_words = set(question.split())\n",
    "        reasoning_words = set(reasoning.split())\n",
    "        \n",
    "        # 질문과 reasoning 간에 공통된 단어의 비율 계산\n",
    "        common_words = question_words.intersection(reasoning_words)\n",
    "        similarity = len(common_words) / len(question_words) if question_words else 0\n",
    "        \n",
    "        return similarity\n",
    "    \n",
    "\n",
    "    def calculate_score(self, prompt: str, topk_tokens: Dict, outputs: Dict) -> DecodingInfo:\n",
    "        \"\"\"\n",
    "        모델 출력과 top-k 토큰 기반 reasoning 결과를 분석하여, 각 경로의 점수를 계산하고 정보를 구조화하는 함수.\n",
    "        top_k 경로 중 어떤 경로가 가장 적절한지 평가한다.\n",
    "          - question: 원래 입력 질문.\n",
    "          - paths: 각 경로(topk_tokens의 각 토큰)별 결과와 점수를 포함한 리스트.\n",
    "        \"\"\"\n",
    "        paths = []\n",
    "        for k, output in enumerate(outputs):\n",
    "            print(\"=\" * 70)\n",
    "            ## 각 outputs 항목에 대해, 모델이 생성한 topk_tokens의 k번째 토큰과 모델 출력 텍스트를 이어붙여 reasoning(추론 결과)을 만든다.\n",
    "            reasoning = topk_tokens['decoded'][k] + output.outputs[0].text\n",
    "            reasoning = reasoning.strip() ## 불필요한 공백 제거\n",
    "            print(f\"topk_tokens : {topk_tokens['decoded'][k]}, output : {output.outputs[0].text}\")\n",
    "            print(f\"reasoning : {reasoning}\")\n",
    "            \n",
    "            ## 질문과 reasoning 간의 유사도를 계산 (간단한 방식으로 질문이 포함되었는지 확인)\n",
    "            ## calculate_question_similarity는 질문과 reasoning에 포함된 공통 단어의 비율을 계산하여 간단한 유사도를 반환. 이 유사도는 패널티를 적용하는 데 사용.\n",
    "            question_similarity = self.calculate_question_similarity(prompt, reasoning)\n",
    "            \n",
    "            ## reasoning을 토크나이저로 인코딩하여 토큰 정보와 offset(mapping) 정보를 얻는다.\n",
    "            encode = self.tokenizer(reasoning, return_offsets_mapping=True) ## 각 토큰이 원래 텍스트에서 차지하는 **시작 및 끝 위치(offset)**를 반환\n",
    "            answer_span = re.findall(self.pattern, reasoning) ## reasoning에서 정규표현식(self.pattern)과 일치하는 텍스트를 찾는다.\n",
    "            print(f\"encode : {encode}\")\n",
    "            print(f\"answer_span : {answer_span}\")\n",
    "            \"\"\"\n",
    "            예시\n",
    "                pattern = r'[가-힣a-zA-Z0-9\\s]+'\n",
    "                reasoning = \"이 외과의사는 소년의 아버지입니다.\"\n",
    "                answer_span = re.findall(pattern, reasoning)\n",
    "\n",
    "                ['이 외과의사는 소년의 아버지입니다']\n",
    "            \"\"\"\n",
    "            \n",
    "            ## answer_span이 reasoning에서 정확히 어디에 있는지 위치를 확인한다.\n",
    "            score = 0\n",
    "            if len(answer_span):\n",
    "                answer_span = answer_span[-1]\n",
    "                last_pattern_span = (reasoning.rfind(answer_span), reasoning.rfind(answer_span) + len(answer_span)) ## answer_span이 reasoning에서 마지막으로 등장하는 위치를 찾는다.\n",
    "                print(f\"last_pattern_span : {last_pattern_span}\")\n",
    "                \"\"\"\n",
    "                reasoning: \"이 외과의사는 소년의 아버지입니다.\"\n",
    "                answer_span: \"소년의 아버지\"\n",
    "                last_pattern_span = (8, 14)  --> 8은 \"소년의\"의 시작 위치, 14는 \"아버지\"의 끝 위치\n",
    "                \"\"\"\n",
    "\n",
    "                ## answer_span에 해당하는 token_id 찾기\n",
    "                idx_answer = [i for i, span in enumerate(encode.offset_mapping)\n",
    "                            if (span[0] >= last_pattern_span[0] and span[1] <= last_pattern_span[1]) or\n",
    "                                (span[0] <= last_pattern_span[0] and span[1] >= last_pattern_span[1]) or\n",
    "                                (span[0] <= last_pattern_span[0] and span[1] > last_pattern_span[0])]\n",
    "\n",
    "                token_id = [encode.input_ids[idx] for idx in idx_answer] ## 위에서 찾은 idx_answer를 바탕으로, encode.input_ids에서 해당 토큰 ID를 가져온다.\n",
    "                print(f\"token_id : {token_id}\")\n",
    "                output.outputs[0].logprobs.insert(0, topk_tokens['logprobs'][k])\n",
    "\n",
    "                filtered_answer = [output for i, output in enumerate(output.outputs[0].logprobs) if i in idx_answer] ## 모델이 생성한 각 토큰의 로그 확률 정보에서, idx_answer에 해당하는 토큰들만 추출\n",
    "                print(f\"filtered_answer : {filtered_answer}\")\n",
    "\n",
    "                ## 로그 확률을 지수 함수(exp)로 변환하여 실제 확률 값으로 바꾼다.\n",
    "                sum_answer_span_probs = 0\n",
    "                for logprob_dict in filtered_answer:\n",
    "                    logprob_list = list(logprob_dict.items())\n",
    "                    if len(logprob_list) == 2:\n",
    "                        prob_diff = (torch.exp(torch.tensor([logprob_list[0][1].logprob])) - torch.exp(torch.tensor([logprob_list[1][1].logprob]))).item()\n",
    "                    else:\n",
    "                        prob_diff = torch.exp(torch.tensor([logprob_list[0][1].logprob])).item()\n",
    "                    sum_answer_span_probs += prob_diff\n",
    "                \n",
    "                ## 질문과 비슷한 답변일 경우 페널티 적용\n",
    "                if question_similarity > 0.5:  # 질문과의 유사도가 높을수록 점수를 낮추기 위해 0.5 이상의 유사도에 패널티 적용\n",
    "                    sum_answer_span_probs *= (1 - question_similarity)  # 유사도가 높을수록 점수를 감소시킴\n",
    "\n",
    "                ## 최종 점수 계산.\n",
    "                score = 0 if len(filtered_answer) == 0 else sum_answer_span_probs / len(filtered_answer)\n",
    "                answer_span = self.tokenizer.decode(token_id, skip_special_tokens=True).strip()\n",
    "            else:\n",
    "                answer_span = '|<NotFound>|'\n",
    "\n",
    "            paths.append(Path(reasoning_text=reasoning, \n",
    "                            score=score,\n",
    "                            answer_span=answer_span,\n",
    "                            num_path=k))\n",
    "\n",
    "        return DecodingInfo(question=prompt, paths=paths)\n",
    "\n",
    "\n",
    "    ## chain of thought 탐색\n",
    "    def search_cots(self, raw_prompt: str) -> DecodingInfo:\n",
    "        formatted_prompt = self.format_prompt(raw_prompt) ## 질문 : ..., 답변 : ... 형식으로 변경\n",
    "        print(f\"format_prompt : {formatted_prompt}\\n\")\n",
    "\n",
    "        ## 질문에 이어서 나올 가능성이 높은 top-k 토큰들을 생성(단어를 top_k개만큼 생성)하고 token_id, 생성된 토큰, 확률값을 저장한 topk_token을 생성.\n",
    "        ## ['이', '아이', '아', ' 이', '소']\n",
    "        topk_tokens = self.get_first_topk_tokens(formatted_prompt)\n",
    "        print(f\"top k tokens\")\n",
    "        for k, v in topk_tokens.items():\n",
    "            print(k, v)\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        ## 생성된 topk 토큰과 질문을 각각 이어 붙여주고 prompts라는 리스트에 저장. \n",
    "        ## '질문:아이의 아버지인 외과의사가 말했어. \"난 수술 못해! 이 아이는 내 아들이라고!\" 이 외과의사는 소년에게 누구일까요?\\n답변:이', \n",
    "        ## '질문:아이의 아버지인 외과의사가 말했어. \"난 수술 못해! 이 아이는 내 아들이라고!\" 이 외과의사는 소년에게 누구일까요?\\n답변:아이'\n",
    "        prompts = [formatted_prompt + token for token in topk_tokens['decoded']] ## top_k가 정답에 이어 붙어진 5개의 독립적인 프롬프트가 리스트 형태로 연결되어 프롬프트를 구성함.\n",
    "        print(f\"prompts\")\n",
    "        for prompt in prompts:\n",
    "            print(prompt)\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        outputs = self.generate_paths(prompts)\n",
    "        # print_output_details(outputs)\n",
    "        print(f\"outputs\")\n",
    "        for output in outputs:\n",
    "            print(output)\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        return self.calculate_score(raw_prompt, topk_tokens, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-23 08:27:13 config.py:2171] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 12-23 08:27:16 config.py:478] This model supports multiple tasks: {'embed', 'score', 'reward', 'classify', 'generate'}. Defaulting to 'generate'.\n",
      "WARNING 12-23 08:27:16 arg_utils.py:1086] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 12-23 08:27:16 config.py:1364] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "INFO 12-23 08:27:16 llm_engine.py:249] Initializing an LLM engine (v0.6.5) with config: model='meta-llama/Llama-3.2-1B', speculative_config=None, tokenizer='meta-llama/Llama-3.2-1B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.2-1B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, mm_cache_preprocessor=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 12-23 08:27:18 selector.py:120] Using Flash Attention backend.\n",
      "INFO 12-23 08:27:18 model_runner.py:1092] Starting to load model meta-llama/Llama-3.2-1B...\n",
      "INFO 12-23 08:27:18 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "INFO 12-23 08:27:19 weight_utils.py:288] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.32it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.32it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-23 08:27:19 model_runner.py:1097] Loading model weights took 2.3185 GB\n",
      "INFO 12-23 08:27:20 worker.py:241] Memory profiling takes 0.46 seconds\n",
      "INFO 12-23 08:27:20 worker.py:241] the current vLLM instance can use total_gpu_memory (23.64GiB) x gpu_memory_utilization (0.90) = 21.28GiB\n",
      "INFO 12-23 08:27:20 worker.py:241] model weights take 2.32GiB; non_torch_memory takes 0.11GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 17.67GiB.\n",
      "INFO 12-23 08:27:20 gpu_executor.py:76] # GPU blocks: 36197, # CPU blocks: 8192\n",
      "INFO 12-23 08:27:20 gpu_executor.py:80] Maximum concurrency for 131072 tokens per request: 4.42x\n",
      "INFO 12-23 08:27:21 model_runner.py:1413] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 12-23 08:27:21 model_runner.py:1417] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 12-23 08:27:30 model_runner.py:1527] Graph capturing finished in 9 secs, took 0.46 GiB\n",
      "INFO 12-23 08:27:30 llm_engine.py:446] init engine (profile, create kv cache, warmup model) took 10.58 seconds\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "decoder = CoTDecoder(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "format_prompt : 질문:아이의 아버지인 외과의사가 말했어. \"난 수술 못해! 이 아이는 내 아들이라고!\" 이 외과의사는 소년에게 누구일까요?\n",
      "답변:\n",
      "\n",
      "top k tokens\n",
      "decoded ['이', '아이', '아', ' 이', '소']\n",
      "probs [0.09347356110811234, 0.06275518238544464, 0.05003279075026512, 0.03631991147994995, 0.02741570770740509]\n",
      "token_id [13094, 114714, 54059, 23955, 44690]\n",
      "logprobs [{13094: Logprob(logprob=-2.3700766563415527, rank=1, decoded_token='이')}, {114714: Logprob(logprob=-2.7685141563415527, rank=2, decoded_token='아이')}, {54059: Logprob(logprob=-2.9950766563415527, rank=3, decoded_token='아')}, {23955: Logprob(logprob=-3.3153891563415527, rank=4, decoded_token=' 이')}, {44690: Logprob(logprob=-3.5966391563415527, rank=5, decoded_token='소')}]\n",
      "\n",
      "\n",
      "prompts\n",
      "질문:아이의 아버지인 외과의사가 말했어. \"난 수술 못해! 이 아이는 내 아들이라고!\" 이 외과의사는 소년에게 누구일까요?\n",
      "답변:이\n",
      "질문:아이의 아버지인 외과의사가 말했어. \"난 수술 못해! 이 아이는 내 아들이라고!\" 이 외과의사는 소년에게 누구일까요?\n",
      "답변:아이\n",
      "질문:아이의 아버지인 외과의사가 말했어. \"난 수술 못해! 이 아이는 내 아들이라고!\" 이 외과의사는 소년에게 누구일까요?\n",
      "답변:아\n",
      "질문:아이의 아버지인 외과의사가 말했어. \"난 수술 못해! 이 아이는 내 아들이라고!\" 이 외과의사는 소년에게 누구일까요?\n",
      "답변: 이\n",
      "질문:아이의 아버지인 외과의사가 말했어. \"난 수술 못해! 이 아이는 내 아들이라고!\" 이 외과의사는 소년에게 누구일까요?\n",
      "답변:소\n",
      "\n",
      "\n",
      "outputs\n",
      "RequestOutput(request_id=1, prompt='질문:아이의 아버지인 외과의사가 말했어. \"난 수술 못해! 이 아이는 내 아들이라고!\" 이 외과의사는 소년에게 누구일까요?\\n답변:이', prompt_token_ids=[128000, 103194, 52688, 25, 114714, 21028, 123816, 32428, 103807, 125701, 114333, 101264, 102621, 32179, 13, 330, 103777, 29833, 102835, 104352, 34983, 0, 23955, 101817, 16969, 67236, 49508, 102823, 105771, 9135, 23955, 103807, 125701, 117396, 101228, 100392, 102244, 123621, 33177, 117677, 5380, 109659, 104449, 25, 13094], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' 외과의사는 소년의 아버지인 외과의사입니다.', token_ids=(103807, 125701, 117396, 101228, 100392, 21028, 123816, 32428, 103807, 125701, 56154, 80052, 13, 128001), cumulative_logprob=-11.74598502740264, logprobs=[{103807: Logprob(logprob=-1.7521418333053589, rank=1, decoded_token=' 외'), 101817: Logprob(logprob=-1.9552668333053589, rank=2, decoded_token=' 아이')}, {125701: Logprob(logprob=-0.08718262612819672, rank=1, decoded_token='과의'), 54780: Logprob(logprob=-2.8059325218200684, rank=2, decoded_token='과')}, {117396: Logprob(logprob=-0.3540898859500885, rank=1, decoded_token='사는'), 114333: Logprob(logprob=-1.9634648561477661, rank=2, decoded_token='사가')}, {101228: Logprob(logprob=-1.9667726755142212, rank=1, decoded_token=' 소'), 101817: Logprob(logprob=-2.0995850563049316, rank=2, decoded_token=' 아이')}, {100392: Logprob(logprob=-0.04229452833533287, rank=1, decoded_token='년'), 116899: Logprob(logprob=-4.370419502258301, rank=2, decoded_token='년에')}, {21028: Logprob(logprob=-0.6454375982284546, rank=1, decoded_token='의'), 102244: Logprob(logprob=-1.7782500982284546, rank=2, decoded_token='에게')}, {123816: Logprob(logprob=-0.8672740459442139, rank=1, decoded_token=' 아버지'), 49508: Logprob(logprob=-2.164149045944214, rank=2, decoded_token=' 아')}, {32428: Logprob(logprob=-1.3710933923721313, rank=1, decoded_token='인'), 80052: Logprob(logprob=-2.269530773162842, rank=2, decoded_token='입니다')}, {103807: Logprob(logprob=-0.7757647037506104, rank=1, decoded_token=' 외'), 123816: Logprob(logprob=-2.7992022037506104, rank=2, decoded_token=' 아버지')}, {125701: Logprob(logprob=-0.06241424381732941, rank=1, decoded_token='과의'), 54780: Logprob(logprob=-2.9842891693115234, rank=2, decoded_token='과')}, {56154: Logprob(logprob=-0.7368443608283997, rank=1, decoded_token='사'), 114333: Logprob(logprob=-0.8462193608283997, rank=2, decoded_token='사가')}, {80052: Logprob(logprob=-1.5447012186050415, rank=1, decoded_token='입니다'), 101574: Logprob(logprob=-2.568138599395752, rank=2, decoded_token='였')}, {13: Logprob(logprob=-0.7634690403938293, rank=1, decoded_token='.'), 627: Logprob(logprob=-0.7790940403938293, rank=2, decoded_token='.\\n')}, {128001: Logprob(logprob=-0.7765048742294312, rank=1, decoded_token=''), 101228: Logprob(logprob=-2.6358799934387207, rank=2, decoded_token=' 소')}], finish_reason=stop, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1734942451.303079, last_token_time=1734942451.303079, first_scheduled_time=1734942451.3039036, first_token_time=1734942451.3109775, time_in_queue=0.0008246898651123047, finished_time=1734942451.362841, scheduler_time=0.0014631892554461956, model_forward_time=None, model_execute_time=None), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})\n",
      "RequestOutput(request_id=2, prompt='질문:아이의 아버지인 외과의사가 말했어. \"난 수술 못해! 이 아이는 내 아들이라고!\" 이 외과의사는 소년에게 누구일까요?\\n답변:아이', prompt_token_ids=[128000, 103194, 52688, 25, 114714, 21028, 123816, 32428, 103807, 125701, 114333, 101264, 102621, 32179, 13, 330, 103777, 29833, 102835, 104352, 34983, 0, 23955, 101817, 16969, 67236, 49508, 102823, 105771, 9135, 23955, 103807, 125701, 117396, 101228, 100392, 102244, 123621, 33177, 117677, 5380, 109659, 104449, 25, 114714], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='의 아버지인 외과의사가 말했어. \"난 수술 못해! 이 아이는 내 아들이라고!\" 이 외과의사는 소년에게 누구일까요?', token_ids=(21028, 123816, 32428, 103807, 125701, 114333, 101264, 102621, 32179, 13, 330, 103777, 29833, 102835, 104352, 34983, 0, 23955, 101817, 16969, 67236, 49508, 102823, 105771, 9135, 23955, 103807, 125701, 117396, 101228, 100392, 102244, 123621, 33177, 117677, 30, 128001), cumulative_logprob=-5.546759177115746, logprobs=[{21028: Logprob(logprob=-0.18891671299934387, rank=1, decoded_token='의'), 20565: Logprob(logprob=-3.8686041831970215, rank=2, decoded_token='가')}, {123816: Logprob(logprob=-0.3250461220741272, rank=1, decoded_token=' 아버지'), 49508: Logprob(logprob=-2.9500460624694824, rank=2, decoded_token=' 아')}, {32428: Logprob(logprob=-1.0971037149429321, rank=1, decoded_token='인'), 20565: Logprob(logprob=-1.3080412149429321, rank=2, decoded_token='가')}, {103807: Logprob(logprob=-0.04613583907485008, rank=1, decoded_token=' 외'), 23955: Logprob(logprob=-6.022698402404785, rank=2, decoded_token=' 이')}, {125701: Logprob(logprob=-0.011889545246958733, rank=1, decoded_token='과의'), 54780: Logprob(logprob=-4.902514457702637, rank=2, decoded_token='과')}, {114333: Logprob(logprob=-0.4176744818687439, rank=1, decoded_token='사가'), 117396: Logprob(logprob=-1.3551745414733887, rank=2, decoded_token='사는')}, {101264: Logprob(logprob=-0.08450466394424438, rank=1, decoded_token=' 말'), 113578: Logprob(logprob=-5.0688796043396, rank=2, decoded_token=' 말을')}, {102621: Logprob(logprob=-0.13536818325519562, rank=1, decoded_token='했'), 24486: Logprob(logprob=-3.2603681087493896, rank=2, decoded_token='한')}, {32179: Logprob(logprob=-0.05231114476919174, rank=1, decoded_token='어'), 18359: Logprob(logprob=-4.567935943603516, rank=2, decoded_token='을')}, {13: Logprob(logprob=-0.06584339588880539, rank=1, decoded_token='.'), 627: Logprob(logprob=-4.09709358215332, rank=2, decoded_token='.\\n')}, {330: Logprob(logprob=-0.09801702946424484, rank=1, decoded_token=' \"'), 1054: Logprob(logprob=-3.1683294773101807, rank=2, decoded_token=' “')}, {103777: Logprob(logprob=-0.01472591981291771, rank=1, decoded_token='난'), 109250: Logprob(logprob=-5.467850685119629, rank=2, decoded_token=' 난')}, {29833: Logprob(logprob=-0.023189539089798927, rank=1, decoded_token=' 수'), 23955: Logprob(logprob=-6.023189544677734, rank=2, decoded_token=' 이')}, {102835: Logprob(logprob=-0.0010609956225380301, rank=1, decoded_token='술'), 101392: Logprob(logprob=-8.86824893951416, rank=2, decoded_token='')}, {104352: Logprob(logprob=-0.01634177751839161, rank=1, decoded_token=' 못'), 120669: Logprob(logprob=-4.797591686248779, rank=2, decoded_token='못')}, {34983: Logprob(logprob=-0.0014988866169005632, rank=1, decoded_token='해'), 61816: Logprob(logprob=-7.345248699188232, rank=2, decoded_token=' 해')}, {0: Logprob(logprob=-0.013585138134658337, rank=1, decoded_token='!'), 9135: Logprob(logprob=-5.201085090637207, rank=2, decoded_token='!\"')}, {23955: Logprob(logprob=-0.022627614438533783, rank=1, decoded_token=' 이'), 109250: Logprob(logprob=-5.561690330505371, rank=2, decoded_token=' 난')}, {101817: Logprob(logprob=-0.009862976148724556, rank=1, decoded_token=' 아이'), 49508: Logprob(logprob=-5.455175399780273, rank=2, decoded_token=' 아')}, {16969: Logprob(logprob=-0.007286402862519026, rank=1, decoded_token='는'), 21028: Logprob(logprob=-5.936974048614502, rank=2, decoded_token='의')}, {67236: Logprob(logprob=-0.01014882605522871, rank=1, decoded_token=' 내'), 109723: Logprob(logprob=-6.197649002075195, rank=2, decoded_token=' 내가')}, {49508: Logprob(logprob=-0.006289569195359945, rank=1, decoded_token=' 아'), 54059: Logprob(logprob=-6.631289482116699, rank=2, decoded_token='아')}, {102823: Logprob(logprob=-0.038405824452638626, rank=1, decoded_token='들이'), 65950: Logprob(logprob=-3.8352808952331543, rank=2, decoded_token='들')}, {105771: Logprob(logprob=-0.006789234932512045, rank=1, decoded_token='라고'), 35495: Logprob(logprob=-6.639601707458496, rank=2, decoded_token='고')}, {9135: Logprob(logprob=-0.17893140017986298, rank=1, decoded_token='!\"'), 25765: Logprob(logprob=-2.210181474685669, rank=2, decoded_token='!\"\\n')}, {23955: Logprob(logprob=-0.12044353038072586, rank=1, decoded_token=' 이'), 13094: Logprob(logprob=-4.222005844116211, rank=2, decoded_token='이')}, {103807: Logprob(logprob=-0.01751081459224224, rank=1, decoded_token=' 외'), 104065: Logprob(logprob=-5.017510890960693, rank=2, decoded_token='외')}, {125701: Logprob(logprob=-0.0071679335087537766, rank=1, decoded_token='과의'), 54780: Logprob(logprob=-6.022792816162109, rank=2, decoded_token='과')}, {117396: Logprob(logprob=-0.19185467064380646, rank=1, decoded_token='사는'), 114333: Logprob(logprob=-1.87935471534729, rank=2, decoded_token='사가')}, {101228: Logprob(logprob=-0.16632337868213654, rank=1, decoded_token=' 소'), 49508: Logprob(logprob=-4.846010684967041, rank=2, decoded_token=' 아')}, {100392: Logprob(logprob=-0.010965327732264996, rank=1, decoded_token='년'), 116899: Logprob(logprob=-5.448465347290039, rank=2, decoded_token='년에')}, {102244: Logprob(logprob=-0.0739128515124321, rank=1, decoded_token='에게'), 21028: Logprob(logprob=-3.4957878589630127, rank=2, decoded_token='의')}, {123621: Logprob(logprob=-0.06142662093043327, rank=1, decoded_token=' 누구'), 106200: Logprob(logprob=-5.225489139556885, rank=2, decoded_token=' 누')}, {33177: Logprob(logprob=-0.01662789285182953, rank=1, decoded_token='일'), 32428: Logprob(logprob=-5.829127788543701, rank=2, decoded_token='인')}, {117677: Logprob(logprob=-0.009986437857151031, rank=1, decoded_token='까요'), 101154: Logprob(logprob=-4.713111400604248, rank=2, decoded_token='까')}, {30: Logprob(logprob=-0.5475040078163147, rank=1, decoded_token='?'), 5380: Logprob(logprob=-0.9381290078163147, rank=2, decoded_token='?\\n')}, {128001: Logprob(logprob=-1.4494807720184326, rank=1, decoded_token=''), 101817: Logprob(logprob=-3.3479182720184326, rank=2, decoded_token=' 아이')}], finish_reason=stop, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1734942451.3033452, last_token_time=1734942451.3033452, first_scheduled_time=1734942451.3039036, first_token_time=1734942451.3109775, time_in_queue=0.0005583763122558594, finished_time=1734942451.4517498, scheduler_time=0.0027826265431940556, model_forward_time=None, model_execute_time=None), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})\n",
      "RequestOutput(request_id=3, prompt='질문:아이의 아버지인 외과의사가 말했어. \"난 수술 못해! 이 아이는 내 아들이라고!\" 이 외과의사는 소년에게 누구일까요?\\n답변:아', prompt_token_ids=[128000, 103194, 52688, 25, 114714, 21028, 123816, 32428, 103807, 125701, 114333, 101264, 102621, 32179, 13, 330, 103777, 29833, 102835, 104352, 34983, 0, 23955, 101817, 16969, 67236, 49508, 102823, 105771, 9135, 23955, 103807, 125701, 117396, 101228, 100392, 102244, 123621, 33177, 117677, 5380, 109659, 104449, 25, 54059], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='빠는 외과의사였습니다. 아이는 소년입니다.', token_ids=(110632, 16969, 103807, 125701, 56154, 101574, 39331, 13, 101817, 16969, 101228, 100392, 80052, 13, 128001), cumulative_logprob=-18.726994190365076, logprobs=[{110632: Logprob(logprob=-2.372326135635376, rank=2, decoded_token='빠'), 111721: Logprob(logprob=-2.372326135635376, rank=1, decoded_token='이가')}, {16969: Logprob(logprob=-1.2213284969329834, rank=1, decoded_token='는'), 20565: Logprob(logprob=-1.5963284969329834, rank=2, decoded_token='가')}, {103807: Logprob(logprob=-2.576667308807373, rank=1, decoded_token=' 외'), 101817: Logprob(logprob=-2.818854808807373, rank=2, decoded_token=' 아이')}, {125701: Logprob(logprob=-0.18366993963718414, rank=1, decoded_token='과의'), 54780: Logprob(logprob=-1.964919924736023, rank=2, decoded_token='과')}, {56154: Logprob(logprob=-0.40786880254745483, rank=1, decoded_token='사'), 114333: Logprob(logprob=-1.4547438621520996, rank=2, decoded_token='사가')}, {101574: Logprob(logprob=-1.972202181816101, rank=1, decoded_token='였'), 80052: Logprob(logprob=-2.5659523010253906, rank=2, decoded_token='입니다')}, {39331: Logprob(logprob=-1.4973011016845703, rank=1, decoded_token='습니다'), 105807: Logprob(logprob=-1.9660511016845703, rank=2, decoded_token='어요')}, {13: Logprob(logprob=-0.3849920928478241, rank=1, decoded_token='.'), 627: Logprob(logprob=-1.4318671226501465, rank=2, decoded_token='.\\n')}, {101817: Logprob(logprob=-2.2230846881866455, rank=1, decoded_token=' 아이'), 128001: Logprob(logprob=-2.2465221881866455, rank=2, decoded_token='')}, {16969: Logprob(logprob=-0.8634384274482727, rank=1, decoded_token='는'), 21028: Logprob(logprob=-1.582188367843628, rank=2, decoded_token='의')}, {101228: Logprob(logprob=-2.4449100494384766, rank=1, decoded_token=' 소'), 49508: Logprob(logprob=-2.5542850494384766, rank=2, decoded_token=' 아')}, {100392: Logprob(logprob=-0.05206810310482979, rank=1, decoded_token='년'), 103284: Logprob(logprob=-4.22394323348999, rank=2, decoded_token='녀')}, {80052: Logprob(logprob=-0.9559745788574219, rank=1, decoded_token='입니다'), 113743: Logprob(logprob=-1.7215995788574219, rank=2, decoded_token='이었')}, {13: Logprob(logprob=-0.5675016045570374, rank=1, decoded_token='.'), 627: Logprob(logprob=-0.9268766045570374, rank=2, decoded_token='.\\n')}, {128001: Logprob(logprob=-1.0036606788635254, rank=1, decoded_token=''), 101228: Logprob(logprob=-2.7614731788635254, rank=2, decoded_token=' 소')}], finish_reason=stop, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1734942451.303469, last_token_time=1734942451.303469, first_scheduled_time=1734942451.3039036, first_token_time=1734942451.3109775, time_in_queue=0.00043463706970214844, finished_time=1734942451.3667474, scheduler_time=0.0015309699811041355, model_forward_time=None, model_execute_time=None), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})\n",
      "RequestOutput(request_id=4, prompt='질문:아이의 아버지인 외과의사가 말했어. \"난 수술 못해! 이 아이는 내 아들이라고!\" 이 외과의사는 소년에게 누구일까요?\\n답변: 이', prompt_token_ids=[128000, 103194, 52688, 25, 114714, 21028, 123816, 32428, 103807, 125701, 114333, 101264, 102621, 32179, 13, 330, 103777, 29833, 102835, 104352, 34983, 0, 23955, 101817, 16969, 67236, 49508, 102823, 105771, 9135, 23955, 103807, 125701, 117396, 101228, 100392, 102244, 123621, 33177, 117677, 5380, 109659, 104449, 25, 23955], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' 외과의사는 소년의 아버지인 외과의사가 말했어. \"난 수술 못해! 이 아이는 내 아들이라고!\" 이 외과의사는 소년에게 누구일까요?', token_ids=(103807, 125701, 117396, 101228, 100392, 21028, 123816, 32428, 103807, 125701, 114333, 101264, 102621, 32179, 13, 330, 103777, 29833, 102835, 104352, 34983, 0, 23955, 101817, 16969, 67236, 49508, 102823, 105771, 9135, 23955, 103807, 125701, 117396, 101228, 100392, 102244, 123621, 33177, 117677, 30, 128001), cumulative_logprob=-15.008802231168374, logprobs=[{103807: Logprob(logprob=-1.7800184488296509, rank=1, decoded_token=' 외'), 101817: Logprob(logprob=-1.7878309488296509, rank=2, decoded_token=' 아이')}, {125701: Logprob(logprob=-0.08989930897951126, rank=1, decoded_token='과의'), 54780: Logprob(logprob=-2.7148993015289307, rank=2, decoded_token='과')}, {117396: Logprob(logprob=-0.3957357406616211, rank=1, decoded_token='사는'), 114333: Logprob(logprob=-1.833235740661621, rank=2, decoded_token='사가')}, {101228: Logprob(logprob=-1.849427342414856, rank=1, decoded_token=' 소'), 101817: Logprob(logprob=-2.2869272232055664, rank=2, decoded_token=' 아이')}, {100392: Logprob(logprob=-0.03867113217711449, rank=1, decoded_token='년'), 116899: Logprob(logprob=-4.429296016693115, rank=2, decoded_token='년에')}, {21028: Logprob(logprob=-0.6946696639060974, rank=1, decoded_token='의'), 102244: Logprob(logprob=-1.6087322235107422, rank=2, decoded_token='에게')}, {123816: Logprob(logprob=-0.8576052188873291, rank=1, decoded_token=' 아버지'), 49508: Logprob(logprob=-2.193542718887329, rank=2, decoded_token=' 아')}, {32428: Logprob(logprob=-1.3991416692733765, rank=1, decoded_token='인'), 80052: Logprob(logprob=-2.352266788482666, rank=2, decoded_token='입니다')}, {103807: Logprob(logprob=-0.6895384788513184, rank=1, decoded_token=' 외'), 123816: Logprob(logprob=-3.0879759788513184, rank=2, decoded_token=' 아버지')}, {125701: Logprob(logprob=-0.05676073208451271, rank=1, decoded_token='과의'), 54780: Logprob(logprob=-3.072385787963867, rank=2, decoded_token='과')}, {114333: Logprob(logprob=-0.7688286900520325, rank=1, decoded_token='사가'), 56154: Logprob(logprob=-0.8000786900520325, rank=2, decoded_token='사')}, {101264: Logprob(logprob=-1.3671327829360962, rank=1, decoded_token=' 말'), 111699: Logprob(logprob=-1.9530702829360962, rank=2, decoded_token='')}, {102621: Logprob(logprob=-0.6018763184547424, rank=1, decoded_token='했'), 24486: Logprob(logprob=-1.9612512588500977, rank=2, decoded_token='한')}, {32179: Logprob(logprob=-0.26329290866851807, rank=1, decoded_token='어'), 18359: Logprob(logprob=-2.7789177894592285, rank=2, decoded_token='을')}, {13: Logprob(logprob=-0.23400308191776276, rank=1, decoded_token='.'), 627: Logprob(logprob=-2.1402530670166016, rank=2, decoded_token='.\\n')}, {330: Logprob(logprob=-0.16106882691383362, rank=1, decoded_token=' \"'), 128001: Logprob(logprob=-4.043881416320801, rank=2, decoded_token='')}, {103777: Logprob(logprob=-0.0432661697268486, rank=1, decoded_token='난'), 96318: Logprob(logprob=-4.926078796386719, rank=2, decoded_token='내')}, {29833: Logprob(logprob=-0.07777655869722366, rank=1, decoded_token=' 수'), 23955: Logprob(logprob=-4.8277764320373535, rank=2, decoded_token=' 이')}, {102835: Logprob(logprob=-0.0012435331009328365, rank=1, decoded_token='술'), 101392: Logprob(logprob=-8.602806091308594, rank=2, decoded_token='')}, {104352: Logprob(logprob=-0.01766730286180973, rank=1, decoded_token=' 못'), 120669: Logprob(logprob=-4.947354793548584, rank=2, decoded_token='못')}, {34983: Logprob(logprob=-0.002820087829604745, rank=1, decoded_token='해'), 61816: Logprob(logprob=-6.862195014953613, rank=2, decoded_token=' 해')}, {0: Logprob(logprob=-0.030178124085068703, rank=1, decoded_token='!'), 9135: Logprob(logprob=-4.170803070068359, rank=2, decoded_token='!\"')}, {23955: Logprob(logprob=-0.04579643905162811, rank=1, decoded_token=' 이'), 109250: Logprob(logprob=-4.9051713943481445, rank=2, decoded_token=' 난')}, {101817: Logprob(logprob=-0.017689086496829987, rank=1, decoded_token=' 아이'), 49508: Logprob(logprob=-4.986439228057861, rank=2, decoded_token=' 아')}, {16969: Logprob(logprob=-0.00943298451602459, rank=1, decoded_token='는'), 21028: Logprob(logprob=-5.704745292663574, rank=2, decoded_token='의')}, {67236: Logprob(logprob=-0.016461392864584923, rank=1, decoded_token=' 내'), 109723: Logprob(logprob=-5.586773872375488, rank=2, decoded_token=' 내가')}, {49508: Logprob(logprob=-0.008231878280639648, rank=1, decoded_token=' 아'), 123683: Logprob(logprob=-6.445732116699219, rank=2, decoded_token=' 딸')}, {102823: Logprob(logprob=-0.049234386533498764, rank=1, decoded_token='들이'), 65950: Logprob(logprob=-3.533609390258789, rank=2, decoded_token='들')}, {105771: Logprob(logprob=-0.008618303574621677, rank=1, decoded_token='라고'), 35495: Logprob(logprob=-6.305493354797363, rank=2, decoded_token='고')}, {9135: Logprob(logprob=-0.19702287018299103, rank=1, decoded_token='!\"'), 25765: Logprob(logprob=-2.1813979148864746, rank=2, decoded_token='!\"\\n')}, {23955: Logprob(logprob=-0.26017627120018005, rank=1, decoded_token=' 이'), 128001: Logprob(logprob=-3.994551181793213, rank=2, decoded_token='')}, {103807: Logprob(logprob=-0.06441116333007812, rank=1, decoded_token=' 외'), 104065: Logprob(logprob=-4.306598663330078, rank=2, decoded_token='외')}, {125701: Logprob(logprob=-0.014521507546305656, rank=1, decoded_token='과의'), 54780: Logprob(logprob=-5.225459098815918, rank=2, decoded_token='과')}, {117396: Logprob(logprob=-0.33501142263412476, rank=1, decoded_token='사는'), 114333: Logprob(logprob=-1.4600114822387695, rank=2, decoded_token='사가')}, {101228: Logprob(logprob=-0.15840573608875275, rank=1, decoded_token=' 소'), 49508: Logprob(logprob=-4.814655780792236, rank=2, decoded_token=' 아')}, {100392: Logprob(logprob=-0.023176144808530807, rank=1, decoded_token='년'), 103284: Logprob(logprob=-4.616926193237305, rank=2, decoded_token='녀')}, {102244: Logprob(logprob=-0.3792417049407959, rank=1, decoded_token='에게'), 21028: Logprob(logprob=-1.332366704940796, rank=2, decoded_token='의')}, {123621: Logprob(logprob=-0.06001947075128555, rank=1, decoded_token=' 누구'), 106200: Logprob(logprob=-5.270956993103027, rank=2, decoded_token=' 누')}, {33177: Logprob(logprob=-0.019290929660201073, rank=1, decoded_token='일'), 32428: Logprob(logprob=-5.425540924072266, rank=2, decoded_token='인')}, {117677: Logprob(logprob=-0.014386854134500027, rank=1, decoded_token='까요'), 101154: Logprob(logprob=-4.3268866539001465, rank=2, decoded_token='까')}, {30: Logprob(logprob=-0.7000465393066406, rank=1, decoded_token='?'), 5380: Logprob(logprob=-0.7312965393066406, rank=2, decoded_token='?\\n')}, {128001: Logprob(logprob=-1.2070050239562988, rank=1, decoded_token=''), 23955: Logprob(logprob=-3.308567523956299, rank=2, decoded_token=' 이')}], finish_reason=stop, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1734942451.3035746, last_token_time=1734942451.3035746, first_scheduled_time=1734942451.3039036, first_token_time=1734942451.3109775, time_in_queue=0.00032901763916015625, finished_time=1734942451.4702091, scheduler_time=0.0029828804545104504, model_forward_time=None, model_execute_time=None), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})\n",
      "RequestOutput(request_id=5, prompt='질문:아이의 아버지인 외과의사가 말했어. \"난 수술 못해! 이 아이는 내 아들이라고!\" 이 외과의사는 소년에게 누구일까요?\\n답변:소', prompt_token_ids=[128000, 103194, 52688, 25, 114714, 21028, 123816, 32428, 103807, 125701, 114333, 101264, 102621, 32179, 13, 330, 103777, 29833, 102835, 104352, 34983, 0, 23955, 101817, 16969, 67236, 49508, 102823, 105771, 9135, 23955, 103807, 125701, 117396, 101228, 100392, 102244, 123621, 33177, 117677, 5380, 109659, 104449, 25, 44690], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='년은 외과의사 아버지의 아들이다.', token_ids=(100392, 34804, 103807, 125701, 56154, 123816, 21028, 49508, 65950, 101568, 13, 128001), cumulative_logprob=-11.845640011131763, logprobs=[{100392: Logprob(logprob=-0.11162563413381577, rank=1, decoded_token='년'), 103284: Logprob(logprob=-3.4553756713867188, rank=2, decoded_token='녀')}, {34804: Logprob(logprob=-1.3065505027770996, rank=1, decoded_token='은'), 21028: Logprob(logprob=-1.4549880027770996, rank=2, decoded_token='의')}, {103807: Logprob(logprob=-2.3810136318206787, rank=1, decoded_token=' 외'), 123816: Logprob(logprob=-2.6622636318206787, rank=2, decoded_token=' 아버지')}, {125701: Logprob(logprob=-0.23260043561458588, rank=1, decoded_token='과의'), 54780: Logprob(logprob=-1.920100450515747, rank=2, decoded_token='과')}, {56154: Logprob(logprob=-0.9367883205413818, rank=1, decoded_token='사'), 114333: Logprob(logprob=-1.3430383205413818, rank=2, decoded_token='사가')}, {123816: Logprob(logprob=-2.1301708221435547, rank=1, decoded_token=' 아버지'), 49508: Logprob(logprob=-2.3645458221435547, rank=2, decoded_token=' 아')}, {21028: Logprob(logprob=-0.7159854769706726, rank=1, decoded_token='의'), 20565: Logprob(logprob=-1.3644230365753174, rank=2, decoded_token='가')}, {49508: Logprob(logprob=-0.6807587742805481, rank=1, decoded_token=' 아'), 101817: Logprob(logprob=-2.9698212146759033, rank=2, decoded_token=' 아이')}, {65950: Logprob(logprob=-0.44352662563323975, rank=1, decoded_token='들'), 102823: Logprob(logprob=-1.2404016256332397, rank=2, decoded_token='들이')}, {101568: Logprob(logprob=-1.2884331941604614, rank=1, decoded_token='이다'), 80052: Logprob(logprob=-1.5384331941604614, rank=2, decoded_token='입니다')}, {13: Logprob(logprob=-0.5996137857437134, rank=1, decoded_token='.'), 627: Logprob(logprob=-0.9902387857437134, rank=2, decoded_token='.\\n')}, {128001: Logprob(logprob=-1.0185728073120117, rank=1, decoded_token=''), 103807: Logprob(logprob=-2.8779478073120117, rank=2, decoded_token=' 외')}], finish_reason=stop, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1734942451.3036735, last_token_time=1734942451.3036735, first_scheduled_time=1734942451.3039036, first_token_time=1734942451.3109775, time_in_queue=0.0002300739288330078, finished_time=1734942451.3549469, scheduler_time=0.0013047531247138977, model_forward_time=None, model_execute_time=None), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})\n",
      "\n",
      "\n",
      "======================================================================\n",
      "topk_tokens : 이, output :  외과의사는 소년의 아버지인 외과의사입니다.\n",
      "reasoning : 이 외과의사는 소년의 아버지인 외과의사입니다.\n",
      "encode : {'input_ids': [128000, 13094, 103807, 125701, 117396, 101228, 100392, 21028, 123816, 32428, 103807, 125701, 56154, 80052, 13], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 0), (0, 1), (1, 3), (3, 5), (5, 7), (7, 9), (9, 10), (10, 11), (11, 15), (15, 16), (16, 18), (18, 20), (20, 21), (21, 24), (24, 25)]}\n",
      "answer_span : ['이 외과의사는 소년의 아버지인 외과의사입니다']\n",
      "last_pattern_span : (0, 24)\n",
      "token_id : [128000, 13094, 103807, 125701, 117396, 101228, 100392, 21028, 123816, 32428, 103807, 125701, 56154, 80052]\n",
      "filtered_answer : [{13094: Logprob(logprob=-2.3700766563415527, rank=1, decoded_token='이')}, {103807: Logprob(logprob=-1.7521418333053589, rank=1, decoded_token=' 외'), 101817: Logprob(logprob=-1.9552668333053589, rank=2, decoded_token=' 아이')}, {125701: Logprob(logprob=-0.08718262612819672, rank=1, decoded_token='과의'), 54780: Logprob(logprob=-2.8059325218200684, rank=2, decoded_token='과')}, {117396: Logprob(logprob=-0.3540898859500885, rank=1, decoded_token='사는'), 114333: Logprob(logprob=-1.9634648561477661, rank=2, decoded_token='사가')}, {101228: Logprob(logprob=-1.9667726755142212, rank=1, decoded_token=' 소'), 101817: Logprob(logprob=-2.0995850563049316, rank=2, decoded_token=' 아이')}, {100392: Logprob(logprob=-0.04229452833533287, rank=1, decoded_token='년'), 116899: Logprob(logprob=-4.370419502258301, rank=2, decoded_token='년에')}, {21028: Logprob(logprob=-0.6454375982284546, rank=1, decoded_token='의'), 102244: Logprob(logprob=-1.7782500982284546, rank=2, decoded_token='에게')}, {123816: Logprob(logprob=-0.8672740459442139, rank=1, decoded_token=' 아버지'), 49508: Logprob(logprob=-2.164149045944214, rank=2, decoded_token=' 아')}, {32428: Logprob(logprob=-1.3710933923721313, rank=1, decoded_token='인'), 80052: Logprob(logprob=-2.269530773162842, rank=2, decoded_token='입니다')}, {103807: Logprob(logprob=-0.7757647037506104, rank=1, decoded_token=' 외'), 123816: Logprob(logprob=-2.7992022037506104, rank=2, decoded_token=' 아버지')}, {125701: Logprob(logprob=-0.06241424381732941, rank=1, decoded_token='과의'), 54780: Logprob(logprob=-2.9842891693115234, rank=2, decoded_token='과')}, {56154: Logprob(logprob=-0.7368443608283997, rank=1, decoded_token='사'), 114333: Logprob(logprob=-0.8462193608283997, rank=2, decoded_token='사가')}, {80052: Logprob(logprob=-1.5447012186050415, rank=1, decoded_token='입니다'), 101574: Logprob(logprob=-2.568138599395752, rank=2, decoded_token='였')}, {13: Logprob(logprob=-0.7634690403938293, rank=1, decoded_token='.'), 627: Logprob(logprob=-0.7790940403938293, rank=2, decoded_token='.\\n')}]\n",
      "======================================================================\n",
      "topk_tokens : 아이, output : 의 아버지인 외과의사가 말했어. \"난 수술 못해! 이 아이는 내 아들이라고!\" 이 외과의사는 소년에게 누구일까요?\n",
      "reasoning : 아이의 아버지인 외과의사가 말했어. \"난 수술 못해! 이 아이는 내 아들이라고!\" 이 외과의사는 소년에게 누구일까요?\n",
      "encode : {'input_ids': [128000, 114714, 21028, 123816, 32428, 103807, 125701, 114333, 101264, 102621, 32179, 13, 330, 103777, 29833, 102835, 104352, 34983, 0, 23955, 101817, 16969, 67236, 49508, 102823, 105771, 9135, 23955, 103807, 125701, 117396, 101228, 100392, 102244, 123621, 33177, 117677, 30], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 0), (0, 2), (2, 3), (3, 7), (7, 8), (8, 10), (10, 12), (12, 14), (14, 16), (16, 17), (17, 18), (18, 19), (19, 21), (21, 22), (22, 24), (24, 25), (25, 27), (27, 28), (28, 29), (29, 31), (31, 34), (34, 35), (35, 37), (37, 39), (39, 41), (41, 43), (43, 45), (45, 47), (47, 49), (49, 51), (51, 53), (53, 55), (55, 56), (56, 58), (58, 61), (61, 62), (62, 64), (64, 65)]}\n",
      "answer_span : ['아이의 아버지인 외과의사가 말했어', ' ', '난 수술 못해', ' 이 아이는 내 아들이라고', ' 이 외과의사는 소년에게 누구일까요']\n",
      "last_pattern_span : (45, 64)\n",
      "token_id : [23955, 103807, 125701, 117396, 101228, 100392, 102244, 123621, 33177, 117677]\n",
      "filtered_answer : [{103807: Logprob(logprob=-0.01751081459224224, rank=1, decoded_token=' 외'), 104065: Logprob(logprob=-5.017510890960693, rank=2, decoded_token='외')}, {125701: Logprob(logprob=-0.0071679335087537766, rank=1, decoded_token='과의'), 54780: Logprob(logprob=-6.022792816162109, rank=2, decoded_token='과')}, {117396: Logprob(logprob=-0.19185467064380646, rank=1, decoded_token='사는'), 114333: Logprob(logprob=-1.87935471534729, rank=2, decoded_token='사가')}, {101228: Logprob(logprob=-0.16632337868213654, rank=1, decoded_token=' 소'), 49508: Logprob(logprob=-4.846010684967041, rank=2, decoded_token=' 아')}, {100392: Logprob(logprob=-0.010965327732264996, rank=1, decoded_token='년'), 116899: Logprob(logprob=-5.448465347290039, rank=2, decoded_token='년에')}, {102244: Logprob(logprob=-0.0739128515124321, rank=1, decoded_token='에게'), 21028: Logprob(logprob=-3.4957878589630127, rank=2, decoded_token='의')}, {123621: Logprob(logprob=-0.06142662093043327, rank=1, decoded_token=' 누구'), 106200: Logprob(logprob=-5.225489139556885, rank=2, decoded_token=' 누')}, {33177: Logprob(logprob=-0.01662789285182953, rank=1, decoded_token='일'), 32428: Logprob(logprob=-5.829127788543701, rank=2, decoded_token='인')}, {117677: Logprob(logprob=-0.009986437857151031, rank=1, decoded_token='까요'), 101154: Logprob(logprob=-4.713111400604248, rank=2, decoded_token='까')}, {30: Logprob(logprob=-0.5475040078163147, rank=1, decoded_token='?'), 5380: Logprob(logprob=-0.9381290078163147, rank=2, decoded_token='?\\n')}]\n",
      "======================================================================\n",
      "topk_tokens : 아, output : 빠는 외과의사였습니다. 아이는 소년입니다.\n",
      "reasoning : 아빠는 외과의사였습니다. 아이는 소년입니다.\n",
      "encode : {'input_ids': [128000, 54059, 110632, 16969, 103807, 125701, 56154, 101574, 39331, 13, 101817, 16969, 101228, 100392, 80052, 13], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 0), (0, 1), (1, 2), (2, 3), (3, 5), (5, 7), (7, 8), (8, 9), (9, 12), (12, 13), (13, 16), (16, 17), (17, 19), (19, 20), (20, 23), (23, 24)]}\n",
      "answer_span : ['아빠는 외과의사였습니다', ' 아이는 소년입니다']\n",
      "last_pattern_span : (13, 23)\n",
      "token_id : [101817, 16969, 101228, 100392, 80052]\n",
      "filtered_answer : [{16969: Logprob(logprob=-0.8634384274482727, rank=1, decoded_token='는'), 21028: Logprob(logprob=-1.582188367843628, rank=2, decoded_token='의')}, {101228: Logprob(logprob=-2.4449100494384766, rank=1, decoded_token=' 소'), 49508: Logprob(logprob=-2.5542850494384766, rank=2, decoded_token=' 아')}, {100392: Logprob(logprob=-0.05206810310482979, rank=1, decoded_token='년'), 103284: Logprob(logprob=-4.22394323348999, rank=2, decoded_token='녀')}, {80052: Logprob(logprob=-0.9559745788574219, rank=1, decoded_token='입니다'), 113743: Logprob(logprob=-1.7215995788574219, rank=2, decoded_token='이었')}, {13: Logprob(logprob=-0.5675016045570374, rank=1, decoded_token='.'), 627: Logprob(logprob=-0.9268766045570374, rank=2, decoded_token='.\\n')}]\n",
      "======================================================================\n",
      "topk_tokens :  이, output :  외과의사는 소년의 아버지인 외과의사가 말했어. \"난 수술 못해! 이 아이는 내 아들이라고!\" 이 외과의사는 소년에게 누구일까요?\n",
      "reasoning : 이 외과의사는 소년의 아버지인 외과의사가 말했어. \"난 수술 못해! 이 아이는 내 아들이라고!\" 이 외과의사는 소년에게 누구일까요?\n",
      "encode : {'input_ids': [128000, 13094, 103807, 125701, 117396, 101228, 100392, 21028, 123816, 32428, 103807, 125701, 114333, 101264, 102621, 32179, 13, 330, 103777, 29833, 102835, 104352, 34983, 0, 23955, 101817, 16969, 67236, 49508, 102823, 105771, 9135, 23955, 103807, 125701, 117396, 101228, 100392, 102244, 123621, 33177, 117677, 30], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 0), (0, 1), (1, 3), (3, 5), (5, 7), (7, 9), (9, 10), (10, 11), (11, 15), (15, 16), (16, 18), (18, 20), (20, 22), (22, 24), (24, 25), (25, 26), (26, 27), (27, 29), (29, 30), (30, 32), (32, 33), (33, 35), (35, 36), (36, 37), (37, 39), (39, 42), (42, 43), (43, 45), (45, 47), (47, 49), (49, 51), (51, 53), (53, 55), (55, 57), (57, 59), (59, 61), (61, 63), (63, 64), (64, 66), (66, 69), (69, 70), (70, 72), (72, 73)]}\n",
      "answer_span : ['이 외과의사는 소년의 아버지인 외과의사가 말했어', ' ', '난 수술 못해', ' 이 아이는 내 아들이라고', ' 이 외과의사는 소년에게 누구일까요']\n",
      "last_pattern_span : (53, 72)\n",
      "token_id : [23955, 103807, 125701, 117396, 101228, 100392, 102244, 123621, 33177, 117677]\n",
      "filtered_answer : [{103807: Logprob(logprob=-0.06441116333007812, rank=1, decoded_token=' 외'), 104065: Logprob(logprob=-4.306598663330078, rank=2, decoded_token='외')}, {125701: Logprob(logprob=-0.014521507546305656, rank=1, decoded_token='과의'), 54780: Logprob(logprob=-5.225459098815918, rank=2, decoded_token='과')}, {117396: Logprob(logprob=-0.33501142263412476, rank=1, decoded_token='사는'), 114333: Logprob(logprob=-1.4600114822387695, rank=2, decoded_token='사가')}, {101228: Logprob(logprob=-0.15840573608875275, rank=1, decoded_token=' 소'), 49508: Logprob(logprob=-4.814655780792236, rank=2, decoded_token=' 아')}, {100392: Logprob(logprob=-0.023176144808530807, rank=1, decoded_token='년'), 103284: Logprob(logprob=-4.616926193237305, rank=2, decoded_token='녀')}, {102244: Logprob(logprob=-0.3792417049407959, rank=1, decoded_token='에게'), 21028: Logprob(logprob=-1.332366704940796, rank=2, decoded_token='의')}, {123621: Logprob(logprob=-0.06001947075128555, rank=1, decoded_token=' 누구'), 106200: Logprob(logprob=-5.270956993103027, rank=2, decoded_token=' 누')}, {33177: Logprob(logprob=-0.019290929660201073, rank=1, decoded_token='일'), 32428: Logprob(logprob=-5.425540924072266, rank=2, decoded_token='인')}, {117677: Logprob(logprob=-0.014386854134500027, rank=1, decoded_token='까요'), 101154: Logprob(logprob=-4.3268866539001465, rank=2, decoded_token='까')}, {30: Logprob(logprob=-0.7000465393066406, rank=1, decoded_token='?'), 5380: Logprob(logprob=-0.7312965393066406, rank=2, decoded_token='?\\n')}]\n",
      "======================================================================\n",
      "topk_tokens : 소, output : 년은 외과의사 아버지의 아들이다.\n",
      "reasoning : 소년은 외과의사 아버지의 아들이다.\n",
      "encode : {'input_ids': [128000, 117546, 34804, 103807, 125701, 56154, 123816, 21028, 49508, 65950, 101568, 13], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 0), (0, 2), (2, 3), (3, 5), (5, 7), (7, 8), (8, 12), (12, 13), (13, 15), (15, 16), (16, 18), (18, 19)]}\n",
      "answer_span : ['소년은 외과의사 아버지의 아들이다']\n",
      "last_pattern_span : (0, 18)\n",
      "token_id : [128000, 117546, 34804, 103807, 125701, 56154, 123816, 21028, 49508, 65950, 101568]\n",
      "filtered_answer : [{44690: Logprob(logprob=-3.5966391563415527, rank=5, decoded_token='소')}, {100392: Logprob(logprob=-0.11162563413381577, rank=1, decoded_token='년'), 103284: Logprob(logprob=-3.4553756713867188, rank=2, decoded_token='녀')}, {34804: Logprob(logprob=-1.3065505027770996, rank=1, decoded_token='은'), 21028: Logprob(logprob=-1.4549880027770996, rank=2, decoded_token='의')}, {103807: Logprob(logprob=-2.3810136318206787, rank=1, decoded_token=' 외'), 123816: Logprob(logprob=-2.6622636318206787, rank=2, decoded_token=' 아버지')}, {125701: Logprob(logprob=-0.23260043561458588, rank=1, decoded_token='과의'), 54780: Logprob(logprob=-1.920100450515747, rank=2, decoded_token='과')}, {56154: Logprob(logprob=-0.9367883205413818, rank=1, decoded_token='사'), 114333: Logprob(logprob=-1.3430383205413818, rank=2, decoded_token='사가')}, {123816: Logprob(logprob=-2.1301708221435547, rank=1, decoded_token=' 아버지'), 49508: Logprob(logprob=-2.3645458221435547, rank=2, decoded_token=' 아')}, {21028: Logprob(logprob=-0.7159854769706726, rank=1, decoded_token='의'), 20565: Logprob(logprob=-1.3644230365753174, rank=2, decoded_token='가')}, {49508: Logprob(logprob=-0.6807587742805481, rank=1, decoded_token=' 아'), 101817: Logprob(logprob=-2.9698212146759033, rank=2, decoded_token=' 아이')}, {65950: Logprob(logprob=-0.44352662563323975, rank=1, decoded_token='들'), 102823: Logprob(logprob=-1.2404016256332397, rank=2, decoded_token='들이')}, {101568: Logprob(logprob=-1.2884331941604614, rank=1, decoded_token='이다'), 80052: Logprob(logprob=-1.5384331941604614, rank=2, decoded_token='입니다')}]\n",
      "Question: 아이의 아버지인 외과의사가 말했어. \"난 수술 못해! 이 아이는 내 아들이라고!\" 이 외과의사는 소년에게 누구일까요?\n",
      "Path 0:\n",
      "  Reasoning: 이 외과의사는 소년의 아버지인 외과의사입니다.\n",
      "  Answer: 이 외과의사는 소년의 아버지인 외과의사입니다\n",
      "  Score: 0.3428\n",
      "\n",
      "Path 1:\n",
      "  Reasoning: 아이의 아버지인 외과의사가 말했어. \"난 수술 못해! 이 아이는 내 아들이라고!\" 이 외과의사는 소년에게 누구일까요?\n",
      "  Answer: 이 외과의사는 소년에게 누구일까요\n",
      "  Score: 0.0000\n",
      "\n",
      "Path 2:\n",
      "  Reasoning: 아빠는 외과의사였습니다. 아이는 소년입니다.\n",
      "  Answer: 아이는 소년입니다\n",
      "  Score: 0.3073\n",
      "\n",
      "Path 3:\n",
      "  Reasoning: 이 외과의사는 소년의 아버지인 외과의사가 말했어. \"난 수술 못해! 이 아이는 내 아들이라고!\" 이 외과의사는 소년에게 누구일까요?\n",
      "  Answer: 이 외과의사는 소년에게 누구일까요\n",
      "  Score: 0.0537\n",
      "\n",
      "Path 4:\n",
      "  Reasoning: 소년은 외과의사 아버지의 아들이다.\n",
      "  Answer: 소년은 외과의사 아버지의 아들이다\n",
      "  Score: 0.2594\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = '아이의 아버지인 외과의사가 말했어. \"난 수술 못해! 이 아이는 내 아들이라고!\" 이 외과의사는 소년에게 누구일까요?'\n",
    "# prompt = \"I have 3 apples, my dad has 2 more apples than me, how many apples do we have in total?\"\n",
    "result = decoder.search_cots(prompt)\n",
    "\n",
    "print(f\"Question: {result.question}\")\n",
    "for path in result.paths:\n",
    "    print(f\"Path {path.num_path}:\")\n",
    "    print(f\"  Reasoning: {path.reasoning_text}\")\n",
    "    print(f\"  Answer: {path.answer_span}\")\n",
    "    print(f\"  Score: {path.score:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
