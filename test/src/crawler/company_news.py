import os
import time

from openai import OpenAI
from collections import deque
from datetime import datetime
from urllib.parse import urljoin, urlparse

from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.remote.webdriver import WebDriver
from selenium.webdriver.support import expected_conditions as EC

from src.crawler.utils.selenium_utils import set_webdriver
from src.crawler.utils.data_utils import save_json, transform_data

from app.utils.logging import logger


SAVE_PATH = "./data/company_news"
NAVER_SEARCH_URL = "https://search.naver.com/search.naver?ssc=tab.news.all&where=news&sm=tab_jum&query="


def get_gpt_response(client: OpenAI, article_text: str, company_name: str):
    completion = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": f"ì£¼ì–´ì§„ ë‰´ìŠ¤ ê¸°ì‚¬ê°€ '{company_name}'ë¼ëŠ” íšŒì‚¬ë¥¼ ì£¼ì œë¡œ ì‘ì„±ëœ ê¸°ì‚¬ì¸ì§€ í™•ì¸í•˜ê³  ê´€ë ¨ì´ ìˆìœ¼ë©´ True, ì—†ìœ¼ë©´ Falseë¥¼ ë°˜í™˜í•˜ì„¸ìš”. ì‘ë‹µì€ ì˜¤ì§ True ë˜ëŠ” Falseë§Œ ë°˜í™˜í•´ì•¼ í•©ë‹ˆë‹¤."},
            {"role": "user", "content": f"ë‰´ìŠ¤ ê¸°ì‚¬: {article_text}"},
        ],
        temperature=0.0
    )
    return completion.choices[0].message.content


def check_correspondence(sub_driver: WebDriver, article_url: str, company_name: str, client: OpenAI):
    """ê¸°ì‚¬ ë³¸ë¬¸ì„ ê°€ì ¸ì™€ì„œ íŠ¹ì • ê¸°ì—…ì´ ì–¸ê¸‰ë˜ì—ˆëŠ”ì§€ í™•ì¸"""
    try:
        sub_driver.get(article_url)
        time.sleep(1)
        WebDriverWait(sub_driver, 5).until(EC.presence_of_element_located((By.TAG_NAME, "body")))

        # ì›¹í˜ì´ì§€ ë³¸ë¬¸ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
        article_text = sub_driver.find_element(By.TAG_NAME, "body").text

        relevant = get_gpt_response(client, article_text, company_name)
        if relevant == "True":
            return True, article_text
        else:
            return False, None

    except Exception as e:
        logger.error(f"âŒ ê¸°ì‚¬ ë³¸ë¬¸ ë¶„ì„ ì‹¤íŒ¨: {article_url} - ì˜¤ë¥˜: {e}")
        return False, None


def get_news_contents(driver: WebDriver, url: str, num_articles: int, company_name: str, client: OpenAI):
    logger.info(f"ë„¤ì´ë²„ ê²€ìƒ‰ ì‹œì‘: {url}")
    driver.get(url)
    
    collected_urls = set()
    collected_ids = set()
    article_urls = []
    last_collected_id = 0

    # ğŸ”¹ ê¸°ì‚¬ ë³¸ë¬¸ í™•ì¸ì„ ìœ„í•œ ë³„ë„ WebDriver ìƒì„± (í•œ ë²ˆë§Œ ì‹¤í–‰)
    sub_driver = set_webdriver()

    try:
        while len(article_urls) < num_articles:
            try:
                wrap = driver.find_element(By.ID, "wrap")
                container = wrap.find_element(By.ID, "container")
                content = container.find_element(By.ID, "content")
                main_pack = content.find_element(By.ID, "main_pack")
                section = main_pack.find_element(By.CLASS_NAME, "sp_nnews")
                list_news = section.find_element(By.CLASS_NAME, "list_news")
                news_items = list_news.find_elements(By.CLASS_NAME, "bx")

                # í˜„ì¬ í˜ì´ì§€ì—ì„œ ë§ˆì§€ë§‰ìœ¼ë¡œ ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ID ì¶”ì¶œ
                current_ids = []
                for item in news_items:
                    news_id = item.get_attribute("id")
                    if news_id and news_id.startswith("sp_nws"):
                        news_num = int(news_id.replace("sp_nws", ""))
                        current_ids.append(news_num)

                if not current_ids:
                    logger.warning("ìƒˆë¡œìš´ ë‰´ìŠ¤ ê¸°ì‚¬ê°€ ë°œê²¬ë˜ì§€ ì•ŠìŒ, ì¤‘ë‹¨")
                    break

                # ê°€ì¥ í° ID ì°¾ê¸° (ë§ˆì§€ë§‰ ê¸°ì‚¬ ID)
                max_current_id = max(current_ids)

                for item in news_items:
                    try:
                        news_id = item.get_attribute("id")
                        if not news_id or not news_id.startswith("sp_nws"):
                            continue

                        news_num = int(news_id.replace("sp_nws", ""))
                        if news_num <= last_collected_id or news_num in collected_ids:
                            continue

                        news_area = item.find_element(By.CLASS_NAME, "news_area")
                        news_contents = news_area.find_element(By.CLASS_NAME, "news_contents")
                        news_title = news_contents.find_element(By.CLASS_NAME, "news_tit")

                        article_url = news_title.get_attribute("href")
                        article_title = news_title.text
                        if article_url and article_url not in collected_urls:
                            collected_urls.add(article_url)
                            collected_ids.add(news_num)

                            # âœ… ê¸°ì‚¬ ë³¸ë¬¸ í™•ì¸ì„ **ë³„ë„ì˜ WebDriver(sub_driver)** ë¡œ ìˆ˜í–‰
                            is_relevant, article_text = check_correspondence(sub_driver, article_url, company_name, client)

                            # TODO : Embedding ê¸°ë°˜ìœ¼ë¡œ ì¤‘ë³µë˜ëŠ” ë¬¸ì„œ ì œê±° í›„ ê³ ìœ í•œ ë¬¸ì„œë§Œ ë¦¬ìŠ¤íŠ¸ì— ë‹´ë„ë¡ ì¶”ê°€.

                            if is_relevant:
                                article_urls.append({"title" : article_title, "url" : article_url, "page_text" : article_text})
                                logger.info(f"âœ… ê¸°ì—… ê´€ë ¨ ê¸°ì‚¬ ì¶”ê°€ë¨: [{len(article_urls)}/{num_articles}] {article_url}")
                            else:
                                logger.warning(f"âŒ ê¸°ì—… ê´€ë ¨ ì—†ìŒ: {article_url}")

                        if len(article_urls) >= num_articles:
                            break

                    except Exception as e:
                        logger.warning(f"ê¸°ì‚¬ URL ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

                last_collected_id = max_current_id

                # ê¸°ì‚¬ ê°œìˆ˜ê°€ ë¶€ì¡±í•˜ë©´ ìŠ¤í¬ë¡¤ì„ ë‚´ë ¤ ë” ë¡œë“œ
                if len(article_urls) < num_articles:
                    driver.find_element(By.TAG_NAME, "body").send_keys(Keys.END)
                    time.sleep(1.5)

            except Exception as e:
                logger.error(f"ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                break

    finally:
        # âœ… WebDriver ì¢…ë£Œ (ì•ˆì •ì ì¸ ì¢…ë£Œ)
        sub_driver.quit()

    logger.info(f"ì´ {len(article_urls)}ê°œì˜ ê¸°ì—… ê´€ë ¨ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")
    return article_urls


def get_company_news(company_name:str, num_articles:int, api_key:str):
    os.makedirs(SAVE_PATH, exist_ok=True)
    client = OpenAI(api_key=api_key)
    driver = set_webdriver()
    
    try:
        search_url = f"{NAVER_SEARCH_URL}{company_name}"
        data = get_news_contents(driver, search_url, num_articles, company_name, client)
        logger.info(f"{company_name} ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ. {len(data)}ê°œ ìˆ˜ì§‘ë¨.")


        data = {
            "company": company_name,
            "collected_date": datetime.today().strftime("%y.%m.%d-%H:%M:%S"),
            "news_data": data
        }

        save_json(data, f"{SAVE_PATH}/{company_name}.json")
        logger.info(f"ê¸°ì—… ë‰´ìŠ¤ ìˆ˜ì§‘ ê²°ê³¼ê°€ {SAVE_PATH}/{company_name}.jsonì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return data
        
    except Exception as e:
        logger.error(f"ê¸°ì—… ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ : {e}")
        return None